<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>VAR &amp; BVAR · MacroEconometricModels.jl</title><meta name="title" content="VAR &amp; BVAR · MacroEconometricModels.jl"/><meta property="og:title" content="VAR &amp; BVAR · MacroEconometricModels.jl"/><meta property="twitter:title" content="VAR &amp; BVAR · MacroEconometricModels.jl"/><meta name="description" content="Documentation for MacroEconometricModels.jl."/><meta property="og:description" content="Documentation for MacroEconometricModels.jl."/><meta property="twitter:description" content="Documentation for MacroEconometricModels.jl."/><meta property="og:url" content="https://chung9207.github.io/MacroEconometricModels.jl/manual/"/><meta property="twitter:url" content="https://chung9207.github.io/MacroEconometricModels.jl/manual/"/><link rel="canonical" href="https://chung9207.github.io/MacroEconometricModels.jl/manual/"/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">MacroEconometricModels.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><span class="tocitem">by Subject</span><ul><li class="is-active"><a class="tocitem" href>VAR &amp; BVAR</a><ul class="internal"><li><a class="tocitem" href="#Vector-Autoregression-(VAR)"><span>Vector Autoregression (VAR)</span></a></li><li><a class="tocitem" href="#Structural-VAR-(SVAR)-and-Identification"><span>Structural VAR (SVAR) and Identification</span></a></li><li><a class="tocitem" href="#Impulse-Response-Functions-(IRF)"><span>Impulse Response Functions (IRF)</span></a></li><li><a class="tocitem" href="#Forecast-Error-Variance-Decomposition-(FEVD)"><span>Forecast Error Variance Decomposition (FEVD)</span></a></li><li><a class="tocitem" href="#Bayesian-VAR-(BVAR)"><span>Bayesian VAR (BVAR)</span></a></li><li><a class="tocitem" href="#Information-Criteria-and-Model-Selection"><span>Information Criteria and Model Selection</span></a></li><li><a class="tocitem" href="#Covariance-Estimation"><span>Covariance Estimation</span></a></li><li><a class="tocitem" href="#References"><span>References</span></a></li></ul></li><li><a class="tocitem" href="../lp/">Local Projections</a></li><li><a class="tocitem" href="../factormodels/">Factor Models</a></li></ul></li><li><a class="tocitem" href="../examples/">Examples</a></li><li><a class="tocitem" href="../api/">API Reference</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">by Subject</a></li><li class="is-active"><a href>VAR &amp; BVAR</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>VAR &amp; BVAR</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/chung9207/MacroEconometricModels.jl/blob/main/docs/src/manual.md#" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Manual"><a class="docs-heading-anchor" href="#Manual">Manual</a><a id="Manual-1"></a><a class="docs-heading-anchor-permalink" href="#Manual" title="Permalink"></a></h1><p>This manual provides a comprehensive theoretical background for the macroeconometric methods implemented in <strong>MacroEconometricModels.jl</strong>, including precise mathematical formulations and references to the literature.</p><h2 id="Vector-Autoregression-(VAR)"><a class="docs-heading-anchor" href="#Vector-Autoregression-(VAR)">Vector Autoregression (VAR)</a><a id="Vector-Autoregression-(VAR)-1"></a><a class="docs-heading-anchor-permalink" href="#Vector-Autoregression-(VAR)" title="Permalink"></a></h2><h3 id="The-Reduced-Form-VAR-Model"><a class="docs-heading-anchor" href="#The-Reduced-Form-VAR-Model">The Reduced-Form VAR Model</a><a id="The-Reduced-Form-VAR-Model-1"></a><a class="docs-heading-anchor-permalink" href="#The-Reduced-Form-VAR-Model" title="Permalink"></a></h3><p>A VAR(p) model for an <span>$n$</span>-dimensional vector of endogenous variables <span>$y_t$</span> is defined as:</p><p class="math-container">\[y_t = c + A_1 y_{t-1} + A_2 y_{t-2} + \cdots + A_p y_{t-p} + u_t\]</p><p>where:</p><ul><li><span>$y_t$</span> is an <span>$n \times 1$</span> vector of endogenous variables at time <span>$t$</span></li><li><span>$c$</span> is an <span>$n \times 1$</span> vector of intercepts</li><li><span>$A_i$</span> are <span>$n \times n$</span> coefficient matrices for lag <span>$i = 1, \ldots, p$</span></li><li><span>$u_t$</span> is an <span>$n \times 1$</span> vector of reduced-form innovations with <span>$E[u_t] = 0$</span> and <span>$E[u_t u_t&#39;] = \Sigma$</span></li></ul><p><strong>Reference</strong>: Sims (1980), Lütkepohl (2005, Chapter 2)</p><h3 id="Compact-Matrix-Representation"><a class="docs-heading-anchor" href="#Compact-Matrix-Representation">Compact Matrix Representation</a><a id="Compact-Matrix-Representation-1"></a><a class="docs-heading-anchor-permalink" href="#Compact-Matrix-Representation" title="Permalink"></a></h3><p>For estimation, we stack observations into matrices. Let <span>$T$</span> denote the effective sample size after accounting for lags. Define:</p><p class="math-container">\[Y = \begin{bmatrix} y_{p+1}&#39; \\ y_{p+2}&#39; \\ \vdots \\ y_T&#39; \end{bmatrix}_{(T-p) \times n}, \quad
X = \begin{bmatrix} 1 &amp; y_p&#39; &amp; y_{p-1}&#39; &amp; \cdots &amp; y_1&#39; \\
1 &amp; y_{p+1}&#39; &amp; y_p&#39; &amp; \cdots &amp; y_2&#39; \\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
1 &amp; y_{T-1}&#39; &amp; y_{T-2}&#39; &amp; \cdots &amp; y_{T-p}&#39; \end{bmatrix}_{(T-p) \times (1+np)}\]</p><p>The VAR can be written in matrix form as:</p><p class="math-container">\[Y = X B + U\]</p><p>where <span>$B = [c, A_1, A_2, \ldots, A_p]&#39;$</span> is a <span>$(1+np) \times n$</span> coefficient matrix.</p><h3 id="OLS-Estimation"><a class="docs-heading-anchor" href="#OLS-Estimation">OLS Estimation</a><a id="OLS-Estimation-1"></a><a class="docs-heading-anchor-permalink" href="#OLS-Estimation" title="Permalink"></a></h3><p>The OLS estimator is given by:</p><p class="math-container">\[\hat{B} = (X&#39;X)^{-1} X&#39;Y\]</p><p>The residual covariance matrix is estimated as:</p><p class="math-container">\[\hat{\Sigma} = \frac{1}{T-p-k} \hat{U}&#39;\hat{U}\]</p><p>where <span>$\hat{U} = Y - X\hat{B}$</span> and <span>$k = 1 + np$</span> is the number of regressors per equation.</p><p><strong>Reference</strong>: Hamilton (1994, Chapter 11), Lütkepohl (2005, Section 3.2)</p><h3 id="Stability-Condition"><a class="docs-heading-anchor" href="#Stability-Condition">Stability Condition</a><a id="Stability-Condition-1"></a><a class="docs-heading-anchor-permalink" href="#Stability-Condition" title="Permalink"></a></h3><p>A VAR(p) is stable (stationary) if all eigenvalues of the companion matrix <span>$F$</span> lie inside the unit circle:</p><p class="math-container">\[F = \begin{bmatrix}
A_1 &amp; A_2 &amp; \cdots &amp; A_{p-1} &amp; A_p \\
I_n &amp; 0 &amp; \cdots &amp; 0 &amp; 0 \\
0 &amp; I_n &amp; \cdots &amp; 0 &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots &amp; \vdots \\
0 &amp; 0 &amp; \cdots &amp; I_n &amp; 0
\end{bmatrix}_{np \times np}\]</p><p><strong>Stability Check</strong>: <span>$|\lambda_i| &lt; 1$</span> for all eigenvalues <span>$\lambda_i$</span> of <span>$F$</span>.</p><h3 id="Information-Criteria-for-Lag-Selection"><a class="docs-heading-anchor" href="#Information-Criteria-for-Lag-Selection">Information Criteria for Lag Selection</a><a id="Information-Criteria-for-Lag-Selection-1"></a><a class="docs-heading-anchor-permalink" href="#Information-Criteria-for-Lag-Selection" title="Permalink"></a></h3><p>The optimal lag length can be selected using information criteria:</p><p><strong>Akaike Information Criterion (AIC)</strong>:</p><p class="math-container">\[\text{AIC}(p) = \log|\hat{\Sigma}| + \frac{2}{T}(n^2 p + n)\]</p><p><strong>Bayesian Information Criterion (BIC)</strong>:</p><p class="math-container">\[\text{BIC}(p) = \log|\hat{\Sigma}| + \frac{\log T}{T}(n^2 p + n)\]</p><p><strong>Hannan-Quinn Criterion (HQ)</strong>:</p><p class="math-container">\[\text{HQ}(p) = \log|\hat{\Sigma}| + \frac{2 \log(\log T)}{T}(n^2 p + n)\]</p><p>Select the lag order <span>$p$</span> that minimizes the criterion.</p><p><strong>Reference</strong>: Lütkepohl (2005, Section 4.3)</p><hr/><h2 id="Structural-VAR-(SVAR)-and-Identification"><a class="docs-heading-anchor" href="#Structural-VAR-(SVAR)-and-Identification">Structural VAR (SVAR) and Identification</a><a id="Structural-VAR-(SVAR)-and-Identification-1"></a><a class="docs-heading-anchor-permalink" href="#Structural-VAR-(SVAR)-and-Identification" title="Permalink"></a></h2><h3 id="From-Reduced-Form-to-Structural-Shocks"><a class="docs-heading-anchor" href="#From-Reduced-Form-to-Structural-Shocks">From Reduced-Form to Structural Shocks</a><a id="From-Reduced-Form-to-Structural-Shocks-1"></a><a class="docs-heading-anchor-permalink" href="#From-Reduced-Form-to-Structural-Shocks" title="Permalink"></a></h3><p>The reduced-form residuals <span>$u_t$</span> are linear combinations of structural shocks <span>$\varepsilon_t$</span>:</p><p class="math-container">\[u_t = B_0 \varepsilon_t\]</p><p>where:</p><ul><li><span>$B_0$</span> is the <span>$n \times n$</span> contemporaneous impact matrix</li><li><span>$\varepsilon_t$</span> are structural shocks with <span>$E[\varepsilon_t \varepsilon_t&#39;] = I_n$</span></li></ul><p>The relationship between the reduced-form and structural covariance is:</p><p class="math-container">\[\Sigma = B_0 B_0&#39;\]</p><p>The <strong>identification problem</strong> is that infinitely many <span>$B_0$</span> matrices satisfy this condition. To identify structural shocks, we need <span>$n(n-1)/2$</span> additional restrictions.</p><p><strong>Reference</strong>: Kilian &amp; Lütkepohl (2017, Chapter 8)</p><h3 id="Cholesky-Identification-(Recursive)"><a class="docs-heading-anchor" href="#Cholesky-Identification-(Recursive)">Cholesky Identification (Recursive)</a><a id="Cholesky-Identification-(Recursive)-1"></a><a class="docs-heading-anchor-permalink" href="#Cholesky-Identification-(Recursive)" title="Permalink"></a></h3><p>The Cholesky decomposition imposes a lower triangular structure on <span>$B_0$</span>:</p><p class="math-container">\[B_0 = \text{chol}(\Sigma)\]</p><p>This implies a recursive causal ordering where variable <span>$i$</span> responds contemporaneously only to variables <span>$1, 2, \ldots, i-1$</span>.</p><p><strong>Economic Interpretation</strong>: The ordering reflects assumptions about the speed of adjustment. Variables ordered first respond only to their own shocks contemporaneously.</p><p><strong>Reference</strong>: Sims (1980), Christiano, Eichenbaum &amp; Evans (1999)</p><h3 id="Sign-Restrictions"><a class="docs-heading-anchor" href="#Sign-Restrictions">Sign Restrictions</a><a id="Sign-Restrictions-1"></a><a class="docs-heading-anchor-permalink" href="#Sign-Restrictions" title="Permalink"></a></h3><p>Sign restrictions identify structural shocks by constraining the signs of impulse responses at selected horizons. Let <span>$\Theta_h$</span> denote the impulse response at horizon <span>$h$</span>. The identification algorithm:</p><ol><li>Compute the Cholesky decomposition: <span>$P = \text{chol}(\Sigma)$</span></li><li>Draw a random orthogonal matrix <span>$Q$</span> from the Haar measure (using QR decomposition of a random matrix)</li><li>Compute candidate impact matrix: <span>$B_0 = PQ$</span></li><li>Check if impulse responses <span>$\Theta_0 = B_0, \Theta_1, \ldots$</span> satisfy the sign restrictions</li><li>If restrictions are satisfied, keep the draw; otherwise, discard and repeat</li></ol><p><strong>Implementation</strong>: We use the algorithm of Rubio-Ramírez, Waggoner &amp; Zha (2010).</p><p><strong>Reference</strong>: Faust (1998), Uhlig (2005), Rubio-Ramírez, Waggoner &amp; Zha (2010)</p><h3 id="Narrative-Restrictions"><a class="docs-heading-anchor" href="#Narrative-Restrictions">Narrative Restrictions</a><a id="Narrative-Restrictions-1"></a><a class="docs-heading-anchor-permalink" href="#Narrative-Restrictions" title="Permalink"></a></h3><p>Narrative restrictions combine sign restrictions with historical information about specific shocks at particular dates. Following Antolín-Díaz &amp; Rubio-Ramírez (2018):</p><ol><li><strong>Shock Sign Narrative</strong>: At date <span>$t^*$</span>, structural shock <span>$j$</span> was positive/negative</li><li><strong>Shock Contribution Narrative</strong>: At date <span>$t^*$</span>, shock <span>$j$</span> was the main driver of variable <span>$i$</span></li></ol><p>The algorithm:</p><ol><li>Draw orthogonal matrix <span>$Q$</span> satisfying sign restrictions</li><li>Recover structural shocks: <span>$\varepsilon = B_0^{-1} u$</span></li><li>Check if narrative constraints are satisfied</li><li>Weight the draw using importance sampling</li></ol><p><strong>Reference</strong>: Antolín-Díaz &amp; Rubio-Ramírez (2018)</p><h3 id="Long-Run-(Blanchard-Quah)-Identification"><a class="docs-heading-anchor" href="#Long-Run-(Blanchard-Quah)-Identification">Long-Run (Blanchard-Quah) Identification</a><a id="Long-Run-(Blanchard-Quah)-Identification-1"></a><a class="docs-heading-anchor-permalink" href="#Long-Run-(Blanchard-Quah)-Identification" title="Permalink"></a></h3><p>Long-run restrictions constrain the cumulative effect of structural shocks. For a stationary VAR, the long-run impact matrix is:</p><p class="math-container">\[C(1) = (I_n - A_1 - A_2 - \cdots - A_p)^{-1} B_0\]</p><p>Blanchard &amp; Quah (1989) impose that certain shocks have zero long-run effect on specific variables by requiring <span>$C(1)$</span> to be lower triangular:</p><p class="math-container">\[C(1) = \text{chol}\left( (I - A(1))^{-1} \Sigma (I - A(1)&#39;)^{-1} \right)\]</p><p>Then <span>$B_0 = (I - A(1)) C(1)$</span>.</p><p><strong>Economic Application</strong>: Demand shocks have no long-run effect on output (supply-driven long-run fluctuations).</p><p><strong>Reference</strong>: Blanchard &amp; Quah (1989), King, Plosser, Stock &amp; Watson (1991)</p><hr/><h2 id="Impulse-Response-Functions-(IRF)"><a class="docs-heading-anchor" href="#Impulse-Response-Functions-(IRF)">Impulse Response Functions (IRF)</a><a id="Impulse-Response-Functions-(IRF)-1"></a><a class="docs-heading-anchor-permalink" href="#Impulse-Response-Functions-(IRF)" title="Permalink"></a></h2><h3 id="Definition"><a class="docs-heading-anchor" href="#Definition">Definition</a><a id="Definition-1"></a><a class="docs-heading-anchor-permalink" href="#Definition" title="Permalink"></a></h3><p>The impulse response function <span>$\Theta_h$</span> measures the effect of a one-unit structural shock at time <span>$t$</span> on the endogenous variables at time <span>$t+h$</span>:</p><p class="math-container">\[\Theta_h = \frac{\partial y_{t+h}}{\partial \varepsilon_t&#39;}\]</p><p>For a VAR, the IRF at horizon <span>$h$</span> is computed recursively:</p><p class="math-container">\[\Theta_h = \sum_{i=1}^{\min(h,p)} A_i \Theta_{h-i}\]</p><p>with <span>$\Theta_0 = B_0$</span> (the structural impact matrix).</p><h3 id="Companion-Form-Representation"><a class="docs-heading-anchor" href="#Companion-Form-Representation">Companion Form Representation</a><a id="Companion-Form-Representation-1"></a><a class="docs-heading-anchor-permalink" href="#Companion-Form-Representation" title="Permalink"></a></h3><p>Using the companion form, IRFs can be computed as:</p><p class="math-container">\[\Theta_h = J F^h J&#39; B_0\]</p><p>where <span>$J = [I_n, 0, \ldots, 0]$</span> is an <span>$n \times np$</span> selection matrix and <span>$F$</span> is the companion matrix.</p><h3 id="Cumulative-IRF"><a class="docs-heading-anchor" href="#Cumulative-IRF">Cumulative IRF</a><a id="Cumulative-IRF-1"></a><a class="docs-heading-anchor-permalink" href="#Cumulative-IRF" title="Permalink"></a></h3><p>The cumulative impulse response up to horizon <span>$H$</span> is:</p><p class="math-container">\[\Theta^{cum}_H = \sum_{h=0}^{H} \Theta_h\]</p><p>As <span>$H \to \infty$</span>, for a stable VAR:</p><p class="math-container">\[\Theta^{cum}_\infty = (I_n - A_1 - \cdots - A_p)^{-1} B_0\]</p><h3 id="Confidence-Intervals"><a class="docs-heading-anchor" href="#Confidence-Intervals">Confidence Intervals</a><a id="Confidence-Intervals-1"></a><a class="docs-heading-anchor-permalink" href="#Confidence-Intervals" title="Permalink"></a></h3><p><strong>Bootstrap (Frequentist)</strong>: We use the residual bootstrap of Kilian (1998):</p><ol><li>Estimate the VAR and save residuals <span>$\hat{u}_t$</span></li><li>Generate bootstrap sample by resampling residuals with replacement</li><li>Re-estimate the VAR and compute IRFs</li><li>Repeat <span>$B$</span> times to build the distribution</li></ol><p><strong>Credible Intervals (Bayesian)</strong>: For each MCMC draw, compute IRFs and report posterior quantiles (e.g., 16th and 84th percentiles for 68% intervals).</p><p><strong>Reference</strong>: Kilian (1998), Lütkepohl (2005, Chapter 3)</p><hr/><h2 id="Forecast-Error-Variance-Decomposition-(FEVD)"><a class="docs-heading-anchor" href="#Forecast-Error-Variance-Decomposition-(FEVD)">Forecast Error Variance Decomposition (FEVD)</a><a id="Forecast-Error-Variance-Decomposition-(FEVD)-1"></a><a class="docs-heading-anchor-permalink" href="#Forecast-Error-Variance-Decomposition-(FEVD)" title="Permalink"></a></h2><h3 id="Definition-2"><a class="docs-heading-anchor" href="#Definition-2">Definition</a><a class="docs-heading-anchor-permalink" href="#Definition-2" title="Permalink"></a></h3><p>The FEVD measures the proportion of the <span>$h$</span>-step ahead forecast error variance of variable <span>$i$</span> attributable to structural shock <span>$j$</span>:</p><p class="math-container">\[\text{FEVD}_{ij}(h) = \frac{\sum_{s=0}^{h-1} (\Theta_s)_{ij}^2}{\sum_{s=0}^{h-1} \sum_{k=1}^{n} (\Theta_s)_{ik}^2}\]</p><p>where <span>$(\Theta_s)_{ij}$</span> is the <span>$(i,j)$</span> element of the impulse response matrix at horizon <span>$s$</span>.</p><h3 id="Properties"><a class="docs-heading-anchor" href="#Properties">Properties</a><a id="Properties-1"></a><a class="docs-heading-anchor-permalink" href="#Properties" title="Permalink"></a></h3><ul><li><span>$0 \leq \text{FEVD}_{ij}(h) \leq 1$</span> for all <span>$i, j, h$</span></li><li><span>$\sum_{j=1}^{n} \text{FEVD}_{ij}(h) = 1$</span> for all <span>$i, h$</span></li><li>As <span>$h \to \infty$</span>, FEVD converges to the unconditional variance decomposition</li></ul><p><strong>Reference</strong>: Lütkepohl (2005, Section 2.3.3)</p><hr/><h2 id="Bayesian-VAR-(BVAR)"><a class="docs-heading-anchor" href="#Bayesian-VAR-(BVAR)">Bayesian VAR (BVAR)</a><a id="Bayesian-VAR-(BVAR)-1"></a><a class="docs-heading-anchor-permalink" href="#Bayesian-VAR-(BVAR)" title="Permalink"></a></h2><h3 id="Bayesian-Framework"><a class="docs-heading-anchor" href="#Bayesian-Framework">Bayesian Framework</a><a id="Bayesian-Framework-1"></a><a class="docs-heading-anchor-permalink" href="#Bayesian-Framework" title="Permalink"></a></h3><p>In the Bayesian approach, we treat the VAR parameters as random variables and update our beliefs using Bayes&#39; theorem:</p><p class="math-container">\[p(B, \Sigma | Y) \propto p(Y | B, \Sigma) \cdot p(B, \Sigma)\]</p><p>where:</p><ul><li><span>$p(Y | B, \Sigma)$</span> is the likelihood</li><li><span>$p(B, \Sigma)$</span> is the prior</li><li><span>$p(B, \Sigma | Y)$</span> is the posterior</li></ul><h3 id="The-Minnesota-Prior"><a class="docs-heading-anchor" href="#The-Minnesota-Prior">The Minnesota Prior</a><a id="The-Minnesota-Prior-1"></a><a class="docs-heading-anchor-permalink" href="#The-Minnesota-Prior" title="Permalink"></a></h3><p>The Minnesota prior (Litterman, 1986; Doan, Litterman &amp; Sims, 1984) shrinks VAR coefficients toward a random walk prior:</p><p><strong>Prior Mean</strong>: Each variable follows a random walk:</p><p class="math-container">\[E[A_{1,ii}] = 1, \quad E[A_{1,ij}] = 0 \text{ for } i \neq j, \quad E[A_l] = 0 \text{ for } l &gt; 1\]</p><p><strong>Prior Variance</strong>: The prior variance for coefficient <span>$(i,j)$</span> at lag <span>$l$</span> is:</p><p class="math-container">\[\text{Var}(A_{l,ij}) = \begin{cases}
\frac{\tau^2}{l^d} &amp; \text{if } i = j \text{ (own lag)} \\
\frac{\tau^2 \omega^2}{l^d} \cdot \frac{\sigma_i^2}{\sigma_j^2} &amp; \text{if } i \neq j \text{ (cross lag)}
\end{cases}\]</p><p>where:</p><ul><li><span>$\tau$</span> is the overall tightness (shrinkage intensity)</li><li><span>$d$</span> is the lag decay (typically <span>$d = 2$</span>)</li><li><span>$\omega$</span> controls cross-variable shrinkage (typically <span>$\omega &lt; 1$</span>)</li><li><span>$\sigma_i^2$</span> is the residual variance from a univariate AR(1) for variable <span>$i$</span></li></ul><h3 id="Dummy-Observations-Approach"><a class="docs-heading-anchor" href="#Dummy-Observations-Approach">Dummy Observations Approach</a><a id="Dummy-Observations-Approach-1"></a><a class="docs-heading-anchor-permalink" href="#Dummy-Observations-Approach" title="Permalink"></a></h3><p>We implement the Minnesota prior using dummy observations (Theil-Goldberger mixed estimation). The augmented data matrices are:</p><p><strong>Prior on coefficients</strong> (tightness dummies):</p><p class="math-container">\[Y_d = \begin{bmatrix}
\text{diag}(\sigma_1, \ldots, \sigma_n) / \tau \\
0_{n(p-1) \times n} \\
\text{diag}(\sigma_1, \ldots, \sigma_n) \\
0_{1 \times n}
\end{bmatrix}, \quad
X_d = \begin{bmatrix}
0_{n \times 1} &amp; J_p \otimes \text{diag}(\sigma_1, \ldots, \sigma_n) / \tau \\
0_{n(p-1) \times 1} &amp; I_{p-1} \otimes \text{diag}(\sigma_1, \ldots, \sigma_n) \\
0_{n \times 1} &amp; 0_{n \times np} \\
c &amp; 0_{1 \times np}
\end{bmatrix}\]</p><p>where <span>$J_p = \text{diag}(1, 2^d, \ldots, p^d)$</span>.</p><p>The posterior is then computed as OLS on the augmented data <span>$[Y; Y_d]$</span> and <span>$[X; X_d]$</span>.</p><p><strong>Reference</strong>: Litterman (1986), Kadiyala &amp; Karlsson (1997), Bańbura, Giannone &amp; Reichlin (2010)</p><h3 id="Hyperparameter-Optimization-(Giannone,-Lenza-and-Primiceri,-2015)"><a class="docs-heading-anchor" href="#Hyperparameter-Optimization-(Giannone,-Lenza-and-Primiceri,-2015)">Hyperparameter Optimization (Giannone, Lenza &amp; Primiceri, 2015)</a><a id="Hyperparameter-Optimization-(Giannone,-Lenza-and-Primiceri,-2015)-1"></a><a class="docs-heading-anchor-permalink" href="#Hyperparameter-Optimization-(Giannone,-Lenza-and-Primiceri,-2015)" title="Permalink"></a></h3><p>Rather than selecting <span>$\tau$</span> subjectively, we can optimize it by maximizing the marginal likelihood:</p><p class="math-container">\[p(Y | \tau) = \int p(Y | B, \Sigma) p(B, \Sigma | \tau) \, dB \, d\Sigma\]</p><p>For the Normal-Inverse-Wishart prior with dummy observations, the log marginal likelihood has an analytical form:</p><p class="math-container">\[\log p(Y | \tau) = c + \frac{T-k}{2} \log|\tilde{S}^{-1}| - \frac{T_d}{2} \log|\tilde{S}_d^{-1}| + \log \frac{\Gamma_n(\frac{T+T_d - k}{2})}{\Gamma_n(\frac{T_d - k}{2})}\]</p><p>where <span>$\tilde{S}$</span> and <span>$\tilde{S}_d$</span> are the residual sum of squares from the augmented and dummy-only regressions.</p><p><strong>Reference</strong>: Giannone, Lenza &amp; Primiceri (2015), Carriero, Clark &amp; Marcellino (2015)</p><h3 id="MCMC-Estimation-with-Turing.jl"><a class="docs-heading-anchor" href="#MCMC-Estimation-with-Turing.jl">MCMC Estimation with Turing.jl</a><a id="MCMC-Estimation-with-Turing.jl-1"></a><a class="docs-heading-anchor-permalink" href="#MCMC-Estimation-with-Turing.jl" title="Permalink"></a></h3><p>For more flexible priors or non-conjugate settings, we use MCMC via Turing.jl with the NUTS sampler:</p><pre><code class="language-julia hljs">@model function bvar_model(Y, X, prior_mean, prior_var, ν₀, S₀)
    n = size(Y, 2)
    k = size(X, 2)

    # Prior on error covariance
    Σ ~ InverseWishart(ν₀, S₀)

    # Prior on coefficients
    B ~ MatrixNormal(prior_mean, prior_var, Σ)

    # Likelihood
    for t in axes(Y, 1)
        Y[t, :] ~ MvNormal(X[t, :]&#39; * B, Σ)
    end
end</code></pre><p><strong>Reference</strong>: Gelman et al. (2013), Hoffman &amp; Gelman (2014)</p><hr/><h2 id="Information-Criteria-and-Model-Selection"><a class="docs-heading-anchor" href="#Information-Criteria-and-Model-Selection">Information Criteria and Model Selection</a><a id="Information-Criteria-and-Model-Selection-1"></a><a class="docs-heading-anchor-permalink" href="#Information-Criteria-and-Model-Selection" title="Permalink"></a></h2><h3 id="Log-Likelihood"><a class="docs-heading-anchor" href="#Log-Likelihood">Log-Likelihood</a><a id="Log-Likelihood-1"></a><a class="docs-heading-anchor-permalink" href="#Log-Likelihood" title="Permalink"></a></h3><p>For a Gaussian VAR, the log-likelihood is:</p><p class="math-container">\[\log L = -\frac{T \cdot n}{2} \log(2\pi) - \frac{T}{2} \log|\Sigma| - \frac{1}{2} \sum_{t=1}^{T} u_t&#39; \Sigma^{-1} u_t\]</p><h3 id="Marginal-Likelihood-(Bayesian)"><a class="docs-heading-anchor" href="#Marginal-Likelihood-(Bayesian)">Marginal Likelihood (Bayesian)</a><a id="Marginal-Likelihood-(Bayesian)-1"></a><a class="docs-heading-anchor-permalink" href="#Marginal-Likelihood-(Bayesian)" title="Permalink"></a></h3><p>For Bayesian model comparison, we use the marginal likelihood (also called evidence):</p><p class="math-container">\[p(Y | \mathcal{M}) = \int p(Y | \theta, \mathcal{M}) p(\theta | \mathcal{M}) \, d\theta\]</p><p>Models with higher marginal likelihood better balance fit and complexity.</p><hr/><h2 id="Covariance-Estimation"><a class="docs-heading-anchor" href="#Covariance-Estimation">Covariance Estimation</a><a id="Covariance-Estimation-1"></a><a class="docs-heading-anchor-permalink" href="#Covariance-Estimation" title="Permalink"></a></h2><h3 id="Newey-West-HAC-Estimator"><a class="docs-heading-anchor" href="#Newey-West-HAC-Estimator">Newey-West HAC Estimator</a><a id="Newey-West-HAC-Estimator-1"></a><a class="docs-heading-anchor-permalink" href="#Newey-West-HAC-Estimator" title="Permalink"></a></h3><p>For robust inference in the presence of heteroskedasticity and autocorrelation, we use the Newey-West (1987, 1994) estimator:</p><p class="math-container">\[\hat{V}_{NW} = (X&#39;X)^{-1} \hat{S} (X&#39;X)^{-1}\]</p><p>where the long-run covariance <span>$\hat{S}$</span> is:</p><p class="math-container">\[\hat{S} = \hat{\Gamma}_0 + \sum_{j=1}^{m} w_j (\hat{\Gamma}_j + \hat{\Gamma}_j&#39;)\]</p><p>with <span>$\hat{\Gamma}_j = \frac{1}{T} \sum_{t=j+1}^{T} \hat{u}_t \hat{u}_{t-j}&#39; x_t x_{t-j}&#39;$</span>.</p><h3 id="Kernel-Functions"><a class="docs-heading-anchor" href="#Kernel-Functions">Kernel Functions</a><a id="Kernel-Functions-1"></a><a class="docs-heading-anchor-permalink" href="#Kernel-Functions" title="Permalink"></a></h3><p>The weight function <span>$w_j$</span> depends on the kernel:</p><p><strong>Bartlett (Newey-West)</strong>:</p><p class="math-container">\[w_j = 1 - \frac{j}{m+1}\]</p><p><strong>Parzen</strong>:</p><p class="math-container">\[w_j = \begin{cases}
1 - 6x^2 + 6|x|^3 &amp; |x| \leq 0.5 \\
2(1-|x|)^3 &amp; 0.5 &lt; |x| \leq 1
\end{cases}\]</p><p>where <span>$x = j/(m+1)$</span>.</p><p><strong>Quadratic Spectral (Andrews, 1991)</strong>:</p><p class="math-container">\[w_j = \frac{25}{12\pi^2 x^2} \left( \frac{\sin(6\pi x/5)}{6\pi x/5} - \cos(6\pi x/5) \right)\]</p><h3 id="Automatic-Bandwidth-Selection"><a class="docs-heading-anchor" href="#Automatic-Bandwidth-Selection">Automatic Bandwidth Selection</a><a id="Automatic-Bandwidth-Selection-1"></a><a class="docs-heading-anchor-permalink" href="#Automatic-Bandwidth-Selection" title="Permalink"></a></h3><p>Newey &amp; West (1994) provide a data-driven bandwidth:</p><p class="math-container">\[m^* = 1.1447 \left( \hat{\alpha} \cdot T \right)^{1/3}\]</p><p>where <span>$\hat{\alpha}$</span> is estimated from an AR(1) fit to the residuals:</p><p class="math-container">\[\hat{\alpha} = \frac{4\hat{\rho}^2}{(1-\hat{\rho})^4}\]</p><p><strong>Reference</strong>: Newey &amp; West (1987, 1994), Andrews (1991)</p><hr/><h2 id="References"><a class="docs-heading-anchor" href="#References">References</a><a id="References-1"></a><a class="docs-heading-anchor-permalink" href="#References" title="Permalink"></a></h2><h3 id="Vector-Autoregression"><a class="docs-heading-anchor" href="#Vector-Autoregression">Vector Autoregression</a><a id="Vector-Autoregression-1"></a><a class="docs-heading-anchor-permalink" href="#Vector-Autoregression" title="Permalink"></a></h3><ul><li>Christiano, L. J., Eichenbaum, M., &amp; Evans, C. L. (1999). &quot;Monetary Policy Shocks: What Have We Learned and to What End?&quot; <em>Handbook of Macroeconomics</em>, 1, 65-148.</li><li>Hamilton, J. D. (1994). <em>Time Series Analysis</em>. Princeton University Press.</li><li>Lütkepohl, H. (2005). <em>New Introduction to Multiple Time Series Analysis</em>. Springer.</li><li>Sims, C. A. (1980). &quot;Macroeconomics and Reality.&quot; <em>Econometrica</em>, 48(1), 1-48.</li></ul><h3 id="Structural-Identification"><a class="docs-heading-anchor" href="#Structural-Identification">Structural Identification</a><a id="Structural-Identification-1"></a><a class="docs-heading-anchor-permalink" href="#Structural-Identification" title="Permalink"></a></h3><ul><li>Antolín-Díaz, J., &amp; Rubio-Ramírez, J. F. (2018). &quot;Narrative Sign Restrictions for SVARs.&quot; <em>American Economic Review</em>, 108(10), 2802-2829.</li><li>Blanchard, O. J., &amp; Quah, D. (1989). &quot;The Dynamic Effects of Aggregate Demand and Supply Disturbances.&quot; <em>American Economic Review</em>, 79(4), 655-673.</li><li>Faust, J. (1998). &quot;The Robustness of Identified VAR Conclusions about Money.&quot; <em>Carnegie-Rochester Conference Series on Public Policy</em>, 49, 207-244.</li><li>Kilian, L., &amp; Lütkepohl, H. (2017). <em>Structural Vector Autoregressive Analysis</em>. Cambridge University Press.</li><li>Rubio-Ramírez, J. F., Waggoner, D. F., &amp; Zha, T. (2010). &quot;Structural Vector Autoregressions: Theory of Identification and Algorithms for Inference.&quot; <em>Review of Economic Studies</em>, 77(2), 665-696.</li><li>Uhlig, H. (2005). &quot;What Are the Effects of Monetary Policy on Output? Results from an Agnostic Identification Procedure.&quot; <em>Journal of Monetary Economics</em>, 52(2), 381-419.</li></ul><h3 id="Bayesian-Methods"><a class="docs-heading-anchor" href="#Bayesian-Methods">Bayesian Methods</a><a id="Bayesian-Methods-1"></a><a class="docs-heading-anchor-permalink" href="#Bayesian-Methods" title="Permalink"></a></h3><ul><li>Bańbura, M., Giannone, D., &amp; Reichlin, L. (2010). &quot;Large Bayesian Vector Auto Regressions.&quot; <em>Journal of Applied Econometrics</em>, 25(1), 71-92.</li><li>Carriero, A., Clark, T. E., &amp; Marcellino, M. (2015). &quot;Bayesian VARs: Specification Choices and Forecast Accuracy.&quot; <em>Journal of Applied Econometrics</em>, 30(1), 46-73.</li><li>Doan, T., Litterman, R., &amp; Sims, C. (1984). &quot;Forecasting and Conditional Projection Using Realistic Prior Distributions.&quot; <em>Econometric Reviews</em>, 3(1), 1-100.</li><li>Giannone, D., Lenza, M., &amp; Primiceri, G. E. (2015). &quot;Prior Selection for Vector Autoregressions.&quot; <em>Review of Economics and Statistics</em>, 97(2), 436-451.</li><li>Kadiyala, K. R., &amp; Karlsson, S. (1997). &quot;Numerical Methods for Estimation and Inference in Bayesian VAR-Models.&quot; <em>Journal of Applied Econometrics</em>, 12(2), 99-132.</li><li>Litterman, R. B. (1986). &quot;Forecasting with Bayesian Vector Autoregressions—Five Years of Experience.&quot; <em>Journal of Business &amp; Economic Statistics</em>, 4(1), 25-38.</li></ul><h3 id="Inference"><a class="docs-heading-anchor" href="#Inference">Inference</a><a id="Inference-1"></a><a class="docs-heading-anchor-permalink" href="#Inference" title="Permalink"></a></h3><ul><li>Andrews, D. W. K. (1991). &quot;Heteroskedasticity and Autocorrelation Consistent Covariance Matrix Estimation.&quot; <em>Econometrica</em>, 59(3), 817-858.</li><li>Gelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A., &amp; Rubin, D. B. (2013). <em>Bayesian Data Analysis</em> (3rd ed.). CRC Press.</li><li>Hoffman, M. D., &amp; Gelman, A. (2014). &quot;The No-U-Turn Sampler: Adaptively Setting Path Lengths in Hamiltonian Monte Carlo.&quot; <em>Journal of Machine Learning Research</em>, 15(1), 1593-1623.</li><li>Kilian, L. (1998). &quot;Small-Sample Confidence Intervals for Impulse Response Functions.&quot; <em>Review of Economics and Statistics</em>, 80(2), 218-230.</li><li>Newey, W. K., &amp; West, K. D. (1987). &quot;A Simple, Positive Semi-Definite, Heteroskedasticity and Autocorrelation Consistent Covariance Matrix.&quot; <em>Econometrica</em>, 55(3), 703-708.</li><li>Newey, W. K., &amp; West, K. D. (1994). &quot;Automatic Lag Selection in Covariance Matrix Estimation.&quot; <em>Review of Economic Studies</em>, 61(4), 631-653.</li></ul></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../">« Home</a><a class="docs-footer-nextpage" href="../lp/">Local Projections »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.16.1 on <span class="colophon-date" title="Saturday 31 January 2026 08:33">Saturday 31 January 2026</span>. Using Julia version 1.12.4.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
