var documenterSearchIndex = {"docs":
[{"location":"volatility/#Volatility-Models","page":"Volatility Models","title":"Volatility Models","text":"This page covers univariate volatility modeling: ARCH, GARCH (including EGARCH and GJR-GARCH), and Stochastic Volatility (SV) models. These models capture time-varying conditional variance — a pervasive feature of financial and macroeconomic time series.","category":"section"},{"location":"volatility/#Quick-Start","page":"Volatility Models","title":"Quick Start","text":"using MacroEconometricModels\n\ny = randn(500)  # Replace with your returns data\n\n# ARCH(5) — Engle (1982)\narch = estimate_arch(y, 5)\n\n# GARCH(1,1) — Bollerslev (1986)\ngarch = estimate_garch(y, 1, 1)\n\n# EGARCH(1,1) — Nelson (1991)\negarch = estimate_egarch(y, 1, 1)\n\n# GJR-GARCH(1,1) — Glosten, Jagannathan & Runkle (1993)\ngjr = estimate_gjr_garch(y, 1, 1)\n\n# Stochastic Volatility — Taylor (1986)\nsv = estimate_sv(y; n_samples=2000, n_adapts=1000)\n\n# Diagnostics\narch_lm_test(y, 5)         # ARCH-LM test\nljung_box_squared(y, 10)   # Ljung-Box on squared residuals\nnic = news_impact_curve(garch)  # News impact curve\n\n# Forecast 20 steps ahead\nfc = forecast(garch, 20; conf_level=0.95)\n\n","category":"section"},{"location":"volatility/#ARCH-Models","page":"Volatility Models","title":"ARCH Models","text":"","category":"section"},{"location":"volatility/#Theory","page":"Volatility Models","title":"Theory","text":"The Autoregressive Conditional Heteroskedasticity (ARCH) model of Engle (1982) allows the conditional variance to depend on past squared innovations. The ARCH(q) specification is:\n\ny_t = mu + varepsilon_t qquad varepsilon_t = sigma_t z_t qquad z_t sim mathcalN(0 1)\n\nsigma^2_t = omega + sum_i=1^q alpha_i varepsilon^2_t-i\n\nwhere\n\ny_t is the observed time series\nmu is the conditional mean (intercept)\nvarepsilon_t is the mean-corrected innovation\nsigma^2_t is the conditional variance at time t\nomega  0 is the variance intercept\nalpha_i geq 0 are the ARCH coefficients\nz_t is a standardized innovation\n\nStationarity condition: The ARCH(q) process is covariance stationary if sum_i=1^q alpha_i  1.\n\nUnconditional variance: Under stationarity, textVar(varepsilon_t) = omega  (1 - sum_i=1^q alpha_i).","category":"section"},{"location":"volatility/#Estimation","page":"Volatility Models","title":"Estimation","text":"ARCH models are estimated by maximum likelihood (MLE) using two-stage optimization:\n\nStage 1 (NelderMead): Derivative-free search to find a good starting region.\nStage 2 (L-BFGS): Gradient-based refinement from the Stage 1 solution.\n\nParameters are log-transformed internally to enforce positivity constraints (omega  0, alpha_i geq 0) without constrained optimization.\n\n# Estimate ARCH(5) model\narch = estimate_arch(y, 5)\n\n# Access estimated parameters\narch.omega      # Variance intercept\narch.alpha      # ARCH coefficients [α₁, ..., α₅]\narch.mu         # Mean\narch.loglik     # Log-likelihood","category":"section"},{"location":"volatility/#Diagnostics","page":"Volatility Models","title":"Diagnostics","text":"Two diagnostic tests check whether ARCH effects have been adequately captured:\n\nARCH-LM Test (Engle 1982): Tests for remaining ARCH effects in model residuals or raw data.\n\n# Test raw data for ARCH effects (H₀: no ARCH effects)\nstat, pval, q = arch_lm_test(y, 5)\n\n# Test standardized residuals after fitting (should fail to reject)\nstat, pval, q = arch_lm_test(arch, 5)\n\nThe test regresses squared residuals on q of their own lags and computes TR^2 sim chi^2(q). Rejection of H_0 indicates ARCH effects are present (or remain after fitting).\n\nLjung-Box Test on Squared Residuals: Tests for serial correlation in z_t^2.\n\nstat, pval, K = ljung_box_squared(arch, 10)\n\nThe test statistic is Q = n(n+2) sum_k=1^K hatrho^2_k  (n - k) sim chi^2(K), where hatrho_k is the sample autocorrelation of squared standardized residuals at lag k. Failure to reject indicates the model has adequately captured the variance dynamics.","category":"section"},{"location":"volatility/#ARCHModel-Return-Values","page":"Volatility Models","title":"ARCHModel Return Values","text":"Field Type Description\ny Vector{T} Original data\nq Int ARCH order\nmu T Estimated mean (intercept)\nomega T Variance intercept omega\nalpha Vector{T} ARCH coefficients alpha_1 ldots alpha_q\nconditional_variance Vector{T} Estimated hatsigma^2_t at each t\nstandardized_residuals Vector{T} hatz_t = hatvarepsilon_t  hatsigma_t\nresiduals Vector{T} Raw residuals hatvarepsilon_t = y_t - hatmu\nfitted Vector{T} Fitted values (mean)\nloglik T Maximized log-likelihood\naic T Akaike Information Criterion\nbic T Bayesian Information Criterion\nmethod Symbol Estimation method (:mle)\nconverged Bool Whether optimization converged\niterations Int Number of optimizer iterations\n\n","category":"section"},{"location":"volatility/#GARCH-Models","page":"Volatility Models","title":"GARCH Models","text":"","category":"section"},{"location":"volatility/#GARCH(p,q)-—-Bollerslev-(1986)","page":"Volatility Models","title":"GARCH(p,q) — Bollerslev (1986)","text":"The Generalized ARCH model extends ARCH by including lagged conditional variances:\n\nsigma^2_t = omega + sum_i=1^q alpha_i varepsilon^2_t-i + sum_j=1^p beta_j sigma^2_t-j\n\nwhere\n\nomega  0 is the variance intercept\nalpha_i geq 0 are the ARCH coefficients (impact of past shocks)\nbeta_j geq 0 are the GARCH coefficients (variance persistence)\np is the GARCH order (lagged variances) and q is the ARCH order (lagged squared residuals)\n\nStationarity condition: sum_i=1^q alpha_i + sum_j=1^p beta_j  1.\n\nUnconditional variance: sigma^2 = omega  (1 - sum alpha_i - sum beta_j).\n\nThe GARCH(1,1) is the most widely used specification in practice, capturing the key empirical regularity of volatility clustering with just three parameters.\n\n# Estimate GARCH(1,1) — the workhorse specification\ngarch = estimate_garch(y, 1, 1)\n\n# Persistence: how quickly volatility reverts to its long-run level\npersistence(garch)              # α₁ + β₁ (close to 1 = slow reversion)\nhalflife(garch)                 # Half-life in periods\nunconditional_variance(garch)   # Long-run variance level","category":"section"},{"location":"volatility/#EGARCH(p,q)-—-Nelson-(1991)","page":"Volatility Models","title":"EGARCH(p,q) — Nelson (1991)","text":"The Exponential GARCH models the log of conditional variance, ensuring positivity without parameter constraints and allowing asymmetric responses to positive and negative shocks:\n\nlog(sigma^2_t) = omega + sum_i=1^q alpha_i (z_t-i - mathbbEz) + sum_i=1^q gamma_i z_t-i + sum_j=1^p beta_j log(sigma^2_t-j)\n\nwhere\n\nz_t = varepsilon_t  sigma_t are standardized residuals\nalpha_i captures the magnitude (symmetric) effect of shocks\ngamma_i captures the sign (asymmetric/leverage) effect — typically gamma_i  0 means negative shocks increase volatility more than positive shocks of equal magnitude\nbeta_j governs persistence of log-variance\nmathbbEz = sqrt2pi for standard normal innovations\n\nStationarity condition: sum_j=1^p beta_j  1 (in log-variance, unconditional parameters).\n\nUnconditional variance: sigma^2 = exp(omega  (1 - sum beta_j)).\n\n# Estimate EGARCH(1,1)\negarch = estimate_egarch(y, 1, 1)\n\n# Leverage parameters (γ < 0 → \"leverage effect\")\negarch.gamma    # Leverage coefficients","category":"section"},{"location":"volatility/#GJR-GARCH(p,q)-—-Glosten,-Jagannathan-and-Runkle-(1993)","page":"Volatility Models","title":"GJR-GARCH(p,q) — Glosten, Jagannathan & Runkle (1993)","text":"The GJR-GARCH (also called Threshold GARCH) adds an indicator function for negative shocks:\n\nsigma^2_t = omega + sum_i=1^q (alpha_i + gamma_i mathbb1(varepsilon_t-i  0)) varepsilon^2_t-i + sum_j=1^p beta_j sigma^2_t-j\n\nwhere\n\ngamma_i geq 0 are leverage parameters\nmathbb1(varepsilon_t-i  0) = 1 when past shocks are negative\n\nWhen gamma_i  0, negative shocks have a larger impact on future variance than positive shocks of equal magnitude. This captures the empirical \"leverage effect\" first documented by Black (1976): stock price declines increase financial leverage, which in turn increases equity volatility.\n\nStationarity condition: sum alpha_i + sum gamma_i  2 + sum beta_j  1.\n\nUnconditional variance: sigma^2 = omega  (1 - sum alpha_i - sum gamma_i  2 - sum beta_j).\n\n# Estimate GJR-GARCH(1,1)\ngjr = estimate_gjr_garch(y, 1, 1)\n\n# Leverage effect: γ > 0 means negative shocks increase variance more\ngjr.gamma    # Leverage coefficients","category":"section"},{"location":"volatility/#News-Impact-Curve","page":"Volatility Models","title":"News Impact Curve","text":"The news impact curve (NIC) shows how a shock varepsilon_t-1 maps to the next-period conditional variance sigma^2_t, holding all other information constant at the unconditional level. For symmetric models (ARCH, GARCH), the NIC is a parabola centered at zero. For asymmetric models (EGARCH, GJR-GARCH), the NIC is steeper for negative shocks.\n\n# Compute news impact curves\nnic_garch  = news_impact_curve(garch)\nnic_egarch = news_impact_curve(egarch; range=(-3.0, 3.0), n_points=200)\nnic_gjr    = news_impact_curve(gjr)\n\n# Returns named tuple: (shocks=Vector, variance=Vector)\nnic_garch.shocks     # Grid of εₜ₋₁ values\nnic_garch.variance   # Corresponding σ²ₜ values\n\nComparing news impact curves across models reveals whether asymmetric specifications (EGARCH, GJR-GARCH) capture economically important leverage effects that symmetric GARCH misses. If the NIC from GARCH and GJR-GARCH are nearly identical, the leverage effect is negligible and the simpler symmetric model suffices.","category":"section"},{"location":"volatility/#GARCH-Family-Return-Values","page":"Volatility Models","title":"GARCH-Family Return Values","text":"GARCHModel Fields\n\nField Type Description\ny Vector{T} Original data\np Int GARCH order (lagged variances)\nq Int ARCH order (lagged squared residuals)\nmu T Estimated mean\nomega T Variance intercept omega\nalpha Vector{T} ARCH coefficients alpha_1 ldots alpha_q\nbeta Vector{T} GARCH coefficients beta_1 ldots beta_p\nconditional_variance Vector{T} Estimated hatsigma^2_t\nstandardized_residuals Vector{T} hatz_t\nresiduals Vector{T} hatvarepsilon_t\nfitted Vector{T} Fitted values\nloglik T Log-likelihood\naic T AIC\nbic T BIC\nmethod Symbol Estimation method\nconverged Bool Convergence status\niterations Int Optimizer iterations\n\nEGARCHModel Fields\n\nField Type Description\ny Vector{T} Original data\np Int Log-variance persistence order\nq Int Shock order\nmu T Estimated mean\nomega T Log-variance intercept\nalpha Vector{T} Magnitude (symmetric) parameters\ngamma Vector{T} Leverage (asymmetric) parameters\nbeta Vector{T} Log-variance persistence parameters\nconditional_variance Vector{T} hatsigma^2_t\nstandardized_residuals Vector{T} hatz_t\nresiduals Vector{T} hatvarepsilon_t\nfitted Vector{T} Fitted values\nloglik T Log-likelihood\naic T AIC\nbic T BIC\nmethod Symbol Estimation method\nconverged Bool Convergence status\niterations Int Optimizer iterations\n\nGJRGARCHModel Fields\n\nField Type Description\ny Vector{T} Original data\np Int GARCH order\nq Int ARCH order\nmu T Estimated mean\nomega T Variance intercept omega\nalpha Vector{T} Symmetric ARCH coefficients\ngamma Vector{T} Leverage parameters gamma_1 ldots gamma_q\nbeta Vector{T} GARCH coefficients\nconditional_variance Vector{T} hatsigma^2_t\nstandardized_residuals Vector{T} hatz_t\nresiduals Vector{T} hatvarepsilon_t\nfitted Vector{T} Fitted values\nloglik T Log-likelihood\naic T AIC\nbic T BIC\nmethod Symbol Estimation method\nconverged Bool Convergence status\niterations Int Optimizer iterations\n\n","category":"section"},{"location":"volatility/#Stochastic-Volatility","page":"Volatility Models","title":"Stochastic Volatility","text":"","category":"section"},{"location":"volatility/#Theory-2","page":"Volatility Models","title":"Theory","text":"The stochastic volatility (SV) model of Taylor (1986) treats the log-variance as a latent autoregressive process:\n\ny_t = exp(h_t  2)  varepsilon_t qquad varepsilon_t sim mathcalN(0 1)\n\nh_t = mu + varphi (h_t-1 - mu) + sigma_eta eta_t qquad eta_t sim mathcalN(0 1)\n\nwhere\n\nh_t is the log-variance at time t\nmu is the log-variance level (unconditional mean of h_t)\nvarphi in (-1 1) is the persistence parameter\nsigma_eta  0 is the volatility of volatility\nvarepsilon_t and eta_t are independent standard normal innovations\n\nThe SV model differs fundamentally from GARCH in that volatility has its own independent source of randomness (eta_t), making it a state-space model with a non-Gaussian observation equation. This provides greater flexibility in capturing empirical volatility dynamics, but precludes closed-form likelihood evaluation.","category":"section"},{"location":"volatility/#Variants","page":"Volatility Models","title":"Variants","text":"Three SV variants are available:\n\nBasic SV (leverage=false, dist=:normal): The standard specification above.\n\nSV with Leverage (leverage=true): Allows correlation between return and volatility innovations:\n\nbeginpmatrix varepsilon_t  eta_t endpmatrix sim mathcalNleft(mathbf0 beginpmatrix 1  rho  rho  1 endpmatrixright)\n\nWhen rho  0 (the typical case for equities), negative returns are associated with increases in volatility, analogous to the leverage effect in EGARCH and GJR-GARCH models.\n\nSV with Student-t Errors (dist=:studentt): Replaces the Gaussian observation equation with Student-t innovations to accommodate heavier tails:\n\ny_t = exp(h_t  2)  varepsilon_t qquad varepsilon_t sim t_nu\n\nwhere nu  2 is the degrees of freedom parameter.","category":"section"},{"location":"volatility/#Priors","page":"Volatility Models","title":"Priors","text":"The SV model is estimated via Bayesian MCMC using Turing.jl. The default priors are:\n\nParameter Prior Rationale\nmu mathcalN(0 10) Weakly informative for log-variance level\nvarphi textBeta(20 15) to (-1 1) Concentrates mass near 1 (high persistence), ensures stationarity\nsigma_eta textHalfNormal(1) Positive, moderately informative for vol-of-vol\nrho (leverage) textUniform(-1 1) Uninformative over correlation range\nnu (Student-t) textExponential(01) + 2 Ensures nu  2 (finite variance)","category":"section"},{"location":"volatility/#MCMC-Estimation","page":"Volatility Models","title":"MCMC Estimation","text":"# Basic SV model\nsv = estimate_sv(y; n_samples=2000, n_adapts=1000)\n\n# SV with leverage effect\nsv_lev = estimate_sv(y; leverage=true, n_samples=2000, n_adapts=1000)\n\n# SV with Student-t errors\nsv_t = estimate_sv(y; dist=:studentt, n_samples=2000, n_adapts=1000)\n\n# Access posterior summaries\nmean(sv.mu_post)          # Posterior mean of μ\nmean(sv.phi_post)         # Posterior mean of φ\nmean(sv.sigma_eta_post)   # Posterior mean of σ_η\n\n# Posterior volatility (time series)\nsv.volatility_mean        # Posterior mean of exp(hₜ) at each t\nsv.volatility_quantiles   # Quantiles (T × n_quantiles matrix)\nsv.quantile_levels        # Default: [0.025, 0.5, 0.975]\n\n# Full MCMC chain for convergence diagnostics\nsv.chain                  # MCMCChains.Chains object\n\nnote: Technical Note\nMCMC estimation of SV models is computationally intensive because the latent log-volatility states h_1 ldots h_T must be sampled jointly with the model parameters. The NUTS sampler (No-U-Turn Sampler) is used by default for efficient exploration of the posterior. Typical run times are 1–3 minutes for T = 500 with 2000 posterior draws.","category":"section"},{"location":"volatility/#SVModel-Return-Values","page":"Volatility Models","title":"SVModel Return Values","text":"Field Type Description\ny Vector{T} Original data\nchain Chains Full MCMC chain (Turing.jl)\nmu_post Vector{T} Posterior draws of mu\nphi_post Vector{T} Posterior draws of varphi\nsigma_eta_post Vector{T} Posterior draws of sigma_eta\nvolatility_mean Vector{T} Posterior mean of exp(h_t) at each t\nvolatility_quantiles Matrix{T} T times n_q quantiles of exp(h_t)\nquantile_levels Vector{T} Quantile levels (e.g., 0025 05 0975)\ndist Symbol Error distribution (:normal or :studentt)\nleverage Bool Whether leverage effect was estimated\nn_samples Int Number of posterior samples\n\n","category":"section"},{"location":"volatility/#Volatility-Forecasting","page":"Volatility Models","title":"Volatility Forecasting","text":"All volatility models support multi-step ahead forecasting via forecast(). ARCH and GARCH-family models use simulation-based confidence intervals; SV models use posterior predictive simulation from MCMC draws.","category":"section"},{"location":"volatility/#ARCH/GARCH-Forecasts","page":"Volatility Models","title":"ARCH/GARCH Forecasts","text":"# Forecast 20 steps ahead\nfc = forecast(garch, 20; conf_level=0.95, n_sim=10000)\n\n# Point forecasts converge to unconditional variance\nfc.forecast     # Vector of length 20\nfc.ci_lower     # Lower CI bound\nfc.ci_upper     # Upper CI bound\nfc.se           # Standard errors\n\n# Compare unconditional variance with long-horizon forecast\nunconditional_variance(garch)\nfc.forecast[end]  # Should be close for large h\n\nFor stationary GARCH processes, multi-step forecasts converge geometrically to the unconditional variance at rate equal to the persistence parameter. The speed of convergence is measured by the half-life: texthalflife = log(05)  log(textpersistence).\n\nConfidence intervals are constructed by simulating n paths forward from the last observed state, generating the distribution of future conditional variances. For ARCH models, forecasts beyond horizon q equal the unconditional variance exactly (no lagged variance terms to propagate).","category":"section"},{"location":"volatility/#SV-Forecasts","page":"Volatility Models","title":"SV Forecasts","text":"# Posterior predictive forecast from SV model\nfc_sv = forecast(sv, 20; conf_level=0.95)\n\nfc_sv.forecast     # Posterior mean forecast\nfc_sv.ci_lower     # 2.5th percentile\nfc_sv.ci_upper     # 97.5th percentile\n\nFor SV models, each MCMC draw provides a full parameter vector (mu varphi sigma_eta). The forecast simulates the log-volatility process forward from the last state for each draw, yielding a posterior predictive distribution of future volatility. The reported intervals are posterior predictive quantiles, not frequentist confidence intervals.","category":"section"},{"location":"volatility/#VolatilityForecast-Return-Values","page":"Volatility Models","title":"VolatilityForecast Return Values","text":"Field Type Description\nforecast Vector{T} Point forecasts of conditional variance hatsigma^2_T+h\nci_lower Vector{T} Lower confidence/credible interval bound\nci_upper Vector{T} Upper confidence/credible interval bound\nse Vector{T} Standard errors of forecasts\nhorizon Int Forecast horizon\nconf_level T Confidence level (e.g., 0.95)\nmodel_type Symbol Source model (:arch, :garch, :egarch, :gjr_garch, :sv)\n\n","category":"section"},{"location":"volatility/#Type-Accessors","page":"Volatility Models","title":"Type Accessors","text":"The following accessor functions provide model-specific summary statistics. The formulas differ across model types:\n\nFunction ARCH GARCH EGARCH GJR-GARCH SV\npersistence(m) sum alpha_i sum alpha_i + sum beta_j sum beta_j sum alpha_i + sum gamma_i2 + sum beta_j mathbbEvarphi\nhalflife(m) log(05)log(p) log(05)log(p) log(05)log(p) log(05)log(p) log(05)log(p)\nunconditional_variance(m) fracomega1 - sum alpha_i fracomega1 - sum alpha_i - sum beta_j expleft(fracomega1 - sum beta_jright) fracomega1 - sum alpha_i - sum gamma_i2 - sum beta_j exp(mathbbEmu)\narch_order(m) q q q q —\ngarch_order(m) — p p p —\n\nIn the table, p denotes persistence(m). The half-life returns Inf if the process is non-stationary (persistence geq 1).\n\npersistence(garch)              # 0.95 → high persistence\nhalflife(garch)                 # ≈ 13.5 periods\nunconditional_variance(garch)   # Long-run variance\narch_order(garch)               # q\ngarch_order(garch)              # p\n\n","category":"section"},{"location":"volatility/#StatsAPI-Interface","page":"Volatility Models","title":"StatsAPI Interface","text":"All volatility models implement the standard StatsAPI interface:\n\nFunction Description\nnobs(m) Number of observations\ncoef(m) Coefficient vector\nresiduals(m) Raw residuals hatvarepsilon_t\npredict(m) Conditional variance series hatsigma^2_t (or posterior mean for SV)\nloglikelihood(m) Maximized log-likelihood (ARCH/GARCH)\naic(m) Akaike Information Criterion\nbic(m) Bayesian Information Criterion\ndof(m) Number of estimated parameters\nislinear(m) false (all volatility models are nonlinear)\n\nnobs(garch)          # Number of observations\nloglikelihood(garch) # Maximized log-likelihood\naic(garch)           # AIC for model comparison\nbic(garch)           # BIC for model comparison\ncoef(garch)          # [μ, ω, α₁, ..., αq, β₁, ..., βp]\n\n","category":"section"},{"location":"volatility/#Complete-Example","page":"Volatility Models","title":"Complete Example","text":"This example estimates all four GARCH-family models on the same data, compares their news impact curves and forecasts, runs diagnostics, and estimates an SV model for comparison.\n\nusing MacroEconometricModels\nusing Random\nusing Statistics\n\nRandom.seed!(42)\n\n# === Generate GARCH(1,1) data with leverage ===\nT = 1000\ny = zeros(T)\nh = zeros(T)\nh[1] = 1.0\n\nfor t in 2:T\n    z = randn()\n    h[t] = 0.01 + 0.08 * y[t-1]^2 + 0.12 * (y[t-1] < 0 ? 1 : 0) * y[t-1]^2 + 0.85 * h[t-1]\n    y[t] = sqrt(h[t]) * z\nend\n\nprintln(\"Simulated T=$T observations from GJR-GARCH(1,1)\")\nprintln(\"Sample kurtosis: \", round(kurtosis(y), digits=2))\n\n# === Step 1: Test for ARCH effects ===\nstat, pval, q = arch_lm_test(y, 5)\nprintln(\"\\nARCH-LM test (q=5):\")\nprintln(\"  Statistic: \", round(stat, digits=2))\nprintln(\"  P-value: \", round(pval, digits=6))\n\nstat2, pval2, K = ljung_box_squared(y, 10)\nprintln(\"\\nLjung-Box squared (K=10):\")\nprintln(\"  Statistic: \", round(stat2, digits=2))\nprintln(\"  P-value: \", round(pval2, digits=6))\n\n# === Step 2: Estimate competing models ===\ngarch   = estimate_garch(y, 1, 1)\negarch  = estimate_egarch(y, 1, 1)\ngjr     = estimate_gjr_garch(y, 1, 1)\n\nprintln(\"\\n\" * \"=\"^60)\nprintln(\"Model Comparison\")\nprintln(\"=\"^60)\nprintln(\"              AIC         BIC     Persistence\")\nprintln(\"  GARCH:   \", round(aic(garch), digits=1),\n        \"    \", round(bic(garch), digits=1),\n        \"    \", round(persistence(garch), digits=4))\nprintln(\"  EGARCH:  \", round(aic(egarch), digits=1),\n        \"    \", round(bic(egarch), digits=1),\n        \"    \", round(persistence(egarch), digits=4))\nprintln(\"  GJR:     \", round(aic(gjr), digits=1),\n        \"    \", round(bic(gjr), digits=1),\n        \"    \", round(persistence(gjr), digits=4))\n\n# === Step 3: Compare news impact curves ===\nnic_g  = news_impact_curve(garch)\nnic_e  = news_impact_curve(egarch)\nnic_j  = news_impact_curve(gjr)\n\nprintln(\"\\nNews Impact at ε = -2 vs ε = +2:\")\nidx_neg = findfirst(x -> x >= -2.0, nic_g.shocks)\nidx_pos = findfirst(x -> x >= 2.0, nic_g.shocks)\n\nprintln(\"  GARCH:  σ²(-2) = \", round(nic_g.variance[idx_neg], digits=4),\n        \"   σ²(+2) = \", round(nic_g.variance[idx_pos], digits=4))\nprintln(\"  EGARCH: σ²(-2) = \", round(nic_e.variance[idx_neg], digits=4),\n        \"   σ²(+2) = \", round(nic_e.variance[idx_pos], digits=4))\nprintln(\"  GJR:    σ²(-2) = \", round(nic_j.variance[idx_neg], digits=4),\n        \"   σ²(+2) = \", round(nic_j.variance[idx_pos], digits=4))\n\n# === Step 4: Check residual diagnostics ===\nprintln(\"\\nResidual ARCH-LM test (q=5):\")\nfor (name, m) in [(\"GARCH\", garch), (\"EGARCH\", egarch), (\"GJR\", gjr)]\n    _, p, _ = arch_lm_test(m, 5)\n    status = p > 0.05 ? \"Pass\" : \"FAIL\"\n    println(\"  $name: p=$(round(p, digits=4))  $status\")\nend\n\n# === Step 5: Forecast volatility ===\nH = 20\nfc_g = forecast(garch, H)\nfc_e = forecast(egarch, H)\nfc_j = forecast(gjr, H)\n\nprintln(\"\\nVolatility forecasts (conditional variance):\")\nprintln(\"  h    GARCH    EGARCH   GJR      Uncond\")\nfor h_idx in [1, 5, 10, 20]\n    println(\"  $h_idx    \",\n            round(fc_g.forecast[h_idx], digits=4), \"  \",\n            round(fc_e.forecast[h_idx], digits=4), \"  \",\n            round(fc_j.forecast[h_idx], digits=4), \"  \",\n            round(unconditional_variance(garch), digits=4))\nend\n\n# === Step 6: Stochastic volatility for comparison ===\nprintln(\"\\nEstimating SV model via MCMC...\")\nsv = estimate_sv(y; n_samples=2000, n_adapts=1000)\n\nprintln(\"SV posterior summary:\")\nprintln(\"  μ (log-vol level):   \", round(mean(sv.mu_post), digits=3),\n        \" [\", round(quantile(sv.mu_post, 0.025), digits=3),\n        \", \", round(quantile(sv.mu_post, 0.975), digits=3), \"]\")\nprintln(\"  φ (persistence):    \", round(mean(sv.phi_post), digits=3),\n        \" [\", round(quantile(sv.phi_post, 0.025), digits=3),\n        \", \", round(quantile(sv.phi_post, 0.975), digits=3), \"]\")\nprintln(\"  σ_η (vol of vol):   \", round(mean(sv.sigma_eta_post), digits=3),\n        \" [\", round(quantile(sv.sigma_eta_post, 0.025), digits=3),\n        \", \", round(quantile(sv.sigma_eta_post, 0.975), digits=3), \"]\")\n\n# SV forecast\nfc_sv = forecast(sv, H)\nprintln(\"\\nSV forecast at h=1: \", round(fc_sv.forecast[1], digits=4))\nprintln(\"SV forecast at h=20: \", round(fc_sv.forecast[end], digits=4))\n\nIn this example, the GJR-GARCH model should provide the best fit (lowest AIC/BIC) since the data was generated from a GJR-GARCH DGP with a leverage effect. The news impact curves reveal the asymmetry: for EGARCH and GJR-GARCH, sigma^2(-2) exceeds sigma^2(+2); for symmetric GARCH, they are equal. All models' standardized residuals should pass the ARCH-LM test after fitting, confirming that the conditional variance dynamics are adequately captured. The SV model provides an independent, Bayesian assessment of the volatility dynamics.\n\n","category":"section"},{"location":"volatility/#References","page":"Volatility Models","title":"References","text":"Black, Fischer. 1976. \"Studies of Stock Price Volatility Changes.\" Proceedings of the 1976 Meetings of the American Statistical Association, 171–177.\nBollerslev, Tim. 1986. \"Generalized Autoregressive Conditional Heteroskedasticity.\" Journal of Econometrics 31 (3): 307–327. https://doi.org/10.1016/0304-4076(86)90063-1\nEngle, Robert F. 1982. \"Autoregressive Conditional Heteroscedasticity with Estimates of the Variance of United Kingdom Inflation.\" Econometrica 50 (4): 987–1007. https://doi.org/10.2307/1912773\nGlosten, Lawrence R., Ravi Jagannathan, and David E. Runkle. 1993. \"On the Relation between the Expected Value and the Volatility of the Nominal Excess Return on Stocks.\" Journal of Finance 48 (5): 1779–1801. https://doi.org/10.1111/j.1540-6261.1993.tb05128.x\nNelson, Daniel B. 1991. \"Conditional Heteroskedasticity in Asset Returns: A New Approach.\" Econometrica 59 (2): 347–370. https://doi.org/10.2307/2938260\nTaylor, Stephen J. 1986. Modelling Financial Time Series. Chichester: Wiley. ISBN 978-0-471-90975-7.","category":"section"},{"location":"bayesian/#Bayesian-VAR-(BVAR)","page":"Bayesian VAR","title":"Bayesian VAR (BVAR)","text":"This chapter covers Bayesian estimation methods for Vector Autoregression models, including the Minnesota prior, hyperparameter optimization, and MCMC inference.","category":"section"},{"location":"bayesian/#Introduction","page":"Bayesian VAR","title":"Introduction","text":"Bayesian VAR (BVAR) estimation addresses the curse of dimensionality in VAR models by incorporating prior information to shrink coefficient estimates. This is particularly valuable when:\n\nThe number of parameters is large relative to sample size\nPrior economic knowledge should influence estimation\nUncertainty quantification via posterior distributions is desired\nForecasting performance is paramount\n\nKey References: Litterman (1986), Doan, Litterman & Sims (1984), Giannone, Lenza & Primiceri (2015)","category":"section"},{"location":"bayesian/#Quick-Start","page":"Bayesian VAR","title":"Quick Start","text":"hyper = MinnesotaHyperparameters(tau=0.5, decay=2.0, lambda=1.0, mu=1.0, omega=1.0)\nbest = optimize_hyperparameters(Y, p; grid_size=20)                 # Optimize tau\nchain = estimate_bvar(Y, 2; n_samples=2000, prior=:minnesota, hyper=best)\nbirf = irf(chain, 2, 3, 20; method=:cholesky)                      # Bayesian IRF\nbfevd = fevd(chain, 2, 3, 20)                                      # Bayesian FEVD\n\n","category":"section"},{"location":"bayesian/#Bayesian-Framework","page":"Bayesian VAR","title":"Bayesian Framework","text":"","category":"section"},{"location":"bayesian/#The-Prior-Likelihood-Posterior-Paradigm","page":"Bayesian VAR","title":"The Prior-Likelihood-Posterior Paradigm","text":"In the Bayesian approach, we treat the VAR parameters as random variables and update our beliefs using Bayes' theorem:\n\np(B Sigma  Y) propto p(Y  B Sigma) cdot p(B Sigma)\n\nwhere:\n\np(Y  B Sigma) is the likelihood\np(B Sigma) is the prior\np(B Sigma  Y) is the posterior","category":"section"},{"location":"bayesian/#Natural-Conjugate-Prior","page":"Bayesian VAR","title":"Natural Conjugate Prior","text":"For computational convenience, we use the Normal-Inverse-Wishart conjugate prior:\n\nSigma sim textIW(nu_0 S_0)\n\ntextvec(B)  Sigma sim N(textvec(B_0) Sigma otimes Omega_0)\n\nThis yields a closed-form posterior of the same family.\n\n","category":"section"},{"location":"bayesian/#The-Minnesota-Prior","page":"Bayesian VAR","title":"The Minnesota Prior","text":"","category":"section"},{"location":"bayesian/#Motivation","page":"Bayesian VAR","title":"Motivation","text":"The Minnesota prior (Litterman, 1986; Doan, Litterman & Sims, 1984) shrinks VAR coefficients toward a random walk prior. This reflects the empirical observation that many macroeconomic variables are well-approximated by random walks, especially at short horizons.","category":"section"},{"location":"bayesian/#Prior-Specification","page":"Bayesian VAR","title":"Prior Specification","text":"Prior Mean: Each variable follows a random walk:\n\nEA_1ii = 1 quad EA_1ij = 0 text for  i neq j quad EA_l = 0 text for  l  1\n\nPrior Variance: The prior variance for coefficient (ij) at lag l is:\n\ntextVar(A_lij) = begincases\nfractau^2l^d  textif  i = j text (own lag) \nfractau^2 omega^2l^d cdot fracsigma_i^2sigma_j^2  textif  i neq j text (cross lag)\nendcases\n\nwhere:\n\ntau is the overall tightness (shrinkage intensity)\nd is the lag decay (typically d = 2)\nomega controls cross-variable shrinkage (typically omega  1)\nsigma_i^2 is the residual variance from a univariate AR(1) for variable i","category":"section"},{"location":"bayesian/#Interpretation-of-Hyperparameters","page":"Bayesian VAR","title":"Interpretation of Hyperparameters","text":"Parameter Effect Typical Values\ntau Overall shrinkage (lower = more shrinkage) 0.01 – 1.0\nd Lag decay (higher = faster decay) 1, 2, 3\nomega Cross-variable penalty (lower = more penalty) 0.5 – 1.0","category":"section"},{"location":"bayesian/#Julia-Implementation","page":"Bayesian VAR","title":"Julia Implementation","text":"using MacroEconometricModels\n\n# Define hyperparameters\nhyper = MinnesotaHyperparameters(\n    tau = 0.5,      # Overall tightness\n    decay = 2.0,    # Lag decay\n    lambda = 1.0,   # Own-lag variance scaling\n    mu = 1.0,       # Cross-lag variance scaling\n    omega = 1.0     # Deterministic terms scaling\n)\n\n# Use in BVAR estimation\nchain = estimate_bvar(Y, 2; n_samples=2000, n_adapts=500,\n                      prior=:minnesota, hyper=hyper)\n\n# Output:\n# MinnesotaHyperparameters{Float64}(tau=0.5, decay=2.0, lambda=1.0, mu=1.0, omega=1.0)\n\nThe tau=0.5 setting provides moderate shrinkage — coefficient estimates will be pulled halfway between the data-driven OLS estimates and the random walk prior. With decay=2.0, the prior variance for lag-l coefficients decays as 1l^2, so distant lags are strongly penalized. Setting mu=1.0 treats cross-variable lags the same as own lags; reducing mu (e.g., to 0.5) would impose stronger shrinkage on cross-variable coefficients, reflecting the common finding that own lags are more informative than other variables' lags.","category":"section"},{"location":"bayesian/#MinnesotaHyperparameters-Return-Values","page":"Bayesian VAR","title":"MinnesotaHyperparameters Return Values","text":"Field Type Description\ntau T Overall tightness (lower = more shrinkage toward prior)\ndecay T Lag decay exponent (higher = faster decay of lag importance)\nlambda T Own-lag variance scaling\nmu T Cross-lag variance scaling (lower = more penalty on cross-variable lags)\nomega T Deterministic terms scaling\n\n","category":"section"},{"location":"bayesian/#Dummy-Observations-Approach","page":"Bayesian VAR","title":"Dummy Observations Approach","text":"","category":"section"},{"location":"bayesian/#Implementation-via-Augmented-Regression","page":"Bayesian VAR","title":"Implementation via Augmented Regression","text":"We implement the Minnesota prior using dummy observations (Theil-Goldberger mixed estimation). The augmented data matrices are:\n\nPrior on coefficients (tightness dummies):\n\nY_d = beginbmatrix\ntextdiag(sigma_1 ldots sigma_n)  tau \n0_n(p-1) times n \ntextdiag(sigma_1 ldots sigma_n) \n0_1 times n\nendbmatrix quad\nX_d = beginbmatrix\n0_n times 1  J_p otimes textdiag(sigma_1 ldots sigma_n)  tau \n0_n(p-1) times 1  I_p-1 otimes textdiag(sigma_1 ldots sigma_n) \n0_n times 1  0_n times np \nc  0_1 times np\nendbmatrix\n\nwhere J_p = textdiag(1 2^d ldots p^d).\n\nThe posterior is then computed as OLS on the augmented data Y Y_d and X X_d.\n\nReference: Litterman (1986), Kadiyala & Karlsson (1997), Bańbura, Giannone & Reichlin (2010)","category":"section"},{"location":"bayesian/#Julia-Implementation-2","page":"Bayesian VAR","title":"Julia Implementation","text":"using MacroEconometricModels\n\n# Generate dummy observations for Minnesota prior\nY_dummy, X_dummy = gen_dummy_obs(Y, p, hyper)\n\n# Augment data\nY_aug = vcat(Y_actual, Y_dummy)\nX_aug = vcat(X_actual, X_dummy)\n\n# Posterior via OLS on augmented data\nB_post = (X_aug'X_aug) \\ (X_aug'Y_aug)\n\n# Output (for n=3, p=2):\n# size(Y_dummy) = (10, 3)    # 3n+n+1 = 10 dummy observations\n# size(X_dummy) = (10, 7)    # 1 + np = 7 regressors\n\nThe dummy observations encode the prior belief: the tightness dummies pull A_1 toward the identity (random walk), while the decay dummies shrink higher-lag coefficients toward zero. Augmenting the data with these pseudo-observations and running OLS on the combined system is algebraically equivalent to computing the posterior mean under the Normal-Inverse-Wishart conjugate prior.\n\n","category":"section"},{"location":"bayesian/#Hyperparameter-Optimization","page":"Bayesian VAR","title":"Hyperparameter Optimization","text":"","category":"section"},{"location":"bayesian/#Marginal-Likelihood","page":"Bayesian VAR","title":"Marginal Likelihood","text":"Rather than selecting tau subjectively, we can optimize it by maximizing the marginal likelihood (Giannone, Lenza & Primiceri, 2015):\n\np(Y  tau) = int p(Y  B Sigma) p(B Sigma  tau)  dB  dSigma\n\nFor the Normal-Inverse-Wishart prior with dummy observations, the log marginal likelihood has an analytical form:\n\nlog p(Y  tau) = c + fracT-k2 logtildeS^-1 - fracT_d2 logtildeS_d^-1 + log fracGamma_n(fracT+T_d - k2)Gamma_n(fracT_d - k2)\n\nwhere\n\nc is a normalization constant\nT is the sample size, k = 1 + np is the number of regressors per equation\nT_d is the number of dummy observations\ntildeS is the residual sum of squares from the augmented regression Y Y_d on X X_d\ntildeS_d is the residual sum of squares from the dummy-only regression\nGamma_n(cdot) is the multivariate gamma function\n\nReference: Giannone, Lenza & Primiceri (2015), Carriero, Clark & Marcellino (2015)","category":"section"},{"location":"bayesian/#Julia-Implementation-3","page":"Bayesian VAR","title":"Julia Implementation","text":"using MacroEconometricModels\n\n# Find optimal shrinkage using marginal likelihood\nbest_hyper = optimize_hyperparameters(Y, p; grid_size=20)\n\nprintln(\"Optimal hyperparameters:\")\nprintln(\"  τ (overall tightness): \", round(best_hyper.tau, digits=4))\nprintln(\"  d (lag decay): \", best_hyper.d)\n\n# Compute log marginal likelihood\nlml = log_marginal_likelihood(Y, p, hyper)\n\n# Output:\n# Optimal hyperparameters:\n#   τ (overall tightness): 0.2143\n#   d (lag decay): 2.0\n\nThe optimal tau balances fit and complexity: values near 0.01 produce near-dogmatic shrinkage to the random walk prior (good for high-dimensional systems), while values near 1.0 produce minimal shrinkage (approaching OLS). The marginal likelihood automatically penalizes overfitting, so the optimal tau increases with sample size as data evidence accumulates.","category":"section"},{"location":"bayesian/#Grid-Search-Options","page":"Bayesian VAR","title":"Grid Search Options","text":"# Custom optimization grid\nbest_hyper = optimize_hyperparameters(Y, p;\n    grid_size = 30,           # Number of grid points\n    tau_range = (0.01, 2.0),  # Range for τ\n    d_values = [1, 2, 3]      # Values for d\n)\n\n","category":"section"},{"location":"bayesian/#MCMC-Estimation-with-Turing.jl","page":"Bayesian VAR","title":"MCMC Estimation with Turing.jl","text":"","category":"section"},{"location":"bayesian/#The-BVAR-Model","page":"Bayesian VAR","title":"The BVAR Model","text":"For more flexible priors or non-conjugate settings, we use MCMC via Turing.jl with the NUTS sampler:\n\n@model function bvar_model(Y, X, prior_mean, prior_var, ν₀, S₀)\n    n = size(Y, 2)\n    k = size(X, 2)\n\n    # Prior on error covariance\n    Σ ~ InverseWishart(ν₀, S₀)\n\n    # Prior on coefficients\n    B ~ MatrixNormal(prior_mean, prior_var, Σ)\n\n    # Likelihood\n    for t in axes(Y, 1)\n        Y[t, :] ~ MvNormal(X[t, :]' * B, Σ)\n    end\nend","category":"section"},{"location":"bayesian/#Julia-Implementation-4","page":"Bayesian VAR","title":"Julia Implementation","text":"using MacroEconometricModels\n\n# Estimate BVAR with MCMC\nchain = estimate_bvar(Y, p;\n    n_samples = 2000,     # Posterior samples\n    n_adapts = 500,       # Adaptation samples\n    prior = :minnesota,   # Prior type\n    hyper = best_hyper    # Hyperparameters\n)\n\n# Access posterior draws\n# chain.samples contains the MCMC draws\n\n# Output:\n# Sampling: 100%|████████████████████████████████| Time: 0:00:45\n# Chains MCMC chain (2000×21×1 Array{Float64, 3})\n# Summary Statistics\n#   parameters     mean     std   naive_se    mcse      ess    rhat\n#   ──────────────────────────────────────────────────────────────\n#   B[1,1]       0.0142  0.0823    0.0018   0.0025   1089.2  1.001\n#   B[2,1]       0.4876  0.0614    0.0014   0.0019   1234.5  1.000\n#   ...\n\nThe MCMC output shows the posterior mean, standard deviation, and convergence diagnostics for each parameter. The ess (effective sample size) should be at least 400 for reliable quantile estimation, and rhat should be below 1.05 for all parameters — values above 1.1 indicate poor mixing.","category":"section"},{"location":"bayesian/#Convergence-Diagnostics","page":"Bayesian VAR","title":"Convergence Diagnostics","text":"# Extract chain parameters\nparams = extract_chain_parameters(chain)\n\n# Check R-hat statistics\n# Check effective sample sizes\n# Trace plots for visual inspection\n\nnote: Technical Note\nMCMC convergence should be assessed before interpreting results. Key diagnostics include: (1) hatR (R-hat) statistics should be below 1.05 for all parameters; (2) effective sample size (ESS) should be at least 400 for reliable posterior quantile estimation; (3) trace plots should show good mixing without trends or multimodality. If n_samples=2000 with n_adapts=500 shows poor convergence, try increasing both values or switching to a more informative prior (lower tau).\n\nReference: Gelman et al. (2013), Hoffman & Gelman (2014)\n\n","category":"section"},{"location":"bayesian/#Posterior-Point-Estimates","page":"Bayesian VAR","title":"Posterior Point Estimates","text":"","category":"section"},{"location":"bayesian/#Extracting-VARModel-from-MCMC-Chain","page":"Bayesian VAR","title":"Extracting VARModel from MCMC Chain","text":"After MCMC estimation, it is often useful to obtain a single VARModel based on the posterior mean or median. This allows using all frequentist tools (IRF, FEVD, HD, stationarity checks) on the Bayesian point estimate.\n\nusing MacroEconometricModels\n\n# After running estimate_bvar:\n# chain = estimate_bvar(Y, p; n_samples=2000, prior=:minnesota, hyper=hyper)\n\n# Extract VARModel with posterior mean parameters\nmean_model = posterior_mean_model(chain, p, n; data=Y)\n\n# Extract VARModel with posterior median parameters\nmedian_model = posterior_median_model(chain, p, n; data=Y)\n\n# Now use standard VAR tools\nstab = is_stationary(mean_model)\nprintln(\"Posterior mean model stationary: \", stab.is_stationary)\nprintln(\"Max eigenvalue modulus: \", round(stab.max_modulus, digits=4))\n\n# Frequentist IRF from the posterior mean\nirfs_mean = irf(mean_model, 20; method=:cholesky)\n\nThe posterior_mean_model averages the coefficient matrix B and covariance Sigma across all MCMC draws, providing a single point estimate that integrates over parameter uncertainty. The posterior_median_model uses the element-wise median instead, which is more robust to outlier draws but may produce a Sigma that is not positive definite in edge cases. When data=Y is provided, the function also computes residuals, enabling historical_decomposition and other residual-based analyses.\n\n","category":"section"},{"location":"bayesian/#Bayesian-Impulse-Response-Functions","page":"Bayesian VAR","title":"Bayesian Impulse Response Functions","text":"","category":"section"},{"location":"bayesian/#Posterior-IRF-Distribution","page":"Bayesian VAR","title":"Posterior IRF Distribution","text":"For each MCMC draw, we compute impulse responses, yielding a posterior distribution over IRFs. We report:\n\nPosterior median: Point estimate\nCredible intervals: 68% (16th-84th percentile) or 90% (5th-95th percentile)","category":"section"},{"location":"bayesian/#Cholesky-Identification","page":"Bayesian VAR","title":"Cholesky Identification","text":"using MacroEconometricModels\n\n# Bayesian IRF with Cholesky identification\nH = 20  # Horizon\nbirf_chol = irf(chain, p, n, H; method=:cholesky)\n\n# birf_chol.quantiles is (H+1) × n × n × 3 array\n# [:, :, :, 1] = 16th percentile\n# [:, :, :, 2] = median\n# [:, :, :, 3] = 84th percentile\n\nprintln(\"Bayesian IRF of GDP to own shock:\")\nfor h in [0, 4, 8, 12, 20]\n    med = round(birf_chol.quantiles[h+1, 1, 1, 2], digits=3)\n    lo = round(birf_chol.quantiles[h+1, 1, 1, 1], digits=3)\n    hi = round(birf_chol.quantiles[h+1, 1, 1, 3], digits=3)\n    println(\"  h=$h: $med [$lo, $hi]\")\nend\n\n# Output:\n# Bayesian IRF of GDP to own shock:\n#   h=0: 0.312 [0.278, 0.347]\n#   h=4: 0.048 [0.011, 0.089]\n#   h=8: 0.006 [-0.015, 0.028]\n#   h=12: 0.001 [-0.012, 0.014]\n#   h=20: 0.000 [-0.006, 0.007]\n\nThe posterior median IRF at h = 0 reflects the impact effect of a one-standard-deviation structural shock. The 68% credible interval text16th text84th narrows toward zero as the horizon increases, consistent with a stationary VAR where shocks dissipate over time. Unlike frequentist bootstrap CIs, Bayesian credible intervals integrate over parameter uncertainty in B and Sigma, often producing wider bands at short horizons.","category":"section"},{"location":"bayesian/#BayesianImpulseResponse-Return-Values","page":"Bayesian VAR","title":"BayesianImpulseResponse Return Values","text":"Field Type Description\nquantiles Array{T,4} (H+1) times n times n times 3: dim 4 = [16th pctl, median, 84th pctl]\nmean Array{T,3} (H+1) times n times n posterior mean IRF\nhorizon Int Maximum IRF horizon\nvariables Vector{String} Variable names\nshocks Vector{String} Shock names\nquantile_levels Vector{T} Quantile levels","category":"section"},{"location":"bayesian/#Sign-Restrictions","page":"Bayesian VAR","title":"Sign Restrictions","text":"# Define sign restriction check function\nfunction check_demand_shock(irf_array)\n    # Demand shock: positive GDP and inflation on impact\n    return irf_array[1, 1, 1] > 0 && irf_array[1, 2, 1] > 0\nend\n\n# Bayesian IRF with sign restrictions\nbirf_sign = irf(chain, p, n, H;\n    method = :sign,\n    check_func = check_demand_shock\n)\n\nprintln(\"Bayesian sign-restricted demand shock → GDP:\")\nfor h in [0, 4, 8, 12]\n    med = round(birf_sign.quantiles[h+1, 1, 1, 2], digits=3)\n    lo = round(birf_sign.quantiles[h+1, 1, 1, 1], digits=3)\n    hi = round(birf_sign.quantiles[h+1, 1, 1, 3], digits=3)\n    println(\"  h=$h: $med [$lo, $hi]\")\nend\n\n# Output:\n# Bayesian sign-restricted demand shock → GDP:\n#   h=0: 0.295 [0.251, 0.342]\n#   h=4: 0.052 [0.018, 0.094]\n#   h=8: 0.008 [-0.011, 0.031]\n#   h=12: 0.001 [-0.009, 0.015]\n\nThe sign-restricted IRFs are set-identified: the credible intervals combine both parameter uncertainty (from MCMC) and identification uncertainty (from the rotation Q). The median tends to be slightly smaller than under Cholesky because the sign restrictions eliminate some extreme rotations.\n\n","category":"section"},{"location":"bayesian/#Bayesian-FEVD","page":"Bayesian VAR","title":"Bayesian FEVD","text":"","category":"section"},{"location":"bayesian/#Posterior-FEVD-Distribution","page":"Bayesian VAR","title":"Posterior FEVD Distribution","text":"Similarly, forecast error variance decomposition can be computed for each posterior draw:\n\nusing MacroEconometricModels\n\n# Bayesian FEVD\nbfevd = fevd(chain, p, n, H; method=:cholesky)\n\n# Report median and credible intervals\nfor h in [1, 4, 12, 20]\n    println(\"FEVD at h=$h:\")\n    med = round(bfevd.quantiles[h, 1, 1, 2] * 100, digits=1)\n    lo = round(bfevd.quantiles[h, 1, 1, 1] * 100, digits=1)\n    hi = round(bfevd.quantiles[h, 1, 1, 3] * 100, digits=1)\n    println(\"  Shock 1 → Var 1: $med% [$lo%, $hi%]\")\nend\n\n# Output:\n# FEVD at h=1:\n#   Shock 1 → Var 1: 97.2% [93.1%, 99.4%]\n# FEVD at h=4:\n#   Shock 1 → Var 1: 88.5% [78.6%, 95.1%]\n# FEVD at h=12:\n#   Shock 1 → Var 1: 82.3% [68.2%, 92.7%]\n# FEVD at h=20:\n#   Shock 1 → Var 1: 80.1% [64.5%, 91.8%]\n\nAt h = 1, own shocks dominate (97%), reflecting the Cholesky ordering where variable 1 is first. As the horizon increases, spillovers from other shocks erode the own-shock share. The wide credible intervals at long horizons reflect cumulating parameter uncertainty through the VMA representation. Bayesian FEVD credible intervals are typically wider than frequentist bootstrap CIs because they integrate over the full posterior distribution of (B Sigma).","category":"section"},{"location":"bayesian/#BayesianFEVD-Return-Values","page":"Bayesian VAR","title":"BayesianFEVD Return Values","text":"Field Type Description\nquantiles Array{T,4} H times n times n times 3: dim 4 = [16th pctl, median, 84th pctl]\nmean Array{T,3} H times n times n posterior mean FEVD proportions\nhorizon Int Maximum horizon\nvariables Vector{String} Variable names\nshocks Vector{String} Shock names\nquantile_levels Vector{T} Quantile levels\n\n","category":"section"},{"location":"bayesian/#Information-Criteria","page":"Bayesian VAR","title":"Information Criteria","text":"","category":"section"},{"location":"bayesian/#Log-Likelihood","page":"Bayesian VAR","title":"Log-Likelihood","text":"For a Gaussian VAR, the log-likelihood is:\n\nlog L = -fracT cdot n2 log(2pi) - fracT2 logSigma - frac12 sum_t=1^T u_t Sigma^-1 u_t","category":"section"},{"location":"bayesian/#Marginal-Likelihood-(Bayesian)","page":"Bayesian VAR","title":"Marginal Likelihood (Bayesian)","text":"For Bayesian model comparison, we use the marginal likelihood (also called evidence):\n\np(Y  mathcalM) = int p(Y  theta mathcalM) p(theta  mathcalM)  dtheta\n\nModels with higher marginal likelihood better balance fit and complexity.\n\n","category":"section"},{"location":"bayesian/#Complete-Example","page":"Bayesian VAR","title":"Complete Example","text":"using MacroEconometricModels\nusing Random\n\nRandom.seed!(42)\n\n# Generate data\nT, n, p = 200, 3, 2\nY = randn(T, n)\nfor t in 2:T\n    Y[t, :] = 0.5 * Y[t-1, :] + 0.3 * randn(n)\nend\n\n# Step 1: Optimize hyperparameters\nprintln(\"Optimizing hyperparameters...\")\nbest_hyper = optimize_hyperparameters(Y, p; grid_size=20)\nprintln(\"Optimal τ: \", round(best_hyper.tau, digits=4))\n\n# Step 2: Estimate BVAR\nprintln(\"\\nEstimating BVAR with MCMC...\")\nchain = estimate_bvar(Y, p;\n    n_samples = 2000,\n    n_adapts = 500,\n    prior = :minnesota,\n    hyper = best_hyper\n)\n\n# Step 3: Compute Bayesian IRF\nH = 20\nbirf = irf(chain, p, n, H; method=:cholesky)\n\n# Step 4: Report results\nprintln(\"\\nBayesian IRF (shock 1 → variable 1):\")\nfor h in [0, 4, 8, 12, 20]\n    med = round(birf.quantiles[h+1, 1, 1, 2], digits=3)\n    lo = round(birf.quantiles[h+1, 1, 1, 1], digits=3)\n    hi = round(birf.quantiles[h+1, 1, 1, 3], digits=3)\n    println(\"  h=$h: $med [$lo, $hi]\")\nend\n\n# Output:\n# Optimizing hyperparameters...\n# Optimal τ: 0.2143\n#\n# Estimating BVAR with MCMC...\n# Sampling: 100%|████████████████████████████████| Time: 0:00:42\n#\n# Bayesian IRF (shock 1 → variable 1):\n#   h=0: 0.305 [0.271, 0.341]\n#   h=4: 0.046 [0.009, 0.085]\n#   h=8: 0.005 [-0.014, 0.026]\n#   h=12: 0.001 [-0.011, 0.013]\n#   h=20: 0.000 [-0.006, 0.006]\n\nThis workflow demonstrates the complete Bayesian pipeline: hyperparameter optimization selects the optimal shrinkage tau via marginal likelihood, then MCMC produces posterior draws from which we compute IRFs with credible intervals. The IRF quickly converges to zero, consistent with the DGP's moderate persistence (A_11 = 05). The credible intervals at h = 0 are tight because the impact effect is well-identified by the Cholesky ordering, while longer horizons show wider bands reflecting cumulating parameter uncertainty.\n\n","category":"section"},{"location":"bayesian/#Large-BVAR","page":"Bayesian VAR","title":"Large BVAR","text":"","category":"section"},{"location":"bayesian/#Handling-High-Dimensional-Systems","page":"Bayesian VAR","title":"Handling High-Dimensional Systems","text":"For large VAR systems (many variables), the Minnesota prior becomes essential:\n\nusing MacroEconometricModels\n\n# Large system: 20 variables\nn = 20\np = 4\n\n# Stronger shrinkage for large systems\nhyper_large = MinnesotaHyperparameters(\n    tau = 0.1,      # Tighter prior\n    decay = 2.0,\n    lambda = 1.0,\n    mu = 0.5,       # Penalize cross-variable coefficients\n    omega = 1.0\n)\n\n# Or optimize automatically\nbest_hyper = optimize_hyperparameters(Y_large, p)\n\nFor large systems (20+ variables), the number of VAR parameters (n^2 p + n) grows quadratically with the number of variables, quickly exceeding the sample size. The Minnesota prior prevents overfitting by shrinking cross-variable coefficients toward zero (mu=0.5) and applying strong overall tightness (tau=0.1). Bańbura, Giannone & Reichlin (2010) show that BVAR with optimized shrinkage outperforms both unrestricted VAR and small-scale models for macroeconomic forecasting.\n\nReference: Bańbura, Giannone & Reichlin (2010)\n\n","category":"section"},{"location":"bayesian/#References","page":"Bayesian VAR","title":"References","text":"","category":"section"},{"location":"bayesian/#Minnesota-Prior-and-BVAR","page":"Bayesian VAR","title":"Minnesota Prior and BVAR","text":"Bańbura, Marta, Domenico Giannone, and Lucrezia Reichlin. 2010. \"Large Bayesian Vector Auto Regressions.\" Journal of Applied Econometrics 25 (1): 71–92. https://doi.org/10.1002/jae.1137\nCarriero, Andrea, Todd E. Clark, and Massimiliano Marcellino. 2015. \"Bayesian VARs: Specification Choices and Forecast Accuracy.\" Journal of Applied Econometrics 30 (1): 46–73. https://doi.org/10.1002/jae.2272\nDoan, Thomas, Robert Litterman, and Christopher Sims. 1984. \"Forecasting and Conditional Projection Using Realistic Prior Distributions.\" Econometric Reviews 3 (1): 1–100. https://doi.org/10.1080/07474938408800053\nGiannone, Domenico, Michele Lenza, and Giorgio E. Primiceri. 2015. \"Prior Selection for Vector Autoregressions.\" Review of Economics and Statistics 97 (2): 436–451. https://doi.org/10.1162/RESTa00483\nKadiyala, K. Rao, and Sune Karlsson. 1997. \"Numerical Methods for Estimation and Inference in Bayesian VAR-Models.\" Journal of Applied Econometrics 12 (2): 99–132. https://doi.org/10.1002/(SICI)1099-1255(199703)12:2<99::AID-JAE429>3.0.CO;2-A\nLitterman, Robert B. 1986. \"Forecasting with Bayesian Vector Autoregressions—Five Years of Experience.\" Journal of Business & Economic Statistics 4 (1): 25–38. https://doi.org/10.1080/07350015.1986.10509491","category":"section"},{"location":"bayesian/#MCMC-and-Bayesian-Inference","page":"Bayesian VAR","title":"MCMC and Bayesian Inference","text":"Gelman, Andrew, John B. Carlin, Hal S. Stern, David B. Dunson, Aki Vehtari, and Donald B. Rubin. 2013. Bayesian Data Analysis. 3rd ed. Boca Raton, FL: CRC Press. ISBN 978-1-4398-4095-5.\nHoffman, Matthew D., and Andrew Gelman. 2014. \"The No-U-Turn Sampler: Adaptively Setting Path Lengths in Hamiltonian Monte Carlo.\" Journal of Machine Learning Research 15 (1): 1593–1623.","category":"section"},{"location":"api_functions/#api_functions","page":"Functions","title":"API Functions","text":"This page documents all functions in MacroEconometricModels.jl, organized by module.\n\n","category":"section"},{"location":"api_functions/#ARIMA-Models","page":"Functions","title":"ARIMA Models","text":"","category":"section"},{"location":"api_functions/#Estimation","page":"Functions","title":"Estimation","text":"","category":"section"},{"location":"api_functions/#Forecasting","page":"Functions","title":"Forecasting","text":"","category":"section"},{"location":"api_functions/#Order-Selection","page":"Functions","title":"Order Selection","text":"","category":"section"},{"location":"api_functions/#VAR-Estimation","page":"Functions","title":"VAR Estimation","text":"","category":"section"},{"location":"api_functions/#Frequentist-Estimation","page":"Functions","title":"Frequentist Estimation","text":"","category":"section"},{"location":"api_functions/#Bayesian-Estimation","page":"Functions","title":"Bayesian Estimation","text":"","category":"section"},{"location":"api_functions/#Prior-Specification","page":"Functions","title":"Prior Specification","text":"","category":"section"},{"location":"api_functions/#Structural-Identification","page":"Functions","title":"Structural Identification","text":"","category":"section"},{"location":"api_functions/#Innovation-Accounting","page":"Functions","title":"Innovation Accounting","text":"","category":"section"},{"location":"api_functions/#Impulse-Response-Functions","page":"Functions","title":"Impulse Response Functions","text":"","category":"section"},{"location":"api_functions/#Forecast-Error-Variance-Decomposition","page":"Functions","title":"Forecast Error Variance Decomposition","text":"","category":"section"},{"location":"api_functions/#Historical-Decomposition","page":"Functions","title":"Historical Decomposition","text":"","category":"section"},{"location":"api_functions/#Summary-Tables","page":"Functions","title":"Summary Tables","text":"","category":"section"},{"location":"api_functions/#Local-Projections","page":"Functions","title":"Local Projections","text":"","category":"section"},{"location":"api_functions/#Core-LP-Estimation-and-Covariance","page":"Functions","title":"Core LP Estimation and Covariance","text":"","category":"section"},{"location":"api_functions/#LP-IV-(Stock-and-Watson-2018)","page":"Functions","title":"LP-IV (Stock & Watson 2018)","text":"","category":"section"},{"location":"api_functions/#Smooth-LP-(Barnichon-and-Brownlees-2019)","page":"Functions","title":"Smooth LP (Barnichon & Brownlees 2019)","text":"","category":"section"},{"location":"api_functions/#State-Dependent-LP-(Auerbach-and-Gorodnichenko-2013)","page":"Functions","title":"State-Dependent LP (Auerbach & Gorodnichenko 2013)","text":"","category":"section"},{"location":"api_functions/#Propensity-Score-LP-(Angrist-et-al.-2018)","page":"Functions","title":"Propensity Score LP (Angrist et al. 2018)","text":"","category":"section"},{"location":"api_functions/#LP-Forecasting","page":"Functions","title":"LP Forecasting","text":"","category":"section"},{"location":"api_functions/#LP-FEVD-(Gorodnichenko-and-Lee-2019)","page":"Functions","title":"LP-FEVD (Gorodnichenko & Lee 2019)","text":"","category":"section"},{"location":"api_functions/#Factor-Models","page":"Functions","title":"Factor Models","text":"","category":"section"},{"location":"api_functions/#Static-Factor-Model","page":"Functions","title":"Static Factor Model","text":"","category":"section"},{"location":"api_functions/#Dynamic-Factor-Model","page":"Functions","title":"Dynamic Factor Model","text":"","category":"section"},{"location":"api_functions/#Generalized-Dynamic-Factor-Model","page":"Functions","title":"Generalized Dynamic Factor Model","text":"","category":"section"},{"location":"api_functions/#GMM-Estimation","page":"Functions","title":"GMM Estimation","text":"","category":"section"},{"location":"api_functions/#Unit-Root-and-Cointegration-Tests","page":"Functions","title":"Unit Root and Cointegration Tests","text":"","category":"section"},{"location":"api_functions/#Volatility-Models","page":"Functions","title":"Volatility Models","text":"","category":"section"},{"location":"api_functions/#ARCH-Estimation-and-Diagnostics","page":"Functions","title":"ARCH Estimation and Diagnostics","text":"","category":"section"},{"location":"api_functions/#GARCH-Estimation-and-Diagnostics","page":"Functions","title":"GARCH Estimation and Diagnostics","text":"","category":"section"},{"location":"api_functions/#Stochastic-Volatility","page":"Functions","title":"Stochastic Volatility","text":"","category":"section"},{"location":"api_functions/#Volatility-Forecasting","page":"Functions","title":"Volatility Forecasting","text":"","category":"section"},{"location":"api_functions/#Volatility-Accessors","page":"Functions","title":"Volatility Accessors","text":"","category":"section"},{"location":"api_functions/#Display-and-References","page":"Functions","title":"Display and References","text":"","category":"section"},{"location":"api_functions/#Non-Gaussian-Structural-Identification","page":"Functions","title":"Non-Gaussian Structural Identification","text":"","category":"section"},{"location":"api_functions/#Normality-Tests","page":"Functions","title":"Normality Tests","text":"","category":"section"},{"location":"api_functions/#ICA-based-Identification","page":"Functions","title":"ICA-based Identification","text":"","category":"section"},{"location":"api_functions/#Non-Gaussian-ML-Identification","page":"Functions","title":"Non-Gaussian ML Identification","text":"","category":"section"},{"location":"api_functions/#Heteroskedasticity-Identification","page":"Functions","title":"Heteroskedasticity Identification","text":"","category":"section"},{"location":"api_functions/#Identifiability-Tests","page":"Functions","title":"Identifiability Tests","text":"","category":"section"},{"location":"api_functions/#Utility-Functions","page":"Functions","title":"Utility Functions","text":"","category":"section"},{"location":"api_functions/#MacroEconometricModels.estimate_ar","page":"Functions","title":"MacroEconometricModels.estimate_ar","text":"estimate_ar(y, p; method=:ols, include_intercept=true) -> ARModel\n\nEstimate AR(p) model: yₜ = c + φ₁yₜ₋₁ + ... + φₚyₜ₋ₚ + εₜ\n\nArguments\n\ny: Time series vector\np: AR order (must be ≥ 1)\nmethod: Estimation method (:ols or :mle)\ninclude_intercept: Whether to include constant term\n\nReturns\n\nARModel with estimated coefficients and diagnostics.\n\nExample\n\ny = randn(200)\nmodel = estimate_ar(y, 2)\nprintln(model.phi)  # AR coefficients\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.estimate_ma","page":"Functions","title":"MacroEconometricModels.estimate_ma","text":"estimate_ma(y, q; method=:css_mle, include_intercept=true, max_iter=500) -> MAModel\n\nEstimate MA(q) model: yₜ = c + εₜ + θ₁εₜ₋₁ + ... + θqεₜ₋q\n\nArguments\n\ny: Time series vector\nq: MA order (must be ≥ 1)\nmethod: Estimation method (:css, :mle, or :css_mle)\ninclude_intercept: Whether to include constant term\nmax_iter: Maximum optimization iterations\n\nReturns\n\nMAModel with estimated coefficients and diagnostics.\n\nExample\n\ny = randn(200)\nmodel = estimate_ma(y, 1)\nprintln(model.theta)  # MA coefficient\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.estimate_arma","page":"Functions","title":"MacroEconometricModels.estimate_arma","text":"estimate_arma(y, p, q; method=:css_mle, include_intercept=true, max_iter=500) -> ARMAModel\n\nEstimate ARMA(p,q) model: yₜ = c + φ₁yₜ₋₁ + ... + φₚyₜ₋ₚ + εₜ + θ₁εₜ₋₁ + ... + θqεₜ₋q\n\nArguments\n\ny: Time series vector\np: AR order\nq: MA order\nmethod: Estimation method (:css, :mle, or :css_mle)\ninclude_intercept: Whether to include constant term\nmax_iter: Maximum optimization iterations\n\nReturns\n\nARMAModel with estimated coefficients and diagnostics.\n\nExample\n\ny = randn(200)\nmodel = estimate_arma(y, 1, 1)\nprintln(\"AR: \", model.phi, \" MA: \", model.theta)\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.estimate_arima","page":"Functions","title":"MacroEconometricModels.estimate_arima","text":"estimate_arima(y, p, d, q; method=:css_mle, include_intercept=true, max_iter=500) -> ARIMAModel\n\nEstimate ARIMA(p,d,q) model by differencing d times and fitting ARMA(p,q).\n\nArguments\n\ny: Time series vector\np: AR order\nd: Integration order (number of differences)\nq: MA order\nmethod: Estimation method (:css, :mle, or :css_mle)\ninclude_intercept: Whether to include constant term (on differenced series)\nmax_iter: Maximum optimization iterations\n\nReturns\n\nARIMAModel with estimated coefficients and diagnostics.\n\nExample\n\ny = cumsum(randn(200))  # Random walk\nmodel = estimate_arima(y, 1, 1, 0)  # ARIMA(1,1,0)\nprintln(model.phi)\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels._compute_psi_weights-Union{Tuple{T}, Tuple{Vector{T}, Vector{T}, Int64}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels._compute_psi_weights","text":"_compute_psi_weights(phi, theta, h) -> Vector{T}\n\nCompute ψ-weights for the MA(∞) representation of an ARMA process.\n\nThe ARMA(p,q) process can be written as: yₜ = μ + Σⱼ₌₀^∞ ψⱼ εₜ₋ⱼ\n\nwhere ψ₀ = 1 and ψⱼ follows the recursion: ψⱼ = φ₁ψⱼ₋₁ + ... + φₚψⱼ₋ₚ + θⱼ\n\nReturns [ψ₁, ψ₂, ..., ψₕ] (excludes ψ₀ = 1).\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels._confidence_band-Union{Tuple{T}, Tuple{Vector{T}, Vector{T}, T}} where T","page":"Functions","title":"MacroEconometricModels._confidence_band","text":"_confidence_band(forecasts, se, conf_level)\n\nCompute symmetric confidence interval bounds from forecasts, standard errors, and a confidence level.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels._forecast_arma-Union{Tuple{T}, Tuple{Vector{T}, Vector{T}, T, Vector{T}, Vector{T}, T, Int64, T}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels._forecast_arma","text":"_forecast_arma(y, resid, c, phi, theta, sigma2, h, conf_level) -> ARIMAForecast\n\nUnified point forecast + CI computation for any ARMA(p,q) model. AR models pass theta=T[], MA models pass phi=T[].\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels._forecast_variance-Union{Tuple{T}, Tuple{T, Vector{T}, Int64}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels._forecast_variance","text":"_forecast_variance(sigma2, psi, h) -> Vector{T}\n\nCompute h-step ahead forecast variance.\n\nVar(eₜ₊ₕ) = σ² (1 + ψ₁² + ψ₂² + ... + ψₕ₋₁²)\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels._integrate_forecasts-Union{Tuple{T}, Tuple{Vector{T}, Vector{T}, Int64}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels._integrate_forecasts","text":"_integrate_forecasts(y, fc_diff, d) -> Vector{T}\n\nIntegrate d-differenced forecasts back to original scale.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels._integrate_se-Union{Tuple{T}, Tuple{Vector{T}, Int64}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels._integrate_se","text":"_integrate_se(se_diff, d) -> Vector{T}\n\nApproximate standard errors after integration.\n\nFor d-fold integration, the variance grows roughly as h^d. This is a conservative approximation.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.forecast-Union{Tuple{T}, Tuple{ARIMAModel{T}, Int64}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.forecast","text":"forecast(model::ARIMAModel, h; conf_level=0.95) -> ARIMAForecast\n\nCompute h-step ahead forecasts with confidence intervals for ARIMA model. Forecasts are computed on the differenced series and then integrated back to the original scale.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.forecast-Union{Tuple{T}, Tuple{ARMAModel{T}, Int64}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.forecast","text":"forecast(model::ARMAModel, h; conf_level=0.95) -> ARIMAForecast\n\nCompute h-step ahead forecasts with confidence intervals for ARMA model.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.forecast-Union{Tuple{T}, Tuple{ARModel{T}, Int64}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.forecast","text":"forecast(model::ARModel, h; conf_level=0.95) -> ARIMAForecast\n\nCompute h-step ahead forecasts with confidence intervals for AR model.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.forecast-Union{Tuple{T}, Tuple{MAModel{T}, Int64}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.forecast","text":"forecast(model::MAModel, h; conf_level=0.95) -> ARIMAForecast\n\nCompute h-step ahead forecasts with confidence intervals for MA model.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#StatsAPI.predict-Tuple{AbstractARIMAModel, Int64}","page":"Functions","title":"StatsAPI.predict","text":"predict(model::AbstractARIMAModel, h::Int) -> Vector{T}\n\nReturn h-step ahead point forecasts (without confidence intervals).\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.select_arima_order","page":"Functions","title":"MacroEconometricModels.select_arima_order","text":"select_arima_order(y, max_p, max_q; criterion=:bic, d=0, method=:css_mle, include_intercept=true)\n\nAutomatically select ARMA/ARIMA order via grid search over information criteria.\n\nSearches over p ∈ 0:maxp and q ∈ 0:maxq, fits each model, and selects the order that minimizes the specified information criterion.\n\nArguments\n\ny: Time series vector\nmax_p: Maximum AR order to consider\nmax_q: Maximum MA order to consider\ncriterion: Selection criterion (:aic or :bic, default :bic)\nd: Integration order for ARIMA (default 0 = ARMA)\nmethod: Estimation method (:css, :mle, or :css_mle)\ninclude_intercept: Whether to include constant term\n\nReturns\n\nARIMAOrderSelection with best orders, IC matrices, and fitted models.\n\nExample\n\ny = randn(200)\nresult = select_arima_order(y, 3, 3; criterion=:bic)\nprintln(\"Best order: p=$(result.best_p_bic), q=$(result.best_q_bic)\")\nbest_model = result.best_model_bic\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.auto_arima","page":"Functions","title":"MacroEconometricModels.auto_arima","text":"auto_arima(y; max_p=5, max_q=5, max_d=2, criterion=:bic, method=:css_mle)\n\nAutomatically select and fit the best ARIMA model.\n\nPerforms order selection over p, d, and q using the specified criterion. For integration order d, uses unit root test heuristics.\n\nArguments\n\ny: Time series vector\nmax_p: Maximum AR order (default 5)\nmax_q: Maximum MA order (default 5)\nmax_d: Maximum integration order (default 2)\ncriterion: Selection criterion (:aic or :bic)\nmethod: Estimation method\n\nReturns\n\nBest fitted ARIMAModel or ARMAModel.\n\nExample\n\ny = cumsum(randn(200))\nmodel = auto_arima(y)\nprintln(model)\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.ic_table","page":"Functions","title":"MacroEconometricModels.ic_table","text":"ic_table(result::ARIMAOrderSelection; criterion=:bic)\n\nReturn a formatted table of IC values for printing.\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.estimate_bvar-Union{Tuple{T}, Tuple{AbstractMatrix{T}, Int64}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.estimate_bvar","text":"estimate_bvar(Y, p; n_samples=1000, n_adapts=500, prior=:normal, hyper=nothing,\n              sampler=:nuts, sampler_args=(;)) -> Chains\n\nEstimate Bayesian VAR via Turing.jl MCMC.\n\nSamplers: :nuts (default), :hmc, :hmcda, :is, :smc, :pg. Prior: :normal (default) or :minnesota with optional hyper::MinnesotaHyperparameters.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.estimate_var-Tuple{DataFrames.DataFrame, Int64}","page":"Functions","title":"MacroEconometricModels.estimate_var","text":"Estimate VAR from DataFrame. Use vars to select columns.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.estimate_var-Union{Tuple{T}, Tuple{AbstractMatrix{T}, Int64}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.estimate_var","text":"estimate_var(Y::AbstractMatrix{T}, p::Int; check_stability::Bool=true) -> VARModel{T}\n\nEstimate VAR(p) via OLS: Yₜ = c + A₁Yₜ₋₁ + ... + AₚYₜ₋ₚ + uₜ.\n\nArguments\n\nY: Data matrix (T × n)\np: Number of lags\ncheck_stability: If true (default), warns if estimated VAR is non-stationary\n\nReturns\n\nVARModel with estimated coefficients, residuals, covariance matrix, and information criteria.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.extract_chain_parameters-Tuple{MCMCChains.Chains}","page":"Functions","title":"MacroEconometricModels.extract_chain_parameters","text":"extract_chain_parameters(chain::Chains) -> (b_vecs, sigmas)\n\nExtract coefficient vectors and covariance matrices from MCMC chain. Handles both diagonal (gradient samplers) and InverseWishart (particle samplers) parameterizations.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.get_sampler-Tuple{Symbol, Int64, NamedTuple}","page":"Functions","title":"MacroEconometricModels.get_sampler","text":"Create Turing sampler from symbol. Supports: :nuts, :hmc, :hmcda, :is, :smc, :pg.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.parameters_to_model-Union{Tuple{T}, Tuple{AbstractVector{T}, AbstractVector{T}, Int64, Int64}, Tuple{AbstractVector{T}, AbstractVector{T}, Int64, Int64, AbstractMatrix{T}}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.parameters_to_model","text":"Convert chain parameters to VARModel. Provide data for residual computation.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.posterior_mean_model-Tuple{MCMCChains.Chains, Int64, Int64}","page":"Functions","title":"MacroEconometricModels.posterior_mean_model","text":"VARModel with posterior mean parameters.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.posterior_median_model-Tuple{MCMCChains.Chains, Int64, Int64}","page":"Functions","title":"MacroEconometricModels.posterior_median_model","text":"VARModel with posterior median parameters.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.select_lag_order-Union{Tuple{T}, Tuple{AbstractMatrix{T}, Int64}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.select_lag_order","text":"Select optimal lag order via information criterion (:aic, :bic, :hqic).\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.var_bayes_sequential-Union{Tuple{T}, Tuple{Matrix{T}, Matrix{T}, Int64, Int64}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.var_bayes_sequential","text":"Sequential BVAR model with full covariance for particle-based samplers (SMC, PG). Uses InverseWishart for compatibility with particle samplers (no AD required).\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.var_bayes_vectorized-Union{Tuple{T}, Tuple{Matrix{T}, Matrix{T}, Int64, Int64}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.var_bayes_vectorized","text":"Vectorized BVAR model for gradient-based samplers (NUTS, HMC, HMCDA). Uses diagonal covariance for numerical stability with ForwardDiff AD.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#StatsAPI.confint-Union{Tuple{VARModel{T}}, Tuple{T}} where T","page":"Functions","title":"StatsAPI.confint","text":"Confidence intervals at given level (default 95%).\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#StatsAPI.loglikelihood-Union{Tuple{VARModel{T}}, Tuple{T}} where T","page":"Functions","title":"StatsAPI.loglikelihood","text":"Gaussian log-likelihood.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#StatsAPI.predict-Tuple{VARModel}","page":"Functions","title":"StatsAPI.predict","text":"In-sample fitted values.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#StatsAPI.predict-Union{Tuple{T}, Tuple{VARModel{T}, Int64}} where T","page":"Functions","title":"StatsAPI.predict","text":"Out-of-sample forecasts for steps periods.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#StatsAPI.r2-Union{Tuple{VARModel{T}}, Tuple{T}} where T","page":"Functions","title":"StatsAPI.r2","text":"R² for each equation.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#StatsAPI.vcov-Union{Tuple{VARModel{T}}, Tuple{T}} where T","page":"Functions","title":"StatsAPI.vcov","text":"Covariance of vectorized coefficients: Σ ⊗ (X'X)⁻¹.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.gen_dummy_obs-Union{Tuple{T}, Tuple{AbstractMatrix{T}, Int64, MinnesotaHyperparameters}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.gen_dummy_obs","text":"gen_dummy_obs(Y, p, hyper) -> (Y_dummy, X_dummy)\n\nGenerate Minnesota prior dummy observations. Hyperparameters: tau (tightness), decay, lambda (sum-of-coef), mu (co-persistence), omega.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.log_marginal_likelihood-Union{Tuple{T}, Tuple{AbstractMatrix{T}, Int64, MinnesotaHyperparameters}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.log_marginal_likelihood","text":"log_marginal_likelihood(Y, p, hyper) -> T\n\nClosed-form log marginal likelihood for BVAR with Minnesota prior.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.optimize_hyperparameters-Union{Tuple{T}, Tuple{AbstractMatrix{T}, Int64}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.optimize_hyperparameters","text":"Optimize tau via grid search on marginal likelihood.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.optimize_hyperparameters_full-Union{Tuple{T}, Tuple{AbstractMatrix{T}, Int64}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.optimize_hyperparameters_full","text":"Full grid search over tau, lambda, mu.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels._build_zero_constraint_matrix-Union{Tuple{T}, Tuple{SVARRestrictions, Int64, Array{Matrix{T}, 1}, LinearAlgebra.LowerTriangular{T, Matrix{T}}}} where T","page":"Functions","title":"MacroEconometricModels._build_zero_constraint_matrix","text":"Build constraint matrix for zero restrictions on shock j.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels._check_sign_restrictions-Union{Tuple{T}, Tuple{Array{T, 3}, SVARRestrictions}} where T","page":"Functions","title":"MacroEconometricModels._check_sign_restrictions","text":"Check if all sign restrictions are satisfied.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels._check_zero_restrictions-Union{Tuple{T}, Tuple{Array{T, 3}, SVARRestrictions}} where T","page":"Functions","title":"MacroEconometricModels._check_zero_restrictions","text":"Check if all zero restrictions are satisfied.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels._compute_importance_weight-Union{Tuple{T}, Tuple{Matrix{T}, SVARRestrictions, Array{Matrix{T}, 1}, LinearAlgebra.LowerTriangular{T, Matrix{T}}}} where T","page":"Functions","title":"MacroEconometricModels._compute_importance_weight","text":"Compute importance weight for Q (corrects non-uniform prior from zero restrictions).\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels._compute_irf_for_Q-Union{Tuple{T}, Tuple{VARModel{T}, Matrix{T}, Array{Matrix{T}, 1}, LinearAlgebra.LowerTriangular{T, Matrix{T}}, Int64}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels._compute_irf_for_Q","text":"Compute structural IRF for rotation Q.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels._compute_ma_coefficients-Union{Tuple{T}, Tuple{VARModel{T}, Int64}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels._compute_ma_coefficients","text":"Compute MA coefficients Φ0, ..., Φhorizon.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels._draw_Q_with_zero_restrictions-Union{Tuple{T}, Tuple{SVARRestrictions, Array{Matrix{T}, 1}, LinearAlgebra.LowerTriangular{T, Matrix{T}}}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels._draw_Q_with_zero_restrictions","text":"Draw orthogonal Q satisfying zero restrictions (Algorithm 2, Arias et al. 2018).\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels._draw_null_space_vector-Union{Tuple{T}, Tuple{Array{Vector{T}, 1}, Int64}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels._draw_null_space_vector","text":"Draw unit vector from null space of constraints.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels._draw_uniform_orthogonal-Union{Tuple{Int64}, Tuple{T}, Tuple{Int64, Type{T}}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels._draw_uniform_orthogonal","text":"Draw uniformly from O(n) via QR decomposition.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels._weighted_quantile-Union{Tuple{S}, Tuple{T}, Tuple{AbstractVector{T}, AbstractVector{S}, Real}} where {T, S}","page":"Functions","title":"MacroEconometricModels._weighted_quantile","text":"Weighted quantile via linear interpolation.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.compute_Q-Union{Tuple{T}, Tuple{VARModel{T}, Symbol, Int64, Any, Any}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.compute_Q","text":"compute_Q(model, method, horizon, check_func, narrative_check;\n          max_draws=100, transition_var=nothing, regime_indicator=nothing)\n\nCompute identification matrix Q for structural VAR analysis.\n\nMethods\n\n:cholesky — Cholesky decomposition (recursive ordering)\n:sign — Sign restrictions (requires check_func)\n:narrative — Narrative restrictions (requires check_func and narrative_check)\n:long_run — Long-run restrictions (Blanchard-Quah)\n:fastica — FastICA (Hyvärinen 1999)\n:jade — JADE (Cardoso 1999)\n:sobi — SOBI (Belouchrani et al. 1997)\n:dcov — Distance covariance ICA (Matteson & Tsay 2017)\n:hsic — HSIC independence ICA (Gretton et al. 2005)\n:student_t — Student-t ML (Lanne et al. 2017)\n:mixture_normal — Mixture of normals ML (Lanne et al. 2017)\n:pml — Pseudo-ML (Gouriéroux et al. 2017)\n:skew_normal — Skew-normal ML (Lanne & Luoto 2020)\n:nongaussian_ml — Unified non-Gaussian ML dispatcher (default: Student-t)\n:markov_switching — Markov-switching heteroskedasticity (Lütkepohl & Netšunajev 2017)\n:garch — GARCH-based heteroskedasticity (Normandin & Phaneuf 2004)\n:smooth_transition — Smooth-transition heteroskedasticity (requires transition_var)\n:external_volatility — External volatility regimes (requires regime_indicator)\n\nKeyword Arguments\n\nmax_draws::Int=100: Maximum draws for sign/narrative identification\ntransition_var::Union{Nothing,AbstractVector}=nothing: Transition variable for :smooth_transition\nregime_indicator::Union{Nothing,AbstractVector{Int}}=nothing: Regime indicator for :external_volatility\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.compute_irf-Union{Tuple{T}, Tuple{VARModel{T}, AbstractMatrix{T}, Int64}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.compute_irf","text":"compute_irf(model, Q, horizon) -> Array{T,3}\n\nCompute IRFs for rotation matrix Q. Returns (horizon × n × n) array. IRF[h, i, j] = response of variable i to shock j at horizon h-1.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.compute_structural_shocks-Union{Tuple{T}, Tuple{VARModel{T}, AbstractMatrix{T}}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.compute_structural_shocks","text":"Compute structural shocks: εₜ = Q'L⁻¹uₜ.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.generate_Q-Union{Tuple{Int64}, Tuple{T}, Tuple{Int64, Type{T}}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.generate_Q","text":"Generate random orthogonal matrix via QR decomposition (Haar measure).\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.identify_arias-Union{Tuple{T}, Tuple{VARModel{T}, SVARRestrictions, Int64}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.identify_arias","text":"identify_arias(model, restrictions, horizon; n_draws=1000, n_rotations=1000) -> AriasSVARResult\n\nIdentify SVAR using Arias et al. (2018) with zero and sign restrictions.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.identify_arias_bayesian-Tuple{Any, Int64, Int64, SVARRestrictions, Int64}","page":"Functions","title":"MacroEconometricModels.identify_arias_bayesian","text":"identify_arias_bayesian(chain, p, n, restrictions, horizon; data=nothing, n_rotations=100, quantiles=[0.16,0.5,0.84])\n\nApply Arias identification to each posterior draw. Returns IRF quantiles, mean, acceptance rates.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.identify_cholesky-Union{Tuple{VARModel{T}}, Tuple{T}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.identify_cholesky","text":"Identify via Cholesky decomposition (recursive ordering). Returns L where Σ = LL'.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.identify_long_run-Union{Tuple{VARModel{T}}, Tuple{T}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.identify_long_run","text":"Identify via long-run restrictions: long-run cumulative impact matrix is lower triangular.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.identify_narrative-Union{Tuple{T}, Tuple{VARModel{T}, Int64, Function, Function}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.identify_narrative","text":"identify_narrative(model, horizon, sign_check, narrative_check; max_draws=1000)\n\nCombine sign and narrative restrictions. Returns (Q, irf, shocks).\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.identify_sign-Union{Tuple{T}, Tuple{VARModel{T}, Int64, Function}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.identify_sign","text":"identify_sign(model, horizon, check_func; max_draws=1000) -> (Q, irf)\n\nFind Q satisfying sign restrictions via random draws.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.irf_mean-Union{Tuple{AriasSVARResult{T}}, Tuple{T}} where T","page":"Functions","title":"MacroEconometricModels.irf_mean","text":"Compute weighted mean IRF from AriasSVARResult.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.irf_percentiles-Union{Tuple{AriasSVARResult{T}}, Tuple{T}} where T","page":"Functions","title":"MacroEconometricModels.irf_percentiles","text":"Compute weighted IRF percentiles from AriasSVARResult.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.sign_restriction-Tuple{Int64, Int64, Symbol}","page":"Functions","title":"MacroEconometricModels.sign_restriction","text":"Create sign restriction: variable response has given sign (:positive/:negative) at horizon.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.zero_restriction-Tuple{Int64, Int64}","page":"Functions","title":"MacroEconometricModels.zero_restriction","text":"Create zero restriction: variable doesn't respond to shock at horizon.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels._simulate_irfs-Union{Tuple{T}, Tuple{VARModel{T}, Symbol, Int64, Any, Any, Symbol, Int64}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels._simulate_irfs","text":"Simulate IRFs for confidence intervals (bootstrap or asymptotic).\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels._simulate_var-Union{Tuple{T}, Tuple{AbstractMatrix{T}, AbstractMatrix{T}, AbstractMatrix{T}, Int64}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels._simulate_var","text":"Simulate VAR data from initial conditions and innovations.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.cumulative_irf-Union{Tuple{LPImpulseResponse{T}}, Tuple{T}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.cumulative_irf","text":"cumulative_irf(irf::LPImpulseResponse{T}) -> LPImpulseResponse{T}\n\nCompute cumulative impulse response: Σₛ₌₀ʰ β_s.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.irf-Tuple{MCMCChains.Chains, Int64, Int64, Int64}","page":"Functions","title":"MacroEconometricModels.irf","text":"irf(chain, p, n, horizon; method=:cholesky, quantiles=[0.16, 0.5, 0.84], ...)\n\nCompute Bayesian IRFs from MCMC chain with posterior quantiles.\n\nMethods\n\n:cholesky, :sign, :narrative, :long_run, :fastica, :jade, :sobi, :dcov, :hsic, :student_t, :mixture_normal, :pml, :skew_normal, :nongaussian_ml, :markov_switching, :garch, :smooth_transition, :external_volatility\n\nNote: :smooth_transition requires transition_var kwarg.       :external_volatility requires regime_indicator kwarg.\n\nUses process_posterior_samples and compute_posterior_quantiles from bayesian_utils.jl.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.irf-Tuple{StructuralLP}","page":"Functions","title":"MacroEconometricModels.irf","text":"irf(slp::StructuralLP) -> ImpulseResponse\n\nExtract the impulse response object from a structural LP result.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.irf-Union{Tuple{T}, Tuple{VARModel{T}, Int64}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.irf","text":"irf(model, horizon; method=:cholesky, ci_type=:none, reps=200, conf_level=0.95, ...)\n\nCompute IRFs with optional confidence intervals.\n\nMethods\n\n:cholesky, :sign, :narrative, :long_run, :fastica, :jade, :sobi, :dcov, :hsic, :student_t, :mixture_normal, :pml, :skew_normal, :nongaussian_ml, :markov_switching, :garch, :smooth_transition, :external_volatility\n\nNote: :smooth_transition requires transition_var kwarg.       :external_volatility requires regime_indicator kwarg.\n\nCI types\n\n:none, :bootstrap, :theoretical\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.lp_irf-Tuple{AbstractMatrix, Int64, Int64}","page":"Functions","title":"MacroEconometricModels.lp_irf","text":"lp_irf(Y::AbstractMatrix, shock_var::Int, horizon::Int; kwargs...) -> LPImpulseResponse\n\nConvenience function: estimate LP and extract IRF in one call.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.lp_irf-Union{Tuple{LPModel{T}}, Tuple{T}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.lp_irf","text":"lp_irf(model::LPModel{T}; conf_level::Real=0.95) -> LPImpulseResponse{T}\n\nExtract impulse response function with confidence intervals from LP model.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels._compute_fevd-Union{Tuple{T}, Tuple{Array{T, 3}, Int64, Int64}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels._compute_fevd","text":"Compute FEVD from IRF array: decomposition[i,j,h] = cumulative MSE contribution.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.fevd-Union{Tuple{T}, Tuple{VARModel{T}, Int64}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.fevd","text":"fevd(model, horizon; method=:cholesky, ...) -> FEVD\n\nCompute FEVD showing proportion of h-step forecast error variance attributable to each shock.\n\nMethods\n\n:cholesky, :sign, :narrative, :long_run, :fastica, :jade, :sobi, :dcov, :hsic, :student_t, :mixture_normal, :pml, :skew_normal, :nongaussian_ml, :markov_switching, :garch, :smooth_transition, :external_volatility\n\nNote: :smooth_transition requires transition_var kwarg.       :external_volatility requires regime_indicator kwarg.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels._compute_hd_contributions-Union{Tuple{T}, Tuple{AbstractMatrix{T}, Array{Matrix{T}, 1}}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels._compute_hd_contributions","text":"Compute historical decomposition contributions from structural shocks and MA coefficients. HD[t, i, j] = Σ{s=0}^{t-1} Θs[i, j] * ε_j(t-s)\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels._compute_initial_conditions-Union{Tuple{T}, Tuple{Matrix{T}, Array{T, 3}}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels._compute_initial_conditions","text":"Compute initial conditions as residual: actual - total shock contributions.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels._compute_structural_ma_coefficients-Union{Tuple{T}, Tuple{VARModel{T}, AbstractMatrix{T}, Int64}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels._compute_structural_ma_coefficients","text":"Compute structural MA coefficients Θs = Φs * P for s = 0, ..., horizon-1. Returns Vector{Matrix{T}} of length horizon.\n\nUses _compute_ma_coefficients from identification.jl to avoid code duplication.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.contribution-Union{Tuple{T}, Tuple{BayesianHistoricalDecomposition{T}, Int64, Int64}} where T","page":"Functions","title":"MacroEconometricModels.contribution","text":"contribution(hd::BayesianHistoricalDecomposition, var, shock; stat=:mean) -> Vector\n\nGet contribution time series for specific variable and shock (Bayesian).\n\nArguments\n\nhd: Bayesian historical decomposition result\nvar: Variable index (Int) or name (String)\nshock: Shock index (Int) or name (String)\nstat: :mean or quantile index (Int)\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.contribution-Union{Tuple{T}, Tuple{HistoricalDecomposition{T}, Int64, Int64}} where T","page":"Functions","title":"MacroEconometricModels.contribution","text":"contribution(hd::HistoricalDecomposition, var, shock) -> Vector\n\nGet contribution time series for specific variable and shock.\n\nArguments\n\nhd: Historical decomposition result\nvar: Variable index (Int) or name (String)\nshock: Shock index (Int) or name (String)\n\nExample\n\ncontrib_y1_s1 = contribution(hd, 1, 1)  # Contribution of shock 1 to variable 1\ncontrib_y1_s1 = contribution(hd, \"Var 1\", \"Shock 1\")\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.historical_decomposition-Tuple{MCMCChains.Chains, Int64, Int64, Int64}","page":"Functions","title":"MacroEconometricModels.historical_decomposition","text":"historical_decomposition(chain::Chains, p, n, horizon; data, ...) -> BayesianHistoricalDecomposition\n\nCompute Bayesian historical decomposition from MCMC chain with posterior quantiles.\n\nArguments\n\nchain::Chains: MCMC chain from estimate_bvar\np::Int: Number of lags\nn::Int: Number of variables\nhorizon::Int: Maximum horizon for MA coefficients\n\nKeyword Arguments\n\ndata::AbstractMatrix: Original data matrix (required)\nmethod::Symbol=:cholesky: Identification method\nquantiles::Vector{<:Real}=[0.16, 0.5, 0.84]: Posterior quantile levels\ncheck_func=nothing: Sign restriction check function\nnarrative_check=nothing: Narrative restriction check function\ntransition_var=nothing: Transition variable (for method=:smooth_transition)\nregime_indicator=nothing: Regime indicator (for method=:external_volatility)\n\nMethods\n\n:cholesky, :sign, :narrative, :long_run, :fastica, :jade, :sobi, :dcov, :hsic, :student_t, :mixture_normal, :pml, :skew_normal, :nongaussian_ml, :markov_switching, :garch, :smooth_transition, :external_volatility\n\nNote: :smooth_transition requires transition_var kwarg.       :external_volatility requires regime_indicator kwarg.\n\nReturns\n\nBayesianHistoricalDecomposition with posterior quantiles and means.\n\nExample\n\nchain = estimate_bvar(Y, 2; n_samples=500)\nhd = historical_decomposition(chain, 2, 3, 198; data=Y)\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.historical_decomposition-Union{Tuple{T}, Tuple{StructuralLP{T}, Int64}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.historical_decomposition","text":"historical_decomposition(slp::StructuralLP{T}, T_hd::Int) -> HistoricalDecomposition{T}\n\nCompute historical decomposition from structural LP.\n\nUses LP-estimated IRFs as the structural MA coefficients Θ_h and the structural shocks from the underlying VAR identification.\n\nArguments\n\nslp: Structural LP result\nT_hd: Number of time periods for decomposition (≤ T_eff of underlying VAR)\n\nReturns\n\nHistoricalDecomposition{T} with contributions, initial conditions, and actual data.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.historical_decomposition-Union{Tuple{T}, Tuple{VARModel{T}, Int64}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.historical_decomposition","text":"historical_decomposition(model::VARModel, horizon; method=:cholesky, ...) -> HistoricalDecomposition\n\nCompute historical decomposition for a VAR model.\n\nDecomposes observed data into contributions from each structural shock plus initial conditions.\n\nArguments\n\nmodel::VARModel: Estimated VAR model\nhorizon::Int: Maximum horizon for MA coefficient computation (typically T_eff)\n\nKeyword Arguments\n\nmethod::Symbol=:cholesky: Identification method\ncheck_func=nothing: Sign restriction check function (for method=:sign or :narrative)\nnarrative_check=nothing: Narrative restriction check function (for method=:narrative)\nmax_draws::Int=1000: Maximum draws for sign/narrative identification\ntransition_var=nothing: Transition variable (for method=:smooth_transition)\nregime_indicator=nothing: Regime indicator (for method=:external_volatility)\n\nMethods\n\n:cholesky, :sign, :narrative, :long_run, :fastica, :jade, :sobi, :dcov, :hsic, :student_t, :mixture_normal, :pml, :skew_normal, :nongaussian_ml, :markov_switching, :garch, :smooth_transition, :external_volatility\n\nNote: :smooth_transition requires transition_var kwarg.       :external_volatility requires regime_indicator kwarg.\n\nReturns\n\nHistoricalDecomposition containing:\n\ncontributions: Shock contributions (Teff × nvars × n_shocks)\ninitial_conditions: Initial condition effects (Teff × nvars)\nactual: Actual data values\nshocks: Structural shocks\n\nExample\n\nmodel = estimate_var(Y, 2)\nhd = historical_decomposition(model, size(Y, 1) - 2)\nverify_decomposition(hd)  # Check decomposition identity\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.historical_decomposition-Union{Tuple{T}, Tuple{VARModel{T}, SVARRestrictions, Int64}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.historical_decomposition","text":"historical_decomposition(model::VARModel, restrictions::SVARRestrictions, horizon; ...) -> BayesianHistoricalDecomposition\n\nCompute historical decomposition using Arias et al. (2018) identification with importance weights.\n\nArguments\n\nmodel::VARModel: Estimated VAR model\nrestrictions::SVARRestrictions: Zero and sign restrictions\nhorizon::Int: Maximum horizon for MA coefficients\n\nKeyword Arguments\n\nn_draws::Int=1000: Number of accepted draws\nn_rotations::Int=1000: Maximum rotation attempts per draw\nquantiles::Vector{<:Real}=[0.16, 0.5, 0.84]: Quantile levels for weighted quantiles\n\nReturns\n\nBayesianHistoricalDecomposition with weighted posterior quantiles and means.\n\nExample\n\nr = SVARRestrictions(3; signs=[sign_restriction(1, 1, :positive)])\nhd = historical_decomposition(model, r, 198; n_draws=500)\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.total_shock_contribution-Union{Tuple{T}, Tuple{HistoricalDecomposition{T}, Int64}} where T","page":"Functions","title":"MacroEconometricModels.total_shock_contribution","text":"total_shock_contribution(hd::AbstractHistoricalDecomposition, var) -> Vector\n\nGet total contribution from all shocks to a variable over time.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.verify_decomposition-Union{Tuple{BayesianHistoricalDecomposition{T}}, Tuple{T}} where T","page":"Functions","title":"MacroEconometricModels.verify_decomposition","text":"verify_decomposition(hd::BayesianHistoricalDecomposition; tol=1e-6) -> Bool\n\nVerify that mean contributions + mean initial_conditions ≈ actual (approximately, due to averaging).\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.verify_decomposition-Union{Tuple{HistoricalDecomposition{T}}, Tuple{T}} where T","page":"Functions","title":"MacroEconometricModels.verify_decomposition","text":"verify_decomposition(hd::HistoricalDecomposition; tol=1e-10) -> Bool\n\nVerify that contributions + initial_conditions ≈ actual.\n\nExample\n\nhd = historical_decomposition(model, horizon)\n@assert verify_decomposition(hd) \"Decomposition identity failed\"\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.has_uncertainty-Tuple{AbstractAnalysisResult}","page":"Functions","title":"MacroEconometricModels.has_uncertainty","text":"has_uncertainty(result::AbstractAnalysisResult) -> Bool\n\nCheck if the result includes uncertainty quantification (confidence intervals or posterior quantiles).\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.point_estimate-Tuple{AbstractAnalysisResult}","page":"Functions","title":"MacroEconometricModels.point_estimate","text":"point_estimate(result::AbstractAnalysisResult)\n\nGet the point estimate from an analysis result.\n\nReturns the main values/estimates (IRF values, FEVD proportions, HD contributions).\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.print_table-Union{Tuple{T}, Tuple{IO, ARIMAForecast{T}}} where T","page":"Functions","title":"MacroEconometricModels.print_table","text":"print_table([io], fc::ARIMAForecast)\n\nPrint formatted ARIMA forecast table.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.print_table-Union{Tuple{T}, Tuple{IO, FEVD{T}, Int64}} where T","page":"Functions","title":"MacroEconometricModels.print_table","text":"print_table([io], f::FEVD, var; horizons=nothing)\n\nPrint formatted FEVD table.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.print_table-Union{Tuple{T}, Tuple{IO, FactorForecast{T}, Int64}} where T","page":"Functions","title":"MacroEconometricModels.print_table","text":"print_table([io], fc::FactorForecast, var_idx; type=:observable)\n\nPrint formatted factor forecast table for a single variable.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.print_table-Union{Tuple{T}, Tuple{IO, HistoricalDecomposition{T}, Int64}} where T","page":"Functions","title":"MacroEconometricModels.print_table","text":"print_table([io], hd::HistoricalDecomposition, var; periods=nothing)\n\nPrint formatted HD table.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.print_table-Union{Tuple{T}, Tuple{IO, ImpulseResponse{T}, Int64, Int64}} where T","page":"Functions","title":"MacroEconometricModels.print_table","text":"print_table([io], irf::ImpulseResponse, var, shock; horizons=nothing)\n\nPrint formatted IRF table.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.print_table-Union{Tuple{T}, Tuple{IO, LPFEVD{T}, Int64}} where T","page":"Functions","title":"MacroEconometricModels.print_table","text":"print_table([io], f::LPFEVD, var_idx; horizons=...)\n\nPrint formatted LP-FEVD table for variable var_idx.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.print_table-Union{Tuple{T}, Tuple{IO, LPForecast{T}}} where T","page":"Functions","title":"MacroEconometricModels.print_table","text":"print_table([io], fc::LPForecast)\n\nPrint formatted LP forecast table.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.print_table-Union{Tuple{T}, Tuple{IO, LPImpulseResponse{T}, Int64}} where T","page":"Functions","title":"MacroEconometricModels.print_table","text":"print_table([io], irf::LPImpulseResponse, var_idx)\n\nPrint formatted LP IRF table for a response variable.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.print_table-Union{Tuple{T}, Tuple{IO, StructuralLP{T}, Int64, Int64}} where T","page":"Functions","title":"MacroEconometricModels.print_table","text":"print_table([io], slp::StructuralLP, var, shock; horizons=nothing)\n\nPrint formatted IRF table for a specific variable-shock pair from structural LP.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.print_table-Union{Tuple{T}, Tuple{IO, VolatilityForecast{T}}} where T","page":"Functions","title":"MacroEconometricModels.print_table","text":"print_table([io], fc::VolatilityForecast)\n\nPrint formatted volatility forecast table.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.refs-Tuple{IO, Vector{Symbol}}","page":"Functions","title":"MacroEconometricModels.refs","text":"refs([io::IO], x; format=get_display_backend())\n\nPrint bibliographic references for a model, result, or method.\n\nSupports four output formats via the format keyword:\n\n:text — AEA plain text (default, follows get_display_backend())\n:latex — \\bibitem{} entries\n:bibtex — BibTeX @article{}/@book{} entries\n:html — HTML with clickable DOI links\n\nDispatch\n\nInstance dispatch: refs(model) prints references for the model type\nSymbol dispatch: refs(:fastica) prints references for a method name\n\nExamples\n\nmodel = estimate_var(Y, 2)\nrefs(model)                        # AEA text to stdout\nrefs(model; format=:bibtex)        # BibTeX entries\n\nrefs(:johansen)                    # Johansen (1991)\nrefs(:fastica; format=:latex)      # Hyvärinen (1999) as \\bibitem\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.report-Tuple{FEVD}","page":"Functions","title":"MacroEconometricModels.report","text":"report(f::FEVD)\nreport(f::BayesianFEVD)\n\nPrint FEVD summary with decomposition at selected horizons.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.report-Tuple{HistoricalDecomposition}","page":"Functions","title":"MacroEconometricModels.report","text":"report(hd::HistoricalDecomposition)\nreport(hd::BayesianHistoricalDecomposition)\n\nPrint HD summary with contribution statistics.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.report-Tuple{ImpulseResponse}","page":"Functions","title":"MacroEconometricModels.report","text":"report(irf::ImpulseResponse)\nreport(irf::BayesianImpulseResponse)\n\nPrint IRF summary with values at selected horizons.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.report-Union{Tuple{VARModel{T}}, Tuple{T}} where T","page":"Functions","title":"MacroEconometricModels.report","text":"report(model::VARModel)\n\nPrint comprehensive VAR model summary including specification, per-equation coefficient estimates with standard errors and significance, information criteria, residual covariance, and stationarity check.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.table-Union{Tuple{ARIMAForecast{T}}, Tuple{T}} where T","page":"Functions","title":"MacroEconometricModels.table","text":"table(fc::ARIMAForecast) -> Matrix\n\nExtract ARIMA forecast data. Returns matrix with columns: [Horizon, Forecast, CIlo, CIhi, SE].\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.table-Union{Tuple{T}, Tuple{BayesianFEVD{T}, Int64}} where T","page":"Functions","title":"MacroEconometricModels.table","text":"table(f::BayesianFEVD, var; horizons=nothing, stat=:mean) -> Matrix\n\nExtract Bayesian FEVD values. stat can be :mean or quantile index.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.table-Union{Tuple{T}, Tuple{BayesianHistoricalDecomposition{T}, Int64}} where T","page":"Functions","title":"MacroEconometricModels.table","text":"table(hd::BayesianHistoricalDecomposition, var; periods=nothing, stat=:mean) -> Matrix\n\nExtract Bayesian HD contributions. stat can be :mean or quantile index.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.table-Union{Tuple{T}, Tuple{BayesianImpulseResponse{T}, Int64, Int64}} where T","page":"Functions","title":"MacroEconometricModels.table","text":"table(irf::BayesianImpulseResponse, var, shock; horizons=nothing) -> Matrix\n\nExtract Bayesian IRF values. Returns [Horizon, Mean, Q1, Q2, ...].\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.table-Union{Tuple{T}, Tuple{FEVD{T}, Int64}} where T","page":"Functions","title":"MacroEconometricModels.table","text":"table(f::FEVD, var; horizons=nothing) -> Matrix\n\nExtract FEVD proportions for a variable. Returns [Horizon, Shock1, Shock2, ...].\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.table-Union{Tuple{T}, Tuple{FactorForecast{T}, Int64}} where T","page":"Functions","title":"MacroEconometricModels.table","text":"table(fc::FactorForecast, var_idx::Int; type=:observable) -> Matrix\n\nExtract factor forecast data for a single variable. type=:observable returns observable forecasts, type=:factor returns factor forecasts. Returns matrix with columns: [Horizon, Forecast, CIlo, CIhi].\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.table-Union{Tuple{T}, Tuple{HistoricalDecomposition{T}, Int64}} where T","page":"Functions","title":"MacroEconometricModels.table","text":"table(hd::HistoricalDecomposition, var; periods=nothing) -> Matrix\n\nExtract HD contributions for a variable. Returns [Period, Actual, Shock1, ..., ShockN, Initial].\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.table-Union{Tuple{T}, Tuple{ImpulseResponse{T}, Int64, Int64}} where T","page":"Functions","title":"MacroEconometricModels.table","text":"table(irf::ImpulseResponse, var, shock; horizons=nothing) -> Matrix\n\nExtract IRF values for a variable-shock pair. Returns matrix with columns: [Horizon, IRF] or [Horizon, IRF, CIlo, CIhi].\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.table-Union{Tuple{T}, Tuple{LPImpulseResponse{T}, Int64}} where T","page":"Functions","title":"MacroEconometricModels.table","text":"table(irf::LPImpulseResponse, var_idx::Int) -> Matrix\n\nExtract LP IRF values for a response variable. Returns matrix with columns: [Horizon, IRF, SE, CIlo, CIhi].\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.table-Union{Tuple{VolatilityForecast{T}}, Tuple{T}} where T","page":"Functions","title":"MacroEconometricModels.table","text":"table(fc::VolatilityForecast) -> Matrix\n\nExtract volatility forecast data. Returns matrix with columns: [Horizon, Forecast, CIlo, CIhi, SE].\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.uncertainty_bounds-Tuple{AbstractAnalysisResult}","page":"Functions","title":"MacroEconometricModels.uncertainty_bounds","text":"uncertainty_bounds(result::AbstractAnalysisResult) -> Union{Nothing, Tuple}\n\nGet uncertainty bounds (lower, upper) if available, otherwise nothing.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels._block_bootstrap-Union{Tuple{T}, Tuple{AbstractMatrix{T}, Int64}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels._block_bootstrap","text":"Generate a block bootstrap sample from matrix Y.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels._build_structural_lp_arrays-Union{Tuple{T}, Tuple{Array{LPModel{T}, 1}, Int64, Int64, Type{T}}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels._build_structural_lp_arrays","text":"Extract 3D IRF and SE arrays from per-shock LP models.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels._lp_predict_at_horizon-Union{Tuple{T}, Tuple{Matrix{T}, Vector{Int64}, Matrix{T}, Int64, Int64}} where T","page":"Functions","title":"MacroEconometricModels._lp_predict_at_horizon","text":"_lp_predict_at_horizon(Y, response_vars, residuals_h, T_eff_h, h)\n\nReconstruct fitted values at horizon h via Yh − residualsh identity.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels._structural_lp_bootstrap-Union{Tuple{T}, Tuple{AbstractMatrix{T}, Int64, Int64, Int64, Symbol, Int64, Symbol, Int64, T, Any, Any, Int64}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels._structural_lp_bootstrap","text":"Block bootstrap for structural LP confidence intervals.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.build_control_columns!-Union{Tuple{T}, Tuple{AbstractMatrix{T}, AbstractMatrix{T}, Vararg{Int64, 4}}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.build_control_columns!","text":"build_control_columns!(X_h::AbstractMatrix{T}, Y::AbstractMatrix{T},\n                       t_start::Int, t_end::Int, lags::Int, start_col::Int) where T\n\nFill control (lagged Y) columns into regressor matrix X_h. Returns the next available column index.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.build_response_matrix-Union{Tuple{T}, Tuple{AbstractMatrix{T}, Int64, Int64, Int64, Vector{Int64}}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.build_response_matrix","text":"build_response_matrix(Y::AbstractMatrix{T}, h::Int, t_start::Int, t_end::Int,\n                      response_vars::Vector{Int}) where T\n\nBuild response matrix Y_h at horizon h.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.compare_var_lp-Union{Tuple{T}, Tuple{AbstractMatrix{T}, Int64}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.compare_var_lp","text":"compare_var_lp(Y::AbstractMatrix{T}, horizon::Int; lags::Int=4) where T\n\nCompare VAR-based and LP-based impulse responses.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.compute_block_robust_vcov-Union{Tuple{T}, Tuple{AbstractMatrix{T}, AbstractMatrix{T}, AbstractCovarianceEstimator}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.compute_block_robust_vcov","text":"compute_block_robust_vcov(X::AbstractMatrix{T}, U::AbstractMatrix{T},\n                          cov_estimator::AbstractCovarianceEstimator) where T\n\nCompute block-diagonal robust covariance for multi-equation system.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.compute_horizon_bounds-Tuple{Int64, Int64, Int64}","page":"Functions","title":"MacroEconometricModels.compute_horizon_bounds","text":"compute_horizon_bounds(T_obs::Int, h::Int, lags::Int) -> (t_start, t_end)\n\nCompute valid observation bounds for horizon h.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.construct_lp_matrices-Union{Tuple{T}, Tuple{AbstractMatrix{T}, Int64, Int64, Int64}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.construct_lp_matrices","text":"construct_lp_matrices(Y::AbstractMatrix{T}, shock_var::Int, h::Int, lags::Int;\n                      response_vars::Vector{Int}=collect(1:size(Y,2))) where T\n\nConstruct regressor and response matrices for LP regression at horizon h.\n\nReturns: (Yh, Xh, valid_idx)\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.create_cov_estimator-Union{Tuple{T}, Tuple{Symbol, Type{T}}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.create_cov_estimator","text":"create_cov_estimator(cov_type::Symbol, ::Type{T}; bandwidth::Int=0) where T\n\nCreate covariance estimator from symbol specification. Eliminates repeated if/else patterns across LP variants.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.estimate_lp-Union{Tuple{T}, Tuple{AbstractMatrix{T}, Int64, Int64}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.estimate_lp","text":"estimate_lp(Y::AbstractMatrix{T}, shock_var::Int, horizon::Int;\n            lags::Int=4, response_vars::Vector{Int}=collect(1:size(Y,2)),\n            cov_type::Symbol=:newey_west, bandwidth::Int=0,\n            conf_level::Real=0.95) -> LPModel{T}\n\nEstimate Local Projection impulse response functions (Jordà 2005).\n\nThe LP regression for horizon h:     y{t+h} = αh + βh * shockt + Γh * controlst + ε_{t+h}\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.estimate_lp_cholesky-Union{Tuple{T}, Tuple{AbstractMatrix{T}, Int64}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.estimate_lp_cholesky","text":"estimate_lp_cholesky(Y::AbstractMatrix{T}, horizon::Int;\n                     lags::Int=4, cov_type::Symbol=:newey_west, kwargs...) -> Vector{LPModel{T}}\n\nEstimate LP with Cholesky-orthogonalized shocks.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.estimate_lp_multi-Union{Tuple{T}, Tuple{AbstractMatrix{T}, Vector{Int64}, Int64}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.estimate_lp_multi","text":"estimate_lp_multi(Y::AbstractMatrix{T}, shock_vars::Vector{Int}, horizon::Int;\n                  kwargs...) -> Vector{LPModel{T}}\n\nEstimate LP for multiple shock variables.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.extract_shock_irf-Union{Tuple{T}, Tuple{Array{Matrix{T}, 1}, Array{Matrix{T}, 1}, Vector{Int64}, Int64}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.extract_shock_irf","text":"extract_shock_irf(B::Vector{Matrix{T}}, vcov::Vector{Matrix{T}},\n                  response_vars::Vector{Int}, shock_coef_idx::Int;\n                  conf_level::Real=0.95) where T\n\nGeneric IRF extraction from coefficient and covariance vectors. Works for LPModel, LPIVModel, PropensityLPModel.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.structural_lp-Union{Tuple{T}, Tuple{AbstractMatrix{T}, Int64}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.structural_lp","text":"structural_lp(Y::AbstractMatrix{T}, horizon::Int;\n              method=:cholesky, lags=4, var_lags=nothing,\n              cov_type=:newey_west, conf_level=0.95,\n              ci_type=:none, reps=200,\n              check_func=nothing, narrative_check=nothing,\n              max_draws=1000,\n              transition_var=nothing, regime_indicator=nothing) -> StructuralLP{T}\n\nEstimate structural LP impulse responses using VAR-based identification with LP estimation.\n\nAlgorithm:\n\nEstimate VAR(p) → obtain Σ and residuals\nCompute rotation matrix Q via chosen identification method\nCompute structural shocks εt = Q'L⁻¹ut\nFor each shock j, run LP with εj as regressor and Yeff as responses\nStack into 3D IRF array: irfs[h, i, j]\n\nArguments\n\nY: T × n data matrix\nhorizon: Maximum IRF horizon H\n\nKeyword Arguments\n\nmethod: Identification method\nlags: Number of LP control lags (default: 4)\nvar_lags: VAR lag order for identification (default: same as lags)\ncov_type: HAC estimator (:newey_west, :white)\nconf_level: Confidence level for CIs (default: 0.95)\nci_type: CI method (:none, :bootstrap)\nreps: Number of bootstrap replications\ncheck_func: Sign restriction check function (for :sign/:narrative)\nnarrative_check: Narrative check function (for :narrative)\nmax_draws: Maximum draws for sign/narrative identification\ntransition_var: Transition variable (for :smooth_transition)\nregime_indicator: Regime indicator (for :external_volatility)\n\nMethods\n\n:cholesky, :sign, :narrative, :long_run, :fastica, :jade, :sobi, :dcov, :hsic, :student_t, :mixture_normal, :pml, :skew_normal, :nongaussian_ml, :markov_switching, :garch, :smooth_transition, :external_volatility\n\nNote: :smooth_transition requires transition_var kwarg.       :external_volatility requires regime_indicator kwarg.\n\nReturns\n\nStructuralLP{T} with 3D IRFs, structural shocks, VAR model, and individual LP models.\n\nReferences\n\nPlagborg-Møller, M. & Wolf, C. K. (2021). \"Local Projections and VARs Estimate the Same Impulse Responses.\" Econometrica, 89(2), 955–980.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.estimate_lp_iv-Union{Tuple{T}, Tuple{AbstractMatrix{T}, Int64, AbstractMatrix{T}, Int64}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.estimate_lp_iv","text":"estimate_lp_iv(Y::AbstractMatrix{T}, shock_var::Int, instruments::AbstractMatrix{T},\n               horizon::Int; lags::Int=4, response_vars::Vector{Int}=collect(1:size(Y,2)),\n               cov_type::Symbol=:newey_west, bandwidth::Int=0) -> LPIVModel{T}\n\nEstimate LP with Instrumental Variables (Stock & Watson 2018) using 2SLS.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.first_stage_regression-Union{Tuple{T}, Tuple{AbstractVector{T}, AbstractMatrix{T}, AbstractMatrix{T}}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.first_stage_regression","text":"first_stage_regression(endog::AbstractVector{T}, instruments::AbstractMatrix{T},\n                       controls::AbstractMatrix{T}) -> NamedTuple\n\nFirst-stage regression for 2SLS with F-statistic for instrument relevance.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.lp_iv_irf-Union{Tuple{LPIVModel{T}}, Tuple{T}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.lp_iv_irf","text":"lp_iv_irf(model::LPIVModel{T}; conf_level::Real=0.95) -> LPImpulseResponse{T}\n\nExtract IRF from LP-IV model.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.sargan_test-Union{Tuple{T}, Tuple{LPIVModel{T}, Int64}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.sargan_test","text":"sargan_test(model::LPIVModel{T}, h::Int) -> NamedTuple\n\nSargan-Hansen J-test for overidentification at horizon h.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.tsls_regression-Union{Tuple{T}, Tuple{AbstractMatrix{T}, AbstractVector{T}, AbstractVector{T}, AbstractMatrix{T}}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.tsls_regression","text":"tsls_regression(Y::AbstractMatrix{T}, endog::AbstractVector{T},\n                endog_fitted::AbstractVector{T}, controls::AbstractMatrix{T};\n                cov_estimator::AbstractCovarianceEstimator=NeweyWestEstimator()) -> NamedTuple\n\nSecond-stage regression using fitted values from first stage.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.weak_instrument_test-Union{Tuple{LPIVModel{T}}, Tuple{T}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.weak_instrument_test","text":"weak_instrument_test(model::LPIVModel{T}; threshold::T=T(10.0)) -> NamedTuple\n\nTest for weak instruments using Stock-Yogo rule of thumb (F > 10).\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.bspline_basis-Tuple{AbstractVector{Int64}, Int64, Int64}","page":"Functions","title":"MacroEconometricModels.bspline_basis","text":"bspline_basis(horizons::AbstractVector{Int}, degree::Int, n_interior_knots::Int;\n              T::Type{<:AbstractFloat}=Float64) -> BSplineBasis{T}\n\nConstruct B-spline basis matrix for given horizons.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.bspline_basis_value-Union{Tuple{T}, Tuple{T, Int64, Int64, Vector{T}}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.bspline_basis_value","text":"bspline_basis_value(x::T, i::Int, degree::Int, knots::Vector{T}) where T\n\nEvaluate B-spline basis function using Cox-de Boor recursion.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.compare_smooth_lp-Union{Tuple{T}, Tuple{AbstractMatrix{T}, Int64, Int64}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.compare_smooth_lp","text":"compare_smooth_lp(Y::AbstractMatrix{T}, shock_var::Int, horizon::Int;\n                  lambda::T=T(1.0), kwargs...) -> NamedTuple\n\nCompare standard LP and smooth LP.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.cross_validate_lambda-Union{Tuple{T}, Tuple{AbstractMatrix{T}, Int64, Int64}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.cross_validate_lambda","text":"cross_validate_lambda(Y::AbstractMatrix{T}, shock_var::Int, horizon::Int;\n                      lambda_grid::Vector{T}=T.(10.0 .^ (-4:0.5:2)),\n                      k_folds::Int=5, kwargs...) -> T\n\nSelect optimal λ via k-fold cross-validation.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.estimate_smooth_lp-Union{Tuple{T}, Tuple{AbstractMatrix{T}, Int64, Int64}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.estimate_smooth_lp","text":"estimate_smooth_lp(Y::AbstractMatrix{T}, shock_var::Int, horizon::Int;\n                   degree::Int=3, n_knots::Int=4, lambda::T=T(0.0),\n                   lags::Int=4, response_vars::Vector{Int}=collect(1:size(Y,2)),\n                   cov_type::Symbol=:newey_west, bandwidth::Int=0) -> SmoothLPModel{T}\n\nEstimate Smooth LP with B-spline parameterization (Barnichon & Brownlees 2019).\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.roughness_penalty_matrix-Union{Tuple{BSplineBasis{T}}, Tuple{T}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.roughness_penalty_matrix","text":"roughness_penalty_matrix(basis::BSplineBasis{T}) -> Matrix{T}\n\nCompute roughness penalty matrix R for B-splines (second derivative penalty).\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.smooth_lp_irf-Union{Tuple{SmoothLPModel{T}}, Tuple{T}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.smooth_lp_irf","text":"smooth_lp_irf(model::SmoothLPModel{T}; conf_level::Real=0.95) -> LPImpulseResponse{T}\n\nExtract smoothed IRF from SmoothLPModel.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.estimate_state_lp-Union{Tuple{T}, Tuple{AbstractMatrix{T}, Int64, AbstractVector{T}, Int64}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.estimate_state_lp","text":"estimate_state_lp(Y::AbstractMatrix{T}, shock_var::Int, state_var::AbstractVector{T},\n                  horizon::Int; gamma::Union{T,Symbol}=:estimate, ...) -> StateLPModel{T}\n\nEstimate state-dependent LP (Auerbach & Gorodnichenko 2013).\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.estimate_transition_params-Union{Tuple{T}, Tuple{AbstractVector{T}, AbstractMatrix{T}, Int64}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.estimate_transition_params","text":"estimate_transition_params(state_var::AbstractVector{T}, Y::AbstractMatrix{T},\n                           shock_var::Int; method::Symbol=:nlls, ...) -> NamedTuple\n\nEstimate smooth transition parameters (γ, c).\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.exponential_transition-Union{Tuple{T}, Tuple{AbstractVector{T}, T, T}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.exponential_transition","text":"exponential_transition(z::AbstractVector{T}, gamma::T, c::T) -> Vector{T}\n\nExponential (symmetric) transition: F(z) = 1 - exp(-γ(z - c)²)\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.indicator_transition-Union{Tuple{T}, Tuple{AbstractVector{T}, T}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.indicator_transition","text":"indicator_transition(z::AbstractVector{T}, c::T) -> Vector{T}\n\nSharp indicator transition: F(z) = 1 if z ≥ c, else 0\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.logistic_transition-Union{Tuple{T}, Tuple{AbstractVector{T}, T, T}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.logistic_transition","text":"logistic_transition(z::AbstractVector{T}, gamma::T, c::T) -> Vector{T}\n\nLogistic transition function: F(z) = exp(-γ(z - c)) / (1 + exp(-γ(z - c)))\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.state_irf-Union{Tuple{StateLPModel{T}}, Tuple{T}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.state_irf","text":"state_irf(model::StateLPModel{T}; regime::Symbol=:both, conf_level::Real=0.95) -> NamedTuple\n\nExtract state-dependent IRFs.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.test_regime_difference-Union{Tuple{StateLPModel{T}}, Tuple{T}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.test_regime_difference","text":"test_regime_difference(model::StateLPModel{T}; h::Union{Int,Nothing}=nothing) -> NamedTuple\n\nTest whether IRFs differ across regimes.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.doubly_robust_lp-Union{Tuple{T}, Tuple{AbstractMatrix{T}, AbstractVector{Bool}, AbstractMatrix{T}, Int64}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.doubly_robust_lp","text":"doubly_robust_lp(Y::AbstractMatrix{T}, treatment::AbstractVector{Bool},\n                 covariates::AbstractMatrix{T}, horizon::Int; ...) -> PropensityLPModel{T}\n\nDoubly robust LP estimator combining IPW and regression adjustment.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.estimate_propensity_lp-Union{Tuple{T}, Tuple{AbstractMatrix{T}, AbstractVector{Bool}, AbstractMatrix{T}, Int64}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.estimate_propensity_lp","text":"estimate_propensity_lp(Y::AbstractMatrix{T}, treatment::AbstractVector{Bool},\n                       covariates::AbstractMatrix{T}, horizon::Int; ...) -> PropensityLPModel{T}\n\nEstimate LP with Inverse Propensity Weighting (Angrist et al. 2018).\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.estimate_propensity_score-Union{Tuple{T}, Tuple{AbstractVector{Bool}, AbstractMatrix{T}}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.estimate_propensity_score","text":"estimate_propensity_score(treatment::AbstractVector{Bool}, X::AbstractMatrix{T};\n                          method::Symbol=:logit) -> Vector{T}\n\nEstimate propensity scores P(D=1|X) via logit or probit.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.inverse_propensity_weights-Union{Tuple{T}, Tuple{AbstractVector{Bool}, AbstractVector{T}}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.inverse_propensity_weights","text":"inverse_propensity_weights(treatment::AbstractVector{Bool}, propensity::AbstractVector{T};\n                           trimming::Tuple{T,T}=(T(0.01), T(0.99)), normalize::Bool=true) -> Vector{T}\n\nCompute IPW weights with optional trimming.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.propensity_diagnostics-Union{Tuple{PropensityLPModel{T}}, Tuple{T}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.propensity_diagnostics","text":"propensity_diagnostics(model::PropensityLPModel{T}) -> NamedTuple\n\nPropensity score diagnostics (overlap, balance).\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.propensity_irf-Union{Tuple{PropensityLPModel{T}}, Tuple{T}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.propensity_irf","text":"propensity_irf(model::PropensityLPModel{T}; conf_level::Real=0.95) -> LPImpulseResponse{T}\n\nExtract treatment effect (ATE) IRF from PropensityLPModel.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels._build_forecast_controls-Union{Tuple{LPModel{T}}, Tuple{T}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels._build_forecast_controls","text":"Build the control vector from LP model's last observations.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels._forecast_ci-Union{Tuple{T}, Tuple{Matrix{T}, Matrix{T}, T, Symbol}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels._forecast_ci","text":"Compute forecast CIs from SEs using normal quantiles.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels._lp_forecast_bootstrap-Union{Tuple{T}, Tuple{LPModel{T}, Vector{T}, Vector{T}, Int64, T}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels._lp_forecast_bootstrap","text":"Bootstrap CIs for LP forecasts via residual resampling.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.forecast-Union{Tuple{T}, Tuple{LPModel{T}, AbstractVector{<:Real}}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.forecast","text":"forecast(lp::LPModel{T}, shock_path::AbstractVector{<:Real};\n         ci_method=:analytical, conf_level=0.95, n_boot=500) -> LPForecast{T}\n\nCompute direct multi-step LP forecasts given a shock trajectory.\n\nFor each horizon h=1,...,H, the forecast uses the LP regression coefficients:     ŷ{T+h} = αh + βh·shockh + Γh·controlsT\n\nwhere controls_T are the last lags observations of Y.\n\nArguments\n\nlp: Estimated LP model\nshock_path: Vector of length H with assumed future shock values\n\nKeyword Arguments\n\nci_method: :analytical (default), :bootstrap, or :none\nconf_level: Confidence level (default: 0.95)\nn_boot: Number of bootstrap replications (default: 500)\n\nReturns\n\nLPForecast{T} with point forecasts, CIs, and standard errors.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.forecast-Union{Tuple{T}, Tuple{StructuralLP{T}, Int64, AbstractVector{<:Real}}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.forecast","text":"forecast(slp::StructuralLP{T}, shock_idx::Int, shock_path::AbstractVector{<:Real};\n         ci_method=:analytical, conf_level=0.95, n_boot=500) -> LPForecast{T}\n\nCompute direct multi-step forecast from a structural LP model using a specific orthogonalized shock.\n\nArguments\n\nslp: Structural LP model\nshock_idx: Index of the structural shock to use (1:n)\nshock_path: Vector of length H with assumed shock values\n\nKeyword Arguments\n\nci_method: :analytical (default), :bootstrap, or :none\nconf_level: Confidence level (default: 0.95)\nn_boot: Number of bootstrap replications (default: 500)\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels._compute_lp_fevd_h-Union{Tuple{T}, Tuple{LPModel{T}, Vector{T}, Int64, Int64, Symbol}} where T","page":"Functions","title":"MacroEconometricModels._compute_lp_fevd_h","text":"Dispatch to R², LP-A, or LP-B estimator at horizon h.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels._lp_fevd_bootstrap-Union{Tuple{T}, Tuple{Vector{T}, Vector{T}, Int64, Int64, Vector{T}, Int64, T, Union{Nothing, Int64}}} where T","page":"Functions","title":"MacroEconometricModels._lp_fevd_bootstrap","text":"Bootstrap bias correction and CIs for LP-FEVD.\n\nFit bivariate VAR(L) on (z, y) with HQIC lag selection\nCompute 'true' FEVD from VAR (theoretical benchmark)\nSimulate B samples from VAR, compute LP-FEVD for each\nBias = mean(bootstrap FEVD) - true FEVD\nBias-corrected = raw - bias\nCIs from centered bootstrap distribution (Kilian 1998)\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels._lp_fevd_lpa_h-Union{Tuple{T}, Tuple{LPModel{T}, Vector{T}, Int64, Int64}} where T","page":"Functions","title":"MacroEconometricModels._lp_fevd_lpa_h","text":"_lp_fevd_lpa_h(lp_model, shock, resp_idx, h) -> T\n\nLP-A FEVD: ŝh = (Σ{i=0}^{h} β̂₀^{i,LP}² σ̂z²) / Var(f̂{t+h|t-1}). Uses IRF coefficients directly — no R² regression needed.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels._lp_fevd_lpb_h-Union{Tuple{T}, Tuple{Vector{T}, Vector{T}, LPModel{T}, Int64, Int64, Int64}} where T","page":"Functions","title":"MacroEconometricModels._lp_fevd_lpb_h","text":"_lp_fevd_lpb_h(f_hat, shock, lp_model, resp_idx, t_start, h) -> T\n\nLP-B FEVD: ŝh = numerator / (numerator + Var(ṽ{t+h|t-1})), where ṽ is the residual from the R²-regression (Eq. 6).\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels._lp_fevd_r2_h-Union{Tuple{T}, Tuple{Vector{T}, Vector{T}, Int64, Int64}} where T","page":"Functions","title":"MacroEconometricModels._lp_fevd_r2_h","text":"_lp_fevd_r2_h(f_hat, shock, t_start, h) -> T\n\nR² FEVD at horizon h: regress forecast errors f̂ on shock leads [z{t+h},...,zt].\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels._r2_on_shock_leads-Union{Tuple{T}, Tuple{Vector{T}, Vector{T}, Int64, Int64}} where T","page":"Functions","title":"MacroEconometricModels._r2_on_shock_leads","text":"Regress fhat on [1, z{t+h}, z{t+h-1}, ..., zt] and return R².\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels._r2_residual_variance-Union{Tuple{T}, Tuple{Vector{T}, Vector{T}, Int64, Int64}} where T","page":"Functions","title":"MacroEconometricModels._r2_residual_variance","text":"Regress f_hat on shock leads and return Var(residuals).\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels._scalar_lp_fevd_r2-Union{Tuple{T}, Tuple{Vector{T}, Vector{T}, Int64, Int64}} where T","page":"Functions","title":"MacroEconometricModels._scalar_lp_fevd_r2","text":"Compute R²-based FEVD for scalar (z, y) pair at horizon h. Used in bootstrap bias correction.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels._simulate_from_var-Union{Tuple{T}, Tuple{VARModel{T}, Int64}} where T","page":"Functions","title":"MacroEconometricModels._simulate_from_var","text":"Simulate T_sim observations from a VAR model with burn-in.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels._var_theoretical_fevd-Union{Tuple{T}, Tuple{VARModel{T}, Int64}} where T","page":"Functions","title":"MacroEconometricModels._var_theoretical_fevd","text":"Compute theoretical FEVD of variable 2 w.r.t. shock 1 from bivariate VAR.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.fevd-Union{Tuple{T}, Tuple{StructuralLP{T}, Int64}} where T","page":"Functions","title":"MacroEconometricModels.fevd","text":"fevd(slp::StructuralLP{T}, horizon::Int; kwargs...) -> LPFEVD{T}\n\nCompute LP-based FEVD for structural LP results. Dispatches to lp_fevd.\n\nSee lp_fevd for full documentation.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.lp_fevd-Union{Tuple{T}, Tuple{StructuralLP{T}, Int64}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.lp_fevd","text":"lp_fevd(slp::StructuralLP{T}, horizon::Int; kwargs...) -> LPFEVD{T}\n\nCompute LP-based FEVD using the R²-based estimator of Gorodnichenko & Lee (2019).\n\nAt each horizon h, the share of variable i's forecast error variance due to shock j is estimated by regressing LP forecast error residuals on structural shock leads z{t+h}, z{t+h-1}, ..., z_t and computing R².\n\nArguments\n\nslp: Structural LP result from structural_lp()\nhorizon: Maximum FEVD horizon (capped at IRF horizon)\n\nKeyword Arguments\n\nmethod: Estimator (:r2, :lpa, :lpb). Default: :r2\nbias_correct: Apply VAR-based bootstrap bias correction. Default: true\nn_boot: Number of bootstrap replications. Default: 500\nconf_level: Confidence level for CIs. Default: 0.95\nvar_lags: VAR lag order for bias correction (default: HQIC-selected)\n\nReturns\n\nLPFEVD{T} with raw proportions, bias-corrected values, SEs, and CIs.\n\nReference\n\nGorodnichenko, Y. & Lee, B. (2019). \"Forecast Error Variance Decompositions with Local Projections.\" JBES, 38(4), 921–933.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.estimate_factors-Union{Tuple{T}, Tuple{AbstractMatrix{T}, Int64}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.estimate_factors","text":"estimate_factors(X, r; standardize=true) -> FactorModel\n\nEstimate static factor model Xt = Λ Ft + e_t via Principal Component Analysis.\n\nArguments\n\nX: Data matrix (T × N), observations × variables\nr: Number of factors to extract\n\nKeyword Arguments\n\nstandardize::Bool=true: Standardize data before estimation\n\nReturns\n\nFactorModel containing factors, loadings, eigenvalues, and explained variance.\n\nExample\n\nX = randn(200, 50)  # 200 observations, 50 variables\nfm = estimate_factors(X, 3)  # Extract 3 factors\nr2(fm)  # R² for each variable\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.forecast-Union{Tuple{T}, Tuple{FactorModel{T}, Int64}} where T","page":"Functions","title":"MacroEconometricModels.forecast","text":"forecast(model::FactorModel, h; p=1, ci_method=:none, conf_level=0.95, n_boot=1000)\n\nForecast factors and observables h steps ahead from a static factor model.\n\nInternally fits a VAR(p) on the extracted factors, then uses the VAR dynamics to produce multi-step forecasts and (optionally) confidence intervals.\n\nArguments\n\nmodel: Estimated static factor model\nh: Forecast horizon\n\nKeyword Arguments\n\np::Int=1: VAR lag order for factor dynamics\nci_method::Symbol=:none: CI method — :none, :theoretical, or :bootstrap\nconf_level::Real=0.95: Confidence level for intervals\nn_boot::Int=1000: Number of bootstrap replications (if ci_method=:bootstrap)\n\nReturns\n\nFactorForecast with factor and observable forecasts (and CIs if requested).\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.ic_criteria-Union{Tuple{T}, Tuple{AbstractMatrix{T}, Int64}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.ic_criteria","text":"ic_criteria(X, max_factors; standardize=true)\n\nCompute Bai-Ng (2002) information criteria IC1, IC2, IC3 for selecting the number of factors.\n\nArguments\n\nX: Data matrix (T × N)\nmax_factors: Maximum number of factors to consider\n\nReturns\n\nNamed tuple with IC values and optimal factor counts:\n\nIC1, IC2, IC3: Information criteria vectors\nr_IC1, r_IC2, r_IC3: Optimal factor counts\n\nExample\n\nresult = ic_criteria(X, 10)\nprintln(\"Optimal factors: IC1=\", result.r_IC1, \", IC2=\", result.r_IC2, \", IC3=\", result.r_IC3)\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.scree_plot_data-Tuple{FactorModel}","page":"Functions","title":"MacroEconometricModels.scree_plot_data","text":"scree_plot_data(m::FactorModel)\n\nReturn data for scree plot: factor indices, explained variance, cumulative variance.\n\nExample\n\ndata = scree_plot_data(fm)\n# Plot: data.factors vs data.explained_variance\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#StatsAPI.dof-Tuple{FactorModel}","page":"Functions","title":"StatsAPI.dof","text":"Degrees of freedom.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#StatsAPI.nobs-Tuple{FactorModel}","page":"Functions","title":"StatsAPI.nobs","text":"Number of observations.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#StatsAPI.predict-Tuple{FactorModel}","page":"Functions","title":"StatsAPI.predict","text":"Predicted values: F * Λ'.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#StatsAPI.r2-Union{Tuple{FactorModel{T}}, Tuple{T}} where T","page":"Functions","title":"StatsAPI.r2","text":"R² for each variable.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#StatsAPI.residuals-Union{Tuple{FactorModel{T}}, Tuple{T}} where T","page":"Functions","title":"StatsAPI.residuals","text":"Residuals: X - predicted.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels._compute_dfm_loglikelihood-Union{Tuple{T}, Tuple{AbstractMatrix{T}, AbstractMatrix{T}, AbstractMatrix{T}, AbstractMatrix{T}, Bool}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels._compute_dfm_loglikelihood","text":"Compute Gaussian log-likelihood given factors and parameters.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels._em_mstep-Union{Tuple{T}, Tuple{AbstractMatrix{T}, AbstractMatrix{T}, AbstractArray{T, 3}, AbstractArray{T, 3}, Int64, Int64, Bool}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels._em_mstep","text":"M-step: update loadings, VAR coefficients, and covariances.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels._estimate_dfm_em-Union{Tuple{T}, Tuple{AbstractMatrix{T}, Int64, Int64}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels._estimate_dfm_em","text":"EM algorithm for maximum likelihood estimation.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels._estimate_dfm_twostep-Union{Tuple{T}, Tuple{AbstractMatrix{T}, Int64, Int64}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels._estimate_dfm_twostep","text":"Two-step estimation: PCA for factors, then VAR on extracted factors.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.companion_matrix_factors-Union{Tuple{DynamicFactorModel{T}}, Tuple{T}} where T","page":"Functions","title":"MacroEconometricModels.companion_matrix_factors","text":"companion_matrix_factors(model::DynamicFactorModel)\n\nConstruct companion matrix for factor VAR dynamics.\n\nThe companion form converts VAR(p) to VAR(1): [Ft; F{t-1}; ...] = C [F{t-1}; F{t-2}; ...] + [η_t; 0; ...]\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.estimate_dynamic_factors-Union{Tuple{T}, Tuple{AbstractMatrix{T}, Int64, Int64}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.estimate_dynamic_factors","text":"estimate_dynamic_factors(X, r, p; method=:twostep, standardize=true, max_iter=100, tol=1e-6)\n\nEstimate dynamic factor model with VAR(p) factor dynamics.\n\nArguments\n\nX: Data matrix (T × N)\nr: Number of factors\np: Number of lags in factor VAR\n\nKeyword Arguments\n\nmethod::Symbol=:twostep: Estimation method (:twostep or :em)\nstandardize::Bool=true: Standardize data\nmax_iter::Int=100: Maximum EM iterations (if method=:em)\ntol::Float64=1e-6: Convergence tolerance (if method=:em)\ndiagonal_idio::Bool=true: Assume diagonal idiosyncratic covariance\n\nReturns\n\nDynamicFactorModel with factors, loadings, VAR coefficients, and diagnostics.\n\nExample\n\ndfm = estimate_dynamic_factors(X, 3, 2)  # 3 factors, VAR(2)\nforecast(dfm, 12)  # 12-step ahead forecast\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.forecast-Union{Tuple{T}, Tuple{DynamicFactorModel{T}, Int64}} where T","page":"Functions","title":"MacroEconometricModels.forecast","text":"forecast(model::DynamicFactorModel, h; ci_method=:none, conf_level=0.95, n_boot=1000, ci=false, ci_level=0.95)\n\nForecast factors and observables h steps ahead.\n\nArguments\n\nmodel: Estimated dynamic factor model\nh: Forecast horizon\n\nKeyword Arguments\n\nci_method::Symbol=:none: CI method — :none, :theoretical, :bootstrap, or :simulation\nconf_level::Real=0.95: Confidence level for intervals\nn_boot::Int=1000: Bootstrap replications (for :bootstrap and :simulation)\nci::Bool=false: Legacy keyword — ci=true maps to ci_method=:simulation\nci_level::Real=0.95: Legacy keyword — maps to conf_level\n\nReturns\n\nFactorForecast with factor and observable forecasts (and CIs if requested).\n\nExample\n\nfc = forecast(dfm, 12; ci_method=:theoretical)\nfc.observables       # h×N matrix of forecasts\nfc.observables_lower # h×N lower CI bounds\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.ic_criteria_dynamic-Union{Tuple{T}, Tuple{AbstractMatrix{T}, Int64, Int64}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.ic_criteria_dynamic","text":"ic_criteria_dynamic(X, max_r, max_p; standardize=true, method=:twostep)\n\nSelect (r, p) via AIC/BIC grid search over factor and lag combinations.\n\nReturns\n\nNamed tuple with AIC/BIC matrices and optimal (r, p) combinations.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.is_stationary-Tuple{DynamicFactorModel}","page":"Functions","title":"MacroEconometricModels.is_stationary","text":"is_stationary(model::DynamicFactorModel) -> Bool\n\nCheck if factor dynamics are stationary (max |eigenvalue| < 1).\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels._compute_common_spectral_density-Union{Tuple{T}, Tuple{Array{Complex{T}, 3}, AbstractMatrix}} where T","page":"Functions","title":"MacroEconometricModels._compute_common_spectral_density","text":"Compute spectral density of common component from loadings and eigenvalues.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels._compute_kernel_weights-Tuple{Int64, Symbol}","page":"Functions","title":"MacroEconometricModels._compute_kernel_weights","text":"Compute kernel weights for spectral smoothing.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels._compute_variance_explained-Union{Tuple{T}, Tuple{Matrix{T}, Int64}} where T","page":"Functions","title":"MacroEconometricModels._compute_variance_explained","text":"Compute variance explained by first q factors (averaged across frequencies).\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels._estimate_spectral_density-Union{Tuple{T}, Tuple{AbstractMatrix{T}, Int64, Symbol}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels._estimate_spectral_density","text":"Estimate spectral density matrix with kernel smoothing.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels._extract_time_domain_factors-Union{Tuple{T}, Tuple{AbstractMatrix{T}, Array{Complex{T}, 3}, Vector{T}}} where T","page":"Functions","title":"MacroEconometricModels._extract_time_domain_factors","text":"Extract time-domain factors via frequency-domain projection.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels._forecast_factors_ar-Union{Tuple{T}, Tuple{Matrix{T}, Int64}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels._forecast_factors_ar","text":"AR(1) forecast for each factor series.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels._reconstruct_time_domain-Union{Tuple{T}, Tuple{Array{Complex{T}, 3}, AbstractMatrix{T}}} where T","page":"Functions","title":"MacroEconometricModels._reconstruct_time_domain","text":"Reconstruct common component in time domain via inverse FFT.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels._select_bandwidth-Tuple{Int64}","page":"Functions","title":"MacroEconometricModels._select_bandwidth","text":"Automatic bandwidth selection: T^(1/3).\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels._spectral_eigendecomposition-Union{Tuple{Array{Complex{T}, 3}}, Tuple{T}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels._spectral_eigendecomposition","text":"Eigendecomposition of spectral density at each frequency.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.common_variance_share-Union{Tuple{GeneralizedDynamicFactorModel{T}}, Tuple{T}} where T","page":"Functions","title":"MacroEconometricModels.common_variance_share","text":"common_variance_share(model::GeneralizedDynamicFactorModel) -> Vector\n\nFraction of each variable's variance explained by the common component.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.estimate_gdfm-Union{Tuple{T}, Tuple{AbstractMatrix{T}, Int64}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.estimate_gdfm","text":"estimate_gdfm(X, q; standardize=true, bandwidth=0, kernel=:bartlett, r=0) -> GeneralizedDynamicFactorModel\n\nEstimate Generalized Dynamic Factor Model using spectral methods.\n\nArguments\n\nX: Data matrix (T × N)\nq: Number of dynamic factors\n\nKeyword Arguments\n\nstandardize::Bool=true: Standardize data\nbandwidth::Int=0: Kernel bandwidth (0 = automatic selection)\nkernel::Symbol=:bartlett: Kernel for spectral smoothing (:bartlett, :parzen, :tukey)\nr::Int=0: Number of static factors (0 = same as q)\n\nReturns\n\nGeneralizedDynamicFactorModel with common/idiosyncratic components and spectral loadings.\n\nExample\n\ngdfm = estimate_gdfm(X, 3)\ncommon_variance_share(gdfm)  # Fraction of variance explained by common component\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.forecast-Union{Tuple{T}, Tuple{GeneralizedDynamicFactorModel{T}, Int64}} where T","page":"Functions","title":"MacroEconometricModels.forecast","text":"forecast(model::GeneralizedDynamicFactorModel, h; method=:ar, ci_method=:none, conf_level=0.95, n_boot=1000)\n\nForecast h steps ahead using AR extrapolation of factors.\n\nArguments\n\nmodel: Estimated GDFM\nh: Forecast horizon\n\nKeyword Arguments\n\nmethod::Symbol=:ar: Forecasting method (currently only :ar supported)\nci_method::Symbol=:none: CI method — :none, :theoretical, or :bootstrap\nconf_level::Real=0.95: Confidence level for intervals\nn_boot::Int=1000: Bootstrap replications (for :bootstrap)\n\nReturns\n\nFactorForecast with factor and observable forecasts (and CIs if requested).\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.ic_criteria_gdfm-Union{Tuple{T}, Tuple{AbstractMatrix{T}, Int64}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.ic_criteria_gdfm","text":"ic_criteria_gdfm(X, max_q; standardize=true, bandwidth=0, kernel=:bartlett)\n\nInformation criteria for selecting number of dynamic factors.\n\nUses eigenvalue ratio test and cumulative variance threshold.\n\nReturns\n\nNamed tuple with:\n\neigenvalue_ratios: Ratios of consecutive eigenvalues\ncumulative_variance: Cumulative variance explained\nq_ratio: Optimal q from eigenvalue ratio\nq_variance: Optimal q from 90% variance threshold\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.spectral_eigenvalue_plot_data-Tuple{GeneralizedDynamicFactorModel}","page":"Functions","title":"MacroEconometricModels.spectral_eigenvalue_plot_data","text":"spectral_eigenvalue_plot_data(model::GeneralizedDynamicFactorModel)\n\nReturn data for plotting eigenvalues across frequencies.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#StatsAPI.dof-Tuple{GeneralizedDynamicFactorModel}","page":"Functions","title":"StatsAPI.dof","text":"Degrees of freedom.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#StatsAPI.nobs-Tuple{GeneralizedDynamicFactorModel}","page":"Functions","title":"StatsAPI.nobs","text":"Number of observations.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#StatsAPI.predict-Tuple{GeneralizedDynamicFactorModel}","page":"Functions","title":"StatsAPI.predict","text":"Predicted values (common component).\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#StatsAPI.r2-Union{Tuple{GeneralizedDynamicFactorModel{T}}, Tuple{T}} where T","page":"Functions","title":"StatsAPI.r2","text":"R² for each variable.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#StatsAPI.residuals-Tuple{GeneralizedDynamicFactorModel}","page":"Functions","title":"StatsAPI.residuals","text":"Residuals (idiosyncratic component).\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.estimate_gmm-Union{Tuple{T}, Tuple{Function, AbstractVector{T}, Any}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.estimate_gmm","text":"estimate_gmm(moment_fn::Function, theta0::AbstractVector{T}, data;\n             weighting::Symbol=:two_step, max_iter::Int=100,\n             tol::T=T(1e-8), hac::Bool=true, bandwidth::Int=0) -> GMMModel{T}\n\nEstimate parameters via Generalized Method of Moments.\n\nMinimizes: Q(θ) = g(θ)'W g(θ) where g(θ) = (1/n) Σᵢ gᵢ(θ)\n\nArguments:\n\nmoment_fn: Function (theta, data) -> Matrix of moment conditions (n × q)\ntheta0: Initial parameter guess\ndata: Data passed to moment function\nweighting: Weighting method (:identity, :optimal, :two_step, :iterated)\nmax_iter: Maximum iterations for optimization and/or iterated GMM\ntol: Convergence tolerance\nhac: Use HAC correction for optimal weighting\nbandwidth: HAC bandwidth (0 = automatic)\n\nReturns:\n\nGMMModel with estimates, covariance, and J-test results\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.estimate_lp_gmm-Union{Tuple{T}, Tuple{AbstractMatrix{T}, Int64, Int64}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.estimate_lp_gmm","text":"estimate_lp_gmm(Y::AbstractMatrix{T}, shock_var::Int, horizon::Int;\n                lags::Int=4, weighting::Symbol=:two_step) -> Vector{GMMModel{T}}\n\nEstimate Local Projection via GMM.\n\nReturns a GMMModel for each horizon.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.gmm_objective-Union{Tuple{T}, Tuple{AbstractVector{T}, Function, Any, AbstractMatrix{T}}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.gmm_objective","text":"gmm_objective(theta::AbstractVector{T}, moment_fn::Function, data,\n              W::AbstractMatrix{T}) -> T\n\nCompute GMM objective: Q(θ) = g(θ)'W g(θ)\n\nwhere g(θ) = (1/n) Σᵢ gᵢ(θ,data)\n\nArguments:\n\ntheta: Parameter vector\nmoment_fn: Function (theta, data) -> Matrix of moment conditions (n × q)\ndata: Data passed to moment function\nW: Weighting matrix (q × q)\n\nReturns:\n\nGMM objective value\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.gmm_summary-Union{Tuple{GMMModel{T}}, Tuple{T}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.gmm_summary","text":"gmm_summary(model::GMMModel{T}) -> NamedTuple\n\nSummary statistics for GMM estimation.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.identity_weighting-Union{Tuple{Int64}, Tuple{T}, Tuple{Int64, Type{T}}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.identity_weighting","text":"identity_weighting(n_moments::Int, ::Type{T}=Float64) -> Matrix{T}\n\nIdentity weighting matrix (one-step GMM).\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.j_test-Union{Tuple{GMMModel{T}}, Tuple{T}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.j_test","text":"j_test(model::GMMModel{T}) -> NamedTuple\n\nHansen's J-test for overidentifying restrictions.\n\nH0: All moment conditions are valid (E[g(θ₀)] = 0) H1: Some moment conditions are violated\n\nReturns:\n\nJ_stat: Test statistic\np_value: p-value from chi-squared distribution\ndf: Degrees of freedom (nmoments - nparams)\nreject_05: Whether to reject at 5% level\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.lp_gmm_moments-Union{Tuple{T}, Tuple{AbstractMatrix{T}, Int64, Int64, Any, Int64}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.lp_gmm_moments","text":"lp_gmm_moments(Y::AbstractMatrix{T}, shock_var::Int, h::Int, theta,\n               lags::Int) -> Matrix{T}\n\nConstruct moment conditions for LP estimated via GMM.\n\nMoments: E[Zt * ε{t+h}] = 0 where ε{t+h} = y{t+h} - θ' * X_t and Z includes all exogenous variables.\n\nThis is useful when you need to impose cross-equation restrictions.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.minimize_gmm-Union{Tuple{T}, Tuple{Function, AbstractVector{T}, Any, AbstractMatrix{T}}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.minimize_gmm","text":"minimize_gmm(moment_fn::Function, theta0::AbstractVector{T}, data,\n             W::AbstractMatrix{T}; max_iter::Int=100, tol::T=T(1e-8)) -> NamedTuple\n\nMinimize GMM objective using gradient descent with BFGS-like updates.\n\nReturns:\n\ntheta: Minimizer\nobjective: Final objective value\nconverged: Convergence flag\niterations: Number of iterations\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.numerical_gradient-Union{Tuple{T}, Tuple{Function, AbstractVector{T}}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.numerical_gradient","text":"numerical_gradient(f::Function, x::AbstractVector{T}; eps::T=T(1e-7)) -> Matrix{T}\n\nCompute numerical gradient (Jacobian) of function f at point x using central differences.\n\nArguments:\n\nf: Function that takes vector x and returns vector (moment conditions)\nx: Point at which to evaluate gradient\neps: Step size for finite differences\n\nReturns:\n\nJacobian matrix (nmoments × nparams)\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.optimal_weighting_matrix-Union{Tuple{T}, Tuple{Function, AbstractVector{T}, Any}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.optimal_weighting_matrix","text":"optimal_weighting_matrix(moment_fn::Function, theta::AbstractVector{T}, data;\n                         hac::Bool=true, bandwidth::Int=0) -> Matrix{T}\n\nCompute optimal GMM weighting matrix: W = inv(Var(g)).\n\nFor i.i.d. data: W = inv((1/n) Σᵢ gᵢ gᵢ') For time series: Uses HAC estimation with Newey-West kernel.\n\nArguments:\n\nmoment_fn: Moment function\ntheta: Current parameter estimate\ndata: Data\nhac: Use HAC correction for serial correlation\nbandwidth: HAC bandwidth (0 = automatic)\n\nReturns:\n\nOptimal weighting matrix (q × q)\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.adf_test-Union{Tuple{AbstractVector{T}}, Tuple{T}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.adf_test","text":"adf_test(y; lags=:aic, max_lags=nothing, regression=:constant) -> ADFResult\n\nAugmented Dickey-Fuller test for unit root.\n\nTests H₀: y has a unit root (non-stationary) against H₁: y is stationary.\n\nArguments\n\ny: Time series vector\nlags: Number of augmenting lags, or :aic/:bic/:hqic for automatic selection\nmax_lags: Maximum lags for automatic selection (default: floor(12*(T/100)^0.25))\nregression: Deterministic terms - :none, :constant (default), or :trend\n\nReturns\n\nADFResult containing test statistic, p-value, critical values, etc.\n\nExample\n\ny = cumsum(randn(200))  # Random walk (has unit root)\nresult = adf_test(y)\nresult.pvalue > 0.05  # Should fail to reject H₀\n\nReferences\n\nDickey, D. A., & Fuller, W. A. (1979). Distribution of the estimators for autoregressive time series with a unit root. JASA, 74(366), 427-431.\nMacKinnon, J. G. (2010). Critical values for cointegration tests. Queen's Economics Department Working Paper No. 1227.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.kpss_test-Union{Tuple{AbstractVector{T}}, Tuple{T}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.kpss_test","text":"kpss_test(y; regression=:constant, bandwidth=:auto) -> KPSSResult\n\nKwiatkowski-Phillips-Schmidt-Shin test for stationarity.\n\nTests H₀: y is stationary against H₁: y has a unit root.\n\nArguments\n\ny: Time series vector\nregression: :constant (level stationarity) or :trend (trend stationarity)\nbandwidth: Bartlett kernel bandwidth, or :auto for Newey-West selection\n\nReturns\n\nKPSSResult containing test statistic, p-value, critical values, etc.\n\nExample\n\ny = randn(200)  # Stationary series\nresult = kpss_test(y)\nresult.pvalue > 0.05  # Should fail to reject H₀ (stationarity)\n\nReferences\n\nKwiatkowski, D., Phillips, P. C., Schmidt, P., & Shin, Y. (1992). Testing the null hypothesis of stationarity against the alternative of a unit root. Journal of Econometrics, 54(1-3), 159-178.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.pp_test-Union{Tuple{AbstractVector{T}}, Tuple{T}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.pp_test","text":"pp_test(y; regression=:constant, bandwidth=:auto) -> PPResult\n\nPhillips-Perron test for unit root with non-parametric correction.\n\nTests H₀: y has a unit root against H₁: y is stationary.\n\nArguments\n\ny: Time series vector\nregression: :none, :constant (default), or :trend\nbandwidth: Newey-West bandwidth, or :auto for automatic selection\n\nReturns\n\nPPResult containing test statistic (Zt), p-value, critical values, etc.\n\nExample\n\ny = cumsum(randn(200))  # Random walk\nresult = pp_test(y)\nresult.pvalue > 0.05  # Should fail to reject H₀\n\nReferences\n\nPhillips, P. C., & Perron, P. (1988). Testing for a unit root in time series regression. Biometrika, 75(2), 335-346.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.za_test-Union{Tuple{AbstractVector{T}}, Tuple{T}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.za_test","text":"za_test(y; regression=:both, trim=0.15, lags=:aic, max_lags=nothing) -> ZAResult\n\nZivot-Andrews test for unit root with endogenous structural break.\n\nTests H₀: y has a unit root without break against H₁: y is stationary with break.\n\nArguments\n\ny: Time series vector\nregression: Type of break - :constant (intercept), :trend (slope), or :both\ntrim: Trimming fraction for break search (default 0.15)\nlags: Number of augmenting lags, or :aic/:bic for automatic selection\nmax_lags: Maximum lags for selection\n\nReturns\n\nZAResult containing minimum t-statistic, break point, p-value, etc.\n\nExample\n\n# Series with structural break\ny = vcat(randn(100), randn(100) .+ 2)\nresult = za_test(y; regression=:constant)\n\nReferences\n\nZivot, E., & Andrews, D. W. K. (1992). Further evidence on the great crash, the oil-price shock, and the unit-root hypothesis. JBES, 10(3), 251-270.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.ngperron_test-Union{Tuple{AbstractVector{T}}, Tuple{T}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.ngperron_test","text":"ngperron_test(y; regression=:constant) -> NgPerronResult\n\nNg-Perron unit root tests with GLS detrending (MZα, MZt, MSB, MPT).\n\nTests H₀: y has a unit root against H₁: y is stationary. These tests have better size properties than ADF/PP in small samples.\n\nArguments\n\ny: Time series vector\nregression: :constant (default) or :trend\n\nReturns\n\nNgPerronResult containing MZα, MZt, MSB, MPT statistics and critical values.\n\nExample\n\ny = cumsum(randn(100))\nresult = ngperron_test(y)\n# Check if MZt rejects at 5%\nresult.MZt < result.critical_values[:MZt][5]\n\nReferences\n\nNg, S., & Perron, P. (2001). Lag length selection and the construction of unit root tests with good size and power. Econometrica, 69(6), 1519-1554.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.johansen_test-Union{Tuple{T}, Tuple{AbstractMatrix{T}, Int64}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.johansen_test","text":"johansen_test(Y, p; deterministic=:constant) -> JohansenResult\n\nJohansen cointegration test for VAR system.\n\nTests for the number of cointegrating relationships among variables using trace and maximum eigenvalue tests.\n\nArguments\n\nY: Data matrix (T × n)\np: Number of lags in the VECM representation\ndeterministic: Specification for deterministic terms\n:none - No deterministic terms\n:constant - Constant in cointegrating relation (default)\n:trend - Linear trend in levels\n\nReturns\n\nJohansenResult containing trace and max-eigenvalue statistics, cointegrating vectors, adjustment coefficients, and estimated rank.\n\nExample\n\n# Generate cointegrated system\nn, T = 3, 200\nY = randn(T, n)\nY[:, 2] = Y[:, 1] + 0.1 * randn(T)  # Y2 cointegrated with Y1\n\nresult = johansen_test(Y, 2)\nresult.rank  # Should detect 1 or 2 cointegrating relations\n\nReferences\n\nJohansen, S. (1991). Estimation and hypothesis testing of cointegration vectors in Gaussian vector autoregressive models. Econometrica, 59(6), 1551-1580.\nOsterwald-Lenum, M. (1992). A note with quantiles of the asymptotic distribution of the ML cointegration rank test statistics. Oxford BEJM.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.is_stationary-Union{Tuple{VARModel{T}}, Tuple{T}} where T","page":"Functions","title":"MacroEconometricModels.is_stationary","text":"is_stationary(model::VARModel) -> VARStationarityResult\n\nCheck if estimated VAR model is stationary.\n\nA VAR(p) is stationary if and only if all eigenvalues of the companion matrix have modulus strictly less than 1.\n\nReturns\n\nVARStationarityResult with:\n\nis_stationary: Boolean indicating stationarity\neigenvalues: Complex eigenvalues of companion matrix\nmax_modulus: Maximum eigenvalue modulus\ncompanion_matrix: The (np × np) companion form matrix\n\nExample\n\nmodel = estimate_var(Y, 2)\nresult = is_stationary(model)\nif !result.is_stationary\n    println(\"Warning: VAR is non-stationary, max modulus = \", result.max_modulus)\nend\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.test_all_variables-Union{Tuple{AbstractMatrix{T}}, Tuple{T}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.test_all_variables","text":"test_all_variables(Y; test=:adf, kwargs...) -> Vector\n\nApply unit root test to each column of Y.\n\nArguments\n\nY: Data matrix (T × n)\ntest: Test to apply (:adf, :kpss, :pp, :za, :ngperron)\nkwargs...: Additional arguments passed to the test\n\nReturns\n\nVector of test results, one per variable.\n\nExample\n\nY = randn(200, 3)\nY[:, 1] = cumsum(Y[:, 1])  # Make first column non-stationary\nresults = test_all_variables(Y; test=:adf)\n[r.pvalue for r in results]  # P-values for each variable\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.unit_root_summary-Union{Tuple{AbstractVector{T}}, Tuple{T}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.unit_root_summary","text":"unit_root_summary(y; tests=[:adf, :kpss, :pp], kwargs...) -> NamedTuple\n\nRun multiple unit root tests and return summary with PrettyTables output.\n\nArguments\n\ny: Time series vector\ntests: Vector of test symbols to run (default: [:adf, :kpss, :pp])\nkwargs...: Additional arguments passed to individual tests\n\nReturns\n\nNamedTuple with test results, conclusion, and summary table.\n\nExample\n\ny = cumsum(randn(200))\nsummary = unit_root_summary(y)\nsummary.conclusion  # Overall conclusion\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.estimate_arch","page":"Functions","title":"MacroEconometricModels.estimate_arch","text":"estimate_arch(y, q; method=:mle) -> ARCHModel\n\nEstimate ARCH(q) model via Maximum Likelihood.\n\nσ²ₜ = ω + α₁ε²ₜ₋₁ + ... + αqε²ₜ₋q\n\nUses two-stage optimization: NelderMead initialization → LBFGS refinement.\n\nArguments\n\ny: Time series vector\nq: ARCH order (≥ 1)\nmethod: Estimation method (currently only :mle)\n\nReturns\n\nARCHModel with estimated parameters and conditional variances.\n\nExample\n\ny = randn(500)\nmodel = estimate_arch(y, 1)\nprintln(\"ω = \", model.omega, \", α₁ = \", model.alpha[1])\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.arch_lm_test","page":"Functions","title":"MacroEconometricModels.arch_lm_test","text":"arch_lm_test(y_or_model, q=5)\n\nARCH-LM test for conditional heteroskedasticity (Engle 1982).\n\nH₀: No ARCH effects (α₁ = ... = αq = 0) H₁: ARCH(q) effects present\n\nTest statistic: T·R² from regression of ε²ₜ on ε²ₜ₋₁,...,ε²ₜ₋q, distributed χ²(q).\n\nArguments\n\ny_or_model: Raw data vector or AbstractVolatilityModel (uses standardized residuals)\nq: Number of lags (default 5)\n\nReturns\n\nNamed tuple (statistic, pvalue, q).\n\nExample\n\nresult = arch_lm_test(randn(500), 5)\nprintln(\"p-value: \", result.pvalue)\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.ljung_box_squared","page":"Functions","title":"MacroEconometricModels.ljung_box_squared","text":"ljung_box_squared(z_or_model, K=10)\n\nLjung-Box test on squared (standardized) residuals.\n\nH₀: No serial correlation in z²ₜ H₁: Serial correlation present in z²ₜ\n\nTest statistic: Q = n(n+2) Σₖ ρ̂²ₖ/(n-k), distributed χ²(K).\n\nArguments\n\nz_or_model: Standardized residuals vector or AbstractVolatilityModel\nK: Number of lags (default 10)\n\nReturns\n\nNamed tuple (statistic, pvalue, K).\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.estimate_garch","page":"Functions","title":"MacroEconometricModels.estimate_garch","text":"estimate_garch(y, p=1, q=1; method=:mle) -> GARCHModel\n\nEstimate GARCH(p,q) model via Maximum Likelihood (Bollerslev 1986).\n\nσ²ₜ = ω + α₁ε²ₜ₋₁ + ... + αqε²ₜ₋q + β₁σ²ₜ₋₁ + ... + βpσ²ₜ₋p\n\nArguments\n\ny: Time series vector\np: GARCH order (default 1)\nq: ARCH order (default 1)\nmethod: Estimation method (currently only :mle)\n\nExample\n\nmodel = estimate_garch(y, 1, 1)\nprintln(\"Persistence: \", persistence(model))\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.estimate_egarch","page":"Functions","title":"MacroEconometricModels.estimate_egarch","text":"estimate_egarch(y, p=1, q=1; method=:mle) -> EGARCHModel\n\nEstimate EGARCH(p,q) model via Maximum Likelihood (Nelson 1991).\n\nlog(σ²ₜ) = ω + Σαᵢ(|zₜ₋ᵢ| - E|z|) + Σγᵢzₜ₋ᵢ + Σβⱼlog(σ²ₜ₋ⱼ)\n\nThe γ parameters capture leverage effects (typically γ < 0 for equities).\n\nArguments\n\ny: Time series vector\np: GARCH order (default 1)\nq: ARCH order (default 1)\nmethod: Estimation method (currently only :mle)\n\nExample\n\nmodel = estimate_egarch(y, 1, 1)\nprintln(\"Leverage: \", model.gamma[1])\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.estimate_gjr_garch","page":"Functions","title":"MacroEconometricModels.estimate_gjr_garch","text":"estimate_gjr_garch(y, p=1, q=1; method=:mle) -> GJRGARCHModel\n\nEstimate GJR-GARCH(p,q) model via Maximum Likelihood (Glosten, Jagannathan & Runkle 1993).\n\nσ²ₜ = ω + Σ(αᵢ + γᵢI(εₜ₋ᵢ<0))ε²ₜ₋ᵢ + Σβⱼσ²ₜ₋ⱼ\n\nγᵢ > 0 captures the asymmetric (leverage) effect.\n\nArguments\n\ny: Time series vector\np: GARCH order (default 1)\nq: ARCH order (default 1)\nmethod: Estimation method (currently only :mle)\n\nExample\n\nmodel = estimate_gjr_garch(y, 1, 1)\nprintln(\"Asymmetry: \", model.gamma[1])\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.news_impact_curve","page":"Functions","title":"MacroEconometricModels.news_impact_curve","text":"news_impact_curve(m; range=(-3,3), n_points=200)\n\nCompute the news impact curve: how a shock εₜ₋₁ maps to σ²ₜ, holding all else at unconditional values.\n\nReturns named tuple (shocks, variance) where both are vectors of length n_points.\n\nSupported models\n\nGARCHModel: Symmetric parabola\nGJRGARCHModel: Asymmetric parabola (steeper for negative shocks)\nEGARCHModel: Asymmetric exponential curve\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.estimate_sv","page":"Functions","title":"MacroEconometricModels.estimate_sv","text":"estimate_sv(y; n_samples=2000, n_adapts=1000, sampler=:nuts,\n            dist=:normal, leverage=false,\n            quantile_levels=[0.025, 0.5, 0.975]) -> SVModel\n\nEstimate a Stochastic Volatility model via Bayesian MCMC.\n\nModel\n\nyₜ = exp(hₜ/2) εₜ\nhₜ = μ + φ(hₜ₋₁ - μ) + σ_η ηₜ\n\nArguments\n\ny: Time series vector\nn_samples: Number of posterior samples (default 2000)\nn_adapts: Number of adaptation steps for NUTS (default 1000)\nsampler: MCMC sampler (:nuts, :hmc, etc.)\ndist: Error distribution (:normal or :studentt)\nleverage: Whether to include leverage effect (correlated innovations)\nquantile_levels: Quantile levels for volatility posterior\n\nExample\n\ny = randn(200) .* exp.(cumsum(0.1 .* randn(200)) ./ 2)\nmodel = estimate_sv(y; n_samples=1000)\nprintln(\"φ = \", mean(model.phi_post))\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.forecast-Union{Tuple{T}, Tuple{ARCHModel{T}, Int64}} where T","page":"Functions","title":"MacroEconometricModels.forecast","text":"forecast(m::ARCHModel, h; conf_level=0.95, n_sim=10000) -> VolatilityForecast\n\nForecast conditional variance from an ARCH(q) model.\n\nFor h > q, the forecast converges to the unconditional variance. Confidence intervals are computed via simulation.\n\nArguments\n\nm: Fitted ARCHModel\nh: Forecast horizon\nconf_level: Confidence level for intervals (default 0.95)\nn_sim: Number of simulation paths for CIs (default 10000)\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.forecast-Union{Tuple{T}, Tuple{EGARCHModel{T}, Int64}} where T","page":"Functions","title":"MacroEconometricModels.forecast","text":"forecast(m::EGARCHModel, h; conf_level=0.95, n_sim=10000) -> VolatilityForecast\n\nForecast conditional variance from an EGARCH(p,q) model via simulation.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.forecast-Union{Tuple{T}, Tuple{GARCHModel{T}, Int64}} where T","page":"Functions","title":"MacroEconometricModels.forecast","text":"forecast(m::GARCHModel, h; conf_level=0.95, n_sim=10000) -> VolatilityForecast\n\nForecast conditional variance from a GARCH(p,q) model.\n\nUses analytical iteration for point forecasts and simulation for CIs. Point forecasts converge to unconditional variance as h → ∞.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.forecast-Union{Tuple{T}, Tuple{GJRGARCHModel{T}, Int64}} where T","page":"Functions","title":"MacroEconometricModels.forecast","text":"forecast(m::GJRGARCHModel, h; conf_level=0.95, n_sim=10000) -> VolatilityForecast\n\nForecast conditional variance from a GJR-GARCH(p,q) model via simulation.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.forecast-Union{Tuple{T}, Tuple{SVModel{T}, Int64}} where T","page":"Functions","title":"MacroEconometricModels.forecast","text":"forecast(m::SVModel, h; conf_level=0.95) -> VolatilityForecast\n\nPosterior predictive forecast of volatility from an SV model.\n\nFor each MCMC draw (μ, φ, ση), simulates the log-volatility path forward h{T+1}, ..., h_{T+h} and returns quantiles of exp(hₜ).\n\nArguments\n\nm: Fitted SVModel\nh: Forecast horizon\nconf_level: Confidence level for intervals (default 0.95)\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.persistence","page":"Functions","title":"MacroEconometricModels.persistence","text":"Return persistence Σαᵢ for ARCH model.\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.halflife","page":"Functions","title":"MacroEconometricModels.halflife","text":"Return half-life of volatility shocks: log(0.5) / log(persistence).\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.unconditional_variance","page":"Functions","title":"MacroEconometricModels.unconditional_variance","text":"Return unconditional variance ω / (1 - Σαᵢ).\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.arch_order","page":"Functions","title":"MacroEconometricModels.arch_order","text":"Return ARCH order q.\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.garch_order","page":"Functions","title":"MacroEconometricModels.garch_order","text":"Return GARCH order p.\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels._matrix_table-Tuple{IO, AbstractMatrix, String}","page":"Functions","title":"MacroEconometricModels._matrix_table","text":"Print a labeled matrix as a PrettyTables table.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels._pretty_table-Tuple{IO, Any}","page":"Functions","title":"MacroEconometricModels._pretty_table","text":"_pretty_table(io::IO, data; kwargs...)\n\nCentral PrettyTables wrapper that respects the global display backend.\n\nFor :text backend, applies _TEXT_TABLE_FORMAT automatically. For :latex and :html backends, omits text-only formatting options.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels._select_horizons-Tuple{Int64}","page":"Functions","title":"MacroEconometricModels._select_horizons","text":"Select representative horizons for display.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.get_display_backend-Tuple{}","page":"Functions","title":"MacroEconometricModels.get_display_backend","text":"get_display_backend() -> Symbol\n\nReturn the current PrettyTables display backend (:text, :latex, or :html).\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.set_display_backend-Tuple{Symbol}","page":"Functions","title":"MacroEconometricModels.set_display_backend","text":"set_display_backend(backend::Symbol)\n\nSet the PrettyTables output backend. Options: :text (default), :latex, :html.\n\nExamples\n\nset_display_backend(:latex)   # all show() methods now emit LaTeX\nset_display_backend(:html)    # switch to HTML tables\nset_display_backend(:text)    # back to terminal-friendly text\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.refs","page":"Functions","title":"MacroEconometricModels.refs","text":"refs([io::IO], x; format=get_display_backend())\n\nPrint bibliographic references for a model, result, or method.\n\nSupports four output formats via the format keyword:\n\n:text — AEA plain text (default, follows get_display_backend())\n:latex — \\bibitem{} entries\n:bibtex — BibTeX @article{}/@book{} entries\n:html — HTML with clickable DOI links\n\nDispatch\n\nInstance dispatch: refs(model) prints references for the model type\nSymbol dispatch: refs(:fastica) prints references for a method name\n\nExamples\n\nmodel = estimate_var(Y, 2)\nrefs(model)                        # AEA text to stdout\nrefs(model; format=:bibtex)        # BibTeX entries\n\nrefs(:johansen)                    # Johansen (1991)\nrefs(:fastica; format=:latex)      # Hyvärinen (1999) as \\bibitem\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.jarque_bera_test","page":"Functions","title":"MacroEconometricModels.jarque_bera_test","text":"jarque_bera_test(model::VARModel; method=:multivariate) -> NormalityTestResult\njarque_bera_test(U::AbstractMatrix; method=:multivariate)  -> NormalityTestResult\n\nMultivariate Jarque-Bera test for normality of VAR residuals.\n\nMethods:\n\n:multivariate — joint test based on multivariate skewness and kurtosis (Lütkepohl 2005)\n:component — component-wise univariate JB tests on standardized residuals\n\nReference: Jarque & Bera (1980), Lütkepohl (2005, §4.5)\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.mardia_test","page":"Functions","title":"MacroEconometricModels.mardia_test","text":"mardia_test(model::VARModel; type=:both) -> NormalityTestResult\nmardia_test(U::AbstractMatrix; type=:both) -> NormalityTestResult\n\nMardia's tests for multivariate normality based on multivariate skewness and kurtosis.\n\nTypes:\n\n:skewness — tests multivariate skewness b₁,ₖ\n:kurtosis — tests multivariate kurtosis b₂,ₖ\n:both — combined test (sum of both statistics)\n\nUnder H₀: T·b₁,ₖ/6 ~ χ²(k(k+1)(k+2)/6), (b₂,ₖ - k(k+2)) / √(8k(k+2)/T) ~ N(0,1).\n\nReference: Mardia (1970)\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.doornik_hansen_test","page":"Functions","title":"MacroEconometricModels.doornik_hansen_test","text":"doornik_hansen_test(model::VARModel) -> NormalityTestResult\ndoornik_hansen_test(U::AbstractMatrix)  -> NormalityTestResult\n\nDoornik-Hansen omnibus test for multivariate normality.\n\nApplies the Bowman-Shenton transformation to each component's skewness and kurtosis, then sums z₁² + z₂² across components. Under H₀: DH ~ χ²(2k).\n\nReference: Doornik & Hansen (2008)\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.henze_zirkler_test","page":"Functions","title":"MacroEconometricModels.henze_zirkler_test","text":"henze_zirkler_test(model::VARModel) -> NormalityTestResult\nhenze_zirkler_test(U::AbstractMatrix)  -> NormalityTestResult\n\nHenze-Zirkler test for multivariate normality based on the empirical characteristic function.\n\nThe test statistic is:\n\nT_beta = frac1n sum_ij e^-beta^2 D_ij2 - 2(1+beta^2)^-k2 sum_i e^-beta^2 d_i^2(2(1+beta^2)) + n(1+2beta^2)^-k2\n\nwhere D_ij = (z_i - z_j)(z_i - z_j) and d_i = z_i z_i.\n\nReference: Henze & Zirkler (1990)\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.normality_test_suite","page":"Functions","title":"MacroEconometricModels.normality_test_suite","text":"normality_test_suite(model::VARModel) -> NormalityTestSuite\nnormality_test_suite(U::AbstractMatrix)  -> NormalityTestSuite\n\nRun all available multivariate normality tests and return a NormalityTestSuite.\n\nTests included:\n\nMultivariate Jarque-Bera\nComponent-wise Jarque-Bera\nMardia skewness\nMardia kurtosis\nMardia combined\nDoornik-Hansen\nHenze-Zirkler\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.identify_fastica","page":"Functions","title":"MacroEconometricModels.identify_fastica","text":"identify_fastica(model::VARModel; contrast=:logcosh, approach=:deflation,\n                 max_iter=200, tol=1e-6) -> ICASVARResult\n\nIdentify SVAR via FastICA (Hyvärinen 1999).\n\nRecovers independent non-Gaussian structural shocks by maximizing non-Gaussianity of the recovered sources.\n\nArguments:\n\ncontrast — non-Gaussianity measure: :logcosh (default, robust), :exp, :kurtosis\napproach — :deflation (one-by-one) or :symmetric (simultaneous)\nmax_iter — maximum iterations per component\ntol — convergence tolerance\n\nReference: Hyvärinen (1999)\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.identify_jade","page":"Functions","title":"MacroEconometricModels.identify_jade","text":"identify_jade(model::VARModel; max_iter=100, tol=1e-6) -> ICASVARResult\n\nIdentify SVAR via JADE (Joint Approximate Diagonalization of Eigenmatrices).\n\nUses fourth-order cumulant matrices and joint diagonalization via Jacobi rotations.\n\nReference: Cardoso & Souloumiac (1993)\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.identify_sobi","page":"Functions","title":"MacroEconometricModels.identify_sobi","text":"identify_sobi(model::VARModel; lags=1:12, max_iter=100, tol=1e-6) -> ICASVARResult\n\nIdentify SVAR via SOBI (Second-Order Blind Identification).\n\nUses autocovariance matrices at multiple lags and joint diagonalization. Exploits temporal structure rather than higher-order statistics.\n\nReference: Belouchrani et al. (1997)\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.identify_dcov","page":"Functions","title":"MacroEconometricModels.identify_dcov","text":"identify_dcov(model::VARModel; max_iter=200, tol=1e-6) -> ICASVARResult\n\nIdentify SVAR by minimizing pairwise distance covariance between recovered shocks.\n\nDistance covariance (Székely et al. 2007) is zero iff the variables are independent, making it a natural criterion for ICA.\n\nReference: Matteson & Tsay (2017)\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.identify_hsic","page":"Functions","title":"MacroEconometricModels.identify_hsic","text":"identify_hsic(model::VARModel; kernel=:gaussian, sigma=1.0,\n              max_iter=200, tol=1e-6) -> ICASVARResult\n\nIdentify SVAR by minimizing pairwise HSIC between recovered shocks.\n\nHSIC with a characteristic kernel (Gaussian) is zero iff variables are independent.\n\nReference: Gretton et al. (2005)\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.identify_student_t","page":"Functions","title":"MacroEconometricModels.identify_student_t","text":"identify_student_t(model::VARModel; max_iter=500, tol=1e-6) -> NonGaussianMLResult\n\nIdentify SVAR assuming Student-t distributed structural shocks.\n\nEach shock εⱼ ~ t(νⱼ) (standardized to unit variance). Identification is achieved when at most one νⱼ = ∞ (Gaussian).\n\nReference: Lanne, Meitz & Saikkonen (2017)\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.identify_mixture_normal","page":"Functions","title":"MacroEconometricModels.identify_mixture_normal","text":"identify_mixture_normal(model::VARModel; n_components=2, max_iter=500, tol=1e-6) -> NonGaussianMLResult\n\nIdentify SVAR assuming mixture-of-normals distributed structural shocks.\n\nEach shock εⱼ ~ pj N(0,σ₁ⱼ²) + (1-pj) N(0,σ₂ⱼ²) with unit variance constraint.\n\nReference: Lanne & Lütkepohl (2010)\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.identify_pml","page":"Functions","title":"MacroEconometricModels.identify_pml","text":"identify_pml(model::VARModel; max_iter=500, tol=1e-6) -> NonGaussianMLResult\n\nIdentify SVAR via Pseudo Maximum Likelihood using Pearson Type IV distributions.\n\nAllows both skewness and excess kurtosis in the structural shocks.\n\nReference: Herwartz (2018)\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.identify_skew_normal","page":"Functions","title":"MacroEconometricModels.identify_skew_normal","text":"identify_skew_normal(model::VARModel; max_iter=500, tol=1e-6) -> NonGaussianMLResult\n\nIdentify SVAR assuming skew-normal distributed structural shocks.\n\nEach shock εⱼ has pdf f(x) = 2 φ(x) Φ(αⱼ x), where αⱼ controls skewness.\n\nReference: Azzalini (1985)\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.identify_nongaussian_ml","page":"Functions","title":"MacroEconometricModels.identify_nongaussian_ml","text":"identify_nongaussian_ml(model::VARModel; distribution=:student_t,\n                        max_iter=500, tol=1e-6) -> NonGaussianMLResult\n\nUnified non-Gaussian ML SVAR identification dispatcher.\n\nSupported distributions:\n\n:student_t — independent Student-t shocks (Lanne, Meitz & Saikkonen 2017)\n:mixture_normal — mixture of two normals (Lanne & Lütkepohl 2010)\n:pml — Pearson Type IV / Pseudo-ML (Herwartz 2018)\n:skew_normal — skew-normal (Azzalini 1985)\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.identify_markov_switching","page":"Functions","title":"MacroEconometricModels.identify_markov_switching","text":"identify_markov_switching(model::VARModel; n_regimes=2, max_iter=500, tol=1e-6) -> MarkovSwitchingSVARResult\n\nIdentify SVAR via Markov-switching heteroskedasticity (Lanne & Lütkepohl 2008).\n\nEstimates regime-specific covariance matrices Σ₁, Σ₂, ..., Σ_K via EM algorithm, then identifies B₀ from the eigendecomposition of Σ₁⁻¹ Σ₂.\n\nIdentification requires that the relative variance ratios (eigenvalues) are distinct.\n\nReference: Lanne & Lütkepohl (2008), Rigobon (2003)\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.identify_garch","page":"Functions","title":"MacroEconometricModels.identify_garch","text":"identify_garch(model::VARModel; max_iter=500, tol=1e-6) -> GARCHSVARResult\n\nIdentify SVAR via GARCH-based heteroskedasticity (Normandin & Phaneuf 2004).\n\nIterative procedure:\n\nStart with Cholesky B₀\nCompute structural shocks εt = B₀⁻¹ ut\nFit GARCH(1,1) to each ε_j,t\nUse conditional covariances to re-estimate B₀\nRepeat until convergence\n\nReference: Normandin & Phaneuf (2004)\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.identify_smooth_transition","page":"Functions","title":"MacroEconometricModels.identify_smooth_transition","text":"identify_smooth_transition(model::VARModel, transition_var::AbstractVector;\n                           max_iter=500, tol=1e-6) -> SmoothTransitionSVARResult\n\nIdentify SVAR via smooth-transition heteroskedasticity (Lütkepohl & Netšunajev 2017).\n\nThe covariance matrix varies smoothly between two regimes:\n\nSigma_t = B_0 I + G(s_t)(Lambda - I) B_0\n\nwhere G(st) = 1/(1 + exp(-γ(st - c))) is the logistic transition function.\n\nArguments:\n\ntransition_var — the transition variable s_t (e.g., a lagged endogenous variable)\n\nReference: Lütkepohl & Netšunajev (2017)\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.identify_external_volatility","page":"Functions","title":"MacroEconometricModels.identify_external_volatility","text":"identify_external_volatility(model::VARModel, regime_indicator::AbstractVector{Int};\n                             regimes=2) -> ExternalVolatilitySVARResult\n\nIdentify SVAR via externally specified volatility regimes (Rigobon 2003).\n\nUses a known regime indicator (e.g., NBER recessions, financial crises) to split the sample and estimate regime-specific covariance matrices.\n\nArguments:\n\nregime_indicator — integer vector of regime labels (1, 2, ..., K)\nregimes — number of distinct regimes (default: 2)\n\nReference: Rigobon (2003)\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.test_identification_strength","page":"Functions","title":"MacroEconometricModels.test_identification_strength","text":"test_identification_strength(model::VARModel; method=:fastica,\n                             n_bootstrap=999) -> IdentifiabilityTestResult\n\nTest the strength of non-Gaussian identification via bootstrap.\n\nResamples residuals with replacement, re-estimates B₀, and computes the Procrustes distance between bootstrap and original B₀. Small distances indicate strong identification.\n\nReturns: test statistic = median Procrustes distance, p-value from distribution.\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.test_shock_gaussianity","page":"Functions","title":"MacroEconometricModels.test_shock_gaussianity","text":"test_shock_gaussianity(result::ICASVARResult) -> IdentifiabilityTestResult\ntest_shock_gaussianity(result::NonGaussianMLResult) -> IdentifiabilityTestResult\n\nTest whether recovered structural shocks are non-Gaussian using univariate JB tests.\n\nNon-Gaussian identification requires at most one shock to be Gaussian. This test checks each shock individually and reports the joint result.\n\nAt most one Gaussian shock → identification holds.\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.test_gaussian_vs_nongaussian","page":"Functions","title":"MacroEconometricModels.test_gaussian_vs_nongaussian","text":"test_gaussian_vs_nongaussian(model::VARModel; distribution=:student_t) -> IdentifiabilityTestResult\n\nLikelihood ratio test: H₀ Gaussian vs H₁ non-Gaussian structural shocks.\n\nUnder H₀, the LR statistic LR = 2(ℓ₁ - ℓ₀) ~ χ²(nextraparams).\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.test_shock_independence","page":"Functions","title":"MacroEconometricModels.test_shock_independence","text":"test_shock_independence(result::ICASVARResult; max_lag=10) -> IdentifiabilityTestResult\ntest_shock_independence(result::NonGaussianMLResult; max_lag=10) -> IdentifiabilityTestResult\n\nTest independence of recovered structural shocks.\n\nUses both cross-correlation (portmanteau) and distance covariance tests. Independence is a necessary condition for valid identification.\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.test_overidentification","page":"Functions","title":"MacroEconometricModels.test_overidentification","text":"test_overidentification(model::VARModel, result::AbstractNonGaussianSVAR;\n                        restrictions=nothing, n_bootstrap=499) -> IdentifiabilityTestResult\n\nTest overidentifying restrictions for non-Gaussian SVAR.\n\nWhen additional restrictions beyond non-Gaussianity are imposed (e.g., zero restrictions on B₀), this test checks whether those restrictions are consistent with the data.\n\nUses a bootstrap approach: compares the restricted log-likelihood to bootstrap distribution.\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels._default_names-Tuple{Int64, String}","page":"Functions","title":"MacroEconometricModels._default_names","text":"Generate default names: [\"prefix 1\", \"prefix 2\", ...]\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels._validate_narrative_data-Tuple{Symbol, AbstractMatrix}","page":"Functions","title":"MacroEconometricModels._validate_narrative_data","text":"Validate that narrative method has required data matrix.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels._validate_var_shock_indices-Tuple{String, String, Vector{String}, Vector{String}}","page":"Functions","title":"MacroEconometricModels._validate_var_shock_indices","text":"Resolve variable/shock names to indices, throwing on invalid names.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.companion_matrix-Union{Tuple{T}, Tuple{AbstractMatrix{T}, Int64, Int64}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.companion_matrix","text":"Construct companion matrix F for VAR(p) → VAR(1) representation.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.construct_var_matrices-Union{Tuple{T}, Tuple{AbstractMatrix{T}, Int64}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.construct_var_matrices","text":"Construct VAR design matrices: Yeff = X * B + U. Returns (Yeff, X) where X = [1, Y{t-1}, ..., Y{t-p}].\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.extract_ar_coefficients-Union{Tuple{T}, Tuple{AbstractMatrix{T}, Int64, Int64}} where T","page":"Functions","title":"MacroEconometricModels.extract_ar_coefficients","text":"Extract AR coefficient matrices [A₁, ..., Aₚ] from stacked B matrix.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.logdet_safe-Union{Tuple{AbstractMatrix{T}}, Tuple{T}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.logdet_safe","text":"Log determinant with eigenvalue fallback for numerical issues.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.robust_inv-Union{Tuple{AbstractMatrix{T}}, Tuple{T}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.robust_inv","text":"Compute inverse with fallback to pseudo-inverse for singular matrices.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.safe_cholesky-Union{Tuple{AbstractMatrix{T}}, Tuple{T}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.safe_cholesky","text":"Cholesky decomposition with automatic jitter for numerical stability.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.univariate_ar_variance-Union{Tuple{AbstractVector{T}}, Tuple{T}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.univariate_ar_variance","text":"AR(1) residual standard deviation for Minnesota prior scaling.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.validate_dynamic_factor_inputs-NTuple{4, Int64}","page":"Functions","title":"MacroEconometricModels.validate_dynamic_factor_inputs","text":"Validate dynamic factor model inputs.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.validate_factor_inputs-Tuple{Int64, Int64, Int64}","page":"Functions","title":"MacroEconometricModels.validate_factor_inputs","text":"Validate factor model inputs: 1 ≤ r ≤ min(T, N).\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.validate_in_range-Tuple{Real, String, Real, Real}","page":"Functions","title":"MacroEconometricModels.validate_in_range","text":"Validate lo ≤ value ≤ hi.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.validate_option-Tuple{Symbol, String, Tuple}","page":"Functions","title":"MacroEconometricModels.validate_option","text":"Validate symbol is in valid_options.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.validate_positive-Tuple{Real, String}","page":"Functions","title":"MacroEconometricModels.validate_positive","text":"Validate value > 0.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.validate_var_inputs-Tuple{Int64, Int64, Int64}","page":"Functions","title":"MacroEconometricModels.validate_var_inputs","text":"Validate VAR inputs: p ≥ 1, T > p + minobsfactor, n ≥ 1.\n\n\n\n\n\n","category":"method"},{"location":"manual/#Manual","page":"VAR","title":"Manual","text":"This manual provides a comprehensive theoretical background for the macroeconometric methods implemented in MacroEconometricModels.jl, including precise mathematical formulations and references to the literature.","category":"section"},{"location":"manual/#Quick-Start","page":"VAR","title":"Quick Start","text":"model = estimate_var(Y, 2)                                 # Estimate VAR(2) via OLS\nsel = select_lag_order(Y, 8)                               # AIC/BIC/HQIC lag selection\nirfs = irf(model, 20; method=:cholesky)                    # Cholesky-identified IRFs\ndecomp = fevd(model, 20)                                   # Forecast error variance decomposition\nid = identify_sign(model; check_func=f, n_draws=1000)      # Sign restriction identification\nhd = historical_decomposition(model, 198)                  # Historical decomposition\n\n","category":"section"},{"location":"manual/#Vector-Autoregression-(VAR)","page":"VAR","title":"Vector Autoregression (VAR)","text":"","category":"section"},{"location":"manual/#The-Reduced-Form-VAR-Model","page":"VAR","title":"The Reduced-Form VAR Model","text":"A VAR(p) model for an n-dimensional vector of endogenous variables y_t is defined as:\n\ny_t = c + A_1 y_t-1 + A_2 y_t-2 + cdots + A_p y_t-p + u_t\n\nwhere:\n\ny_t is an n times 1 vector of endogenous variables at time t\nc is an n times 1 vector of intercepts\nA_i are n times n coefficient matrices for lag i = 1 ldots p\nu_t is an n times 1 vector of reduced-form innovations with Eu_t = 0 and Eu_t u_t = Sigma\n\nReference: Sims (1980), Lütkepohl (2005, Chapter 2)","category":"section"},{"location":"manual/#Compact-Matrix-Representation","page":"VAR","title":"Compact Matrix Representation","text":"For estimation, we stack observations into matrices. Let T denote the effective sample size after accounting for lags. Define:\n\nY = beginbmatrix y_p+1  y_p+2  vdots  y_T endbmatrix_(T-p) times n quad\nX = beginbmatrix 1  y_p  y_p-1  cdots  y_1 \n1  y_p+1  y_p  cdots  y_2 \nvdots  vdots  vdots  ddots  vdots \n1  y_T-1  y_T-2  cdots  y_T-p endbmatrix_(T-p) times (1+np)\n\nThe VAR can be written in matrix form as:\n\nY = X B + U\n\nwhere B = c A_1 A_2 ldots A_p is a (1+np) times n coefficient matrix.","category":"section"},{"location":"manual/#OLS-Estimation","page":"VAR","title":"OLS Estimation","text":"The OLS estimator is given by:\n\nhatB = (XX)^-1 XY\n\nThe residual covariance matrix is estimated as:\n\nhatSigma = frac1T-p-k hatUhatU\n\nwhere hatU = Y - XhatB and k = 1 + np is the number of regressors per equation.\n\nReference: Hamilton (1994, Chapter 11), Lütkepohl (2005, Section 3.2)","category":"section"},{"location":"manual/#VARModel-Return-Values","page":"VAR","title":"VARModel Return Values","text":"estimate_var returns a VARModel{T} with the following fields:\n\nField Type Description\nY Matrix{T} Original T times n data matrix\np Int Number of lags\nB Matrix{T} (1+np) times n coefficient matrix c A_1 ldots A_p\nU Matrix{T} (T-p) times n residual matrix\nSigma Matrix{T} n times n residual covariance matrix\naic T Akaike Information Criterion\nbic T Bayesian Information Criterion\nhqic T Hannan-Quinn Information Criterion\n\nnote: Technical Note\nThe coefficient matrix B stores the intercept in the first row, followed by A_1 A_2 ldots A_p stacked vertically. To extract lag-i coefficients: A_i = model.B[(i-1)*n+2 : i*n+1, :]. The intercept is model.B[1, :].","category":"section"},{"location":"manual/#Stability-Condition","page":"VAR","title":"Stability Condition","text":"A VAR(p) is stable (stationary) if all eigenvalues of the companion matrix F lie inside the unit circle:\n\nF = beginbmatrix\nA_1  A_2  cdots  A_p-1  A_p \nI_n  0  cdots  0  0 \n0  I_n  cdots  0  0 \nvdots  vdots  ddots  vdots  vdots \n0  0  cdots  I_n  0\nendbmatrix_np times np\n\nStability Check: lambda_i  1 for all eigenvalues lambda_i of F.","category":"section"},{"location":"manual/#Information-Criteria-for-Lag-Selection","page":"VAR","title":"Information Criteria for Lag Selection","text":"The optimal lag length can be selected using information criteria:\n\nAkaike Information Criterion (AIC):\n\ntextAIC(p) = loghatSigma + frac2T(n^2 p + n)\n\nBayesian Information Criterion (BIC):\n\ntextBIC(p) = loghatSigma + fraclog TT(n^2 p + n)\n\nHannan-Quinn Criterion (HQ):\n\ntextHQ(p) = loghatSigma + frac2 log(log T)T(n^2 p + n)\n\nSelect the lag order p that minimizes the criterion.\n\nReference: Lütkepohl (2005, Section 4.3)\n\n","category":"section"},{"location":"manual/#Structural-VAR-(SVAR)-and-Identification","page":"VAR","title":"Structural VAR (SVAR) and Identification","text":"","category":"section"},{"location":"manual/#From-Reduced-Form-to-Structural-Shocks","page":"VAR","title":"From Reduced-Form to Structural Shocks","text":"The reduced-form residuals u_t are linear combinations of structural shocks varepsilon_t:\n\nu_t = B_0 varepsilon_t\n\nwhere:\n\nB_0 is the n times n contemporaneous impact matrix\nvarepsilon_t are structural shocks with Evarepsilon_t varepsilon_t = I_n\n\nThe relationship between the reduced-form and structural covariance is:\n\nSigma = B_0 B_0\n\nThe identification problem is that infinitely many B_0 matrices satisfy this condition. To identify structural shocks, we need n(n-1)2 additional restrictions.\n\nReference: Kilian & Lütkepohl (2017, Chapter 8)","category":"section"},{"location":"manual/#Cholesky-Identification-(Recursive)","page":"VAR","title":"Cholesky Identification (Recursive)","text":"The Cholesky decomposition imposes a lower triangular structure on B_0:\n\nB_0 = textchol(Sigma)\n\nThis implies a recursive causal ordering where variable i responds contemporaneously only to variables 1 2 ldots i-1.\n\nEconomic Interpretation: The ordering reflects assumptions about the speed of adjustment. Variables ordered first respond only to their own shocks contemporaneously.\n\nReference: Sims (1980), Christiano, Eichenbaum & Evans (1999)","category":"section"},{"location":"manual/#Sign-Restrictions","page":"VAR","title":"Sign Restrictions","text":"Sign restrictions identify structural shocks by constraining the signs of impulse responses at selected horizons. Let Theta_h denote the impulse response at horizon h. The identification algorithm:\n\nCompute the Cholesky decomposition: P = textchol(Sigma)\nDraw a random orthogonal matrix Q from the Haar measure (using QR decomposition of a random matrix)\nCompute candidate impact matrix: B_0 = PQ\nCheck if impulse responses Theta_0 = B_0 Theta_1 ldots satisfy the sign restrictions\nIf restrictions are satisfied, keep the draw; otherwise, discard and repeat\n\nImplementation: We use the algorithm of Rubio-Ramírez, Waggoner & Zha (2010).\n\nReference: Faust (1998), Uhlig (2005), Rubio-Ramírez, Waggoner & Zha (2010)","category":"section"},{"location":"manual/#Narrative-Restrictions","page":"VAR","title":"Narrative Restrictions","text":"Narrative restrictions combine sign restrictions with historical information about specific shocks at particular dates. Following Antolín-Díaz & Rubio-Ramírez (2018):\n\nShock Sign Narrative: At date t^*, structural shock j was positive/negative\nShock Contribution Narrative: At date t^*, shock j was the main driver of variable i\n\nThe algorithm:\n\nDraw orthogonal matrix Q satisfying sign restrictions\nRecover structural shocks: varepsilon = B_0^-1 u\nCheck if narrative constraints are satisfied\nWeight the draw using importance sampling\n\nReference: Antolín-Díaz & Rubio-Ramírez (2018)","category":"section"},{"location":"manual/#Long-Run-(Blanchard-Quah)-Identification","page":"VAR","title":"Long-Run (Blanchard-Quah) Identification","text":"Long-run restrictions constrain the cumulative effect of structural shocks. For a stationary VAR, the long-run impact matrix is:\n\nC(1) = (I_n - A_1 - A_2 - cdots - A_p)^-1 B_0\n\nBlanchard & Quah (1989) impose that certain shocks have zero long-run effect on specific variables by requiring C(1) to be lower triangular:\n\nC(1) = textcholleft( (I - A(1))^-1 Sigma (I - A(1))^-1 right)\n\nThen B_0 = (I - A(1)) C(1).\n\nEconomic Application: Demand shocks have no long-run effect on output (supply-driven long-run fluctuations).\n\nReference: Blanchard & Quah (1989), King, Plosser, Stock & Watson (1991)","category":"section"},{"location":"manual/#Arias-et-al.-(2018)-Identification","page":"VAR","title":"Arias et al. (2018) Identification","text":"When sign restrictions alone are insufficient, one can impose zero restrictions on specific impulse responses in addition to sign constraints. Arias, Rubio-Ramírez & Waggoner (2018) develop an algorithm that draws orthogonal rotation matrices Q from a distribution that is uniform over the set satisfying the zero restrictions, then filters for sign satisfaction.\n\nRestriction Types:\n\nType Function Description\nZero zero_restriction(var, shock; horizon=0) Variable var does not respond to shock at horizon\nSign sign_restriction(var, shock, :positive; horizon=0) Response has required sign at horizon\n\nAlgorithm: For n variables with r_j zero restrictions on shock j:\n\nCompute MA coefficients Phi_0 ldots Phi_H and Cholesky factor L\nFor each draw, construct Q column-by-column via QR decomposition in the null space of the zero restriction matrix\nCheck sign restrictions on the candidate IRF Theta_h = Phi_h L Q\nCorrect non-uniform sampling via importance weights when zero restrictions reduce the dimension\n\nusing MacroEconometricModels\nusing Random\n\nRandom.seed!(42)\nY = randn(200, 3)\nfor t in 2:200; Y[t,:] = 0.5*Y[t-1,:] + 0.3*randn(3); end\nmodel = estimate_var(Y, 2)\n\n# Define restrictions\nrestrictions = SVARRestrictions(3;\n    zeros = [zero_restriction(3, 1; horizon=0)],   # Shock 1 has no impact on var 3\n    signs = [sign_restriction(1, 1, :positive),     # Shock 1 → var 1 positive on impact\n             sign_restriction(2, 1, :positive)]     # Shock 1 → var 2 positive on impact\n)\n\n# Identify\nresult = identify_arias(model, restrictions, 20; n_draws=1000)\nprintln(\"Acceptance rate: \", round(result.acceptance_rate * 100, digits=1), \"%\")\n\n# Weighted IRF percentiles\npct = irf_percentiles(result; probs=[0.16, 0.5, 0.84])\nprintln(\"Median IRF(1→1, h=0): \", round(pct[1, 1, 1, 2], digits=3))\n\n# Bayesian version\n# bresult = identify_arias_bayesian(chain, p, n, restrictions, 20)\n\nThe acceptance rate indicates what fraction of random draws satisfy all restrictions simultaneously. Low rates (below 1%) suggest the restrictions may be nearly contradictory or overly stringent. The importance weights correct for non-uniform sampling induced by zero restrictions — the weighted percentiles provide correctly calibrated credible intervals.","category":"section"},{"location":"manual/#AriasSVARResult-Return-Values","page":"VAR","title":"AriasSVARResult Return Values","text":"Field Type Description\nQ_draws Vector{Matrix{T}} Accepted rotation matrices\nirf_draws Array{T,4} n_draws times H times n times n IRF draws\nweights Vector{T} Importance weights (normalized to sum to 1)\nacceptance_rate T Fraction of draws satisfying all restrictions\nrestrictions SVARRestrictions The imposed restrictions\n\nReference: Arias, Rubio-Ramírez & Waggoner (2018)\n\n","category":"section"},{"location":"manual/#Innovation-Accounting","page":"VAR","title":"Innovation Accounting","text":"For detailed coverage of innovation accounting tools, see the dedicated Innovation Accounting chapter. This includes:\n\nImpulse Response Functions (IRF): Dynamic effects of structural shocks\nForecast Error Variance Decomposition (FEVD): Variance contribution of each shock\nHistorical Decomposition (HD): Decompose observed movements into shock contributions\nSummary Tables: Publication-quality output with report(), table(), print_table()\n\n","category":"section"},{"location":"manual/#Bayesian-VAR-(BVAR)","page":"VAR","title":"Bayesian VAR (BVAR)","text":"For comprehensive coverage of Bayesian VAR estimation, see the dedicated Bayesian VAR chapter. Key topics include:\n\nMinnesota/Litterman prior specification\nHyperparameter optimization via marginal likelihood (Giannone, Lenza & Primiceri, 2015)\nMCMC estimation with Turing.jl\nPosterior inference and credible intervals\n\n","category":"section"},{"location":"manual/#Information-Criteria-and-Model-Selection","page":"VAR","title":"Information Criteria and Model Selection","text":"","category":"section"},{"location":"manual/#Log-Likelihood","page":"VAR","title":"Log-Likelihood","text":"For a Gaussian VAR, the log-likelihood is:\n\nlog L = -fracT cdot n2 log(2pi) - fracT2 logSigma - frac12 sum_t=1^T u_t Sigma^-1 u_t","category":"section"},{"location":"manual/#Marginal-Likelihood-(Bayesian)","page":"VAR","title":"Marginal Likelihood (Bayesian)","text":"For Bayesian model comparison, we use the marginal likelihood (also called evidence):\n\np(Y  mathcalM) = int p(Y  theta mathcalM) p(theta  mathcalM)  dtheta\n\nModels with higher marginal likelihood better balance fit and complexity.\n\n","category":"section"},{"location":"manual/#Covariance-Estimation","page":"VAR","title":"Covariance Estimation","text":"","category":"section"},{"location":"manual/#Newey-West-HAC-Estimator","page":"VAR","title":"Newey-West HAC Estimator","text":"For robust inference in the presence of heteroskedasticity and autocorrelation, we use the Newey-West (1987, 1994) estimator:\n\nhatV_NW = (XX)^-1 hatS (XX)^-1\n\nwhere the long-run covariance hatS is:\n\nhatS = hatGamma_0 + sum_j=1^m w_j (hatGamma_j + hatGamma_j)\n\nwith hatGamma_j = frac1T sum_t=j+1^T hatu_t hatu_t-j x_t x_t-j.","category":"section"},{"location":"manual/#Kernel-Functions","page":"VAR","title":"Kernel Functions","text":"The weight function w_j depends on the kernel:\n\nBartlett (Newey-West):\n\nw_j = 1 - fracjm+1\n\nParzen:\n\nw_j = begincases\n1 - 6x^2 + 6x^3  x leq 05 \n2(1-x)^3  05  x leq 1\nendcases\n\nwhere x = j(m+1).\n\nQuadratic Spectral (Andrews, 1991):\n\nw_j = frac2512pi^2 x^2 left( fracsin(6pi x5)6pi x5 - cos(6pi x5) right)","category":"section"},{"location":"manual/#Automatic-Bandwidth-Selection","page":"VAR","title":"Automatic Bandwidth Selection","text":"Newey & West (1994) provide a data-driven bandwidth:\n\nm^* = 11447 left( hatalpha cdot T right)^13\n\nwhere hatalpha is estimated from an AR(1) fit to the residuals:\n\nhatalpha = frac4hatrho^2(1-hatrho)^4","category":"section"},{"location":"manual/#White-Heteroscedasticity-Robust-Estimator-(HC0)","page":"VAR","title":"White Heteroscedasticity-Robust Estimator (HC0)","text":"When errors are heteroscedastic but serially uncorrelated, the White (1980) estimator provides consistent standard errors without requiring bandwidth selection:\n\nhatV_W = (XX)^-1 left( sum_t=1^T hatu_t^2 x_t x_t right) (XX)^-1\n\nwhere\n\nhatu_t are the OLS residuals\nx_t is the k times 1 regressor vector at time t","category":"section"},{"location":"manual/#Driscoll-Kraay-Panel-Robust-Estimator","page":"VAR","title":"Driscoll-Kraay Panel-Robust Estimator","text":"For panel data with both cross-sectional and temporal dependence, the Driscoll & Kraay (1998) estimator applies HAC estimation to the cross-sectional averages of the moment conditions. This produces standard errors robust to both heteroscedasticity, serial correlation, and cross-sectional dependence.","category":"section"},{"location":"manual/#Julia-Implementation","page":"VAR","title":"Julia Implementation","text":"using MacroEconometricModels\n\nY = randn(200, 3)\nfor t in 2:200; Y[t,:] = 0.5*Y[t-1,:] + 0.3*randn(3); end\n\n# Construct design matrices\nY_eff, X = construct_var_matrices(Y, 2)\nresiduals = Y_eff - X * ((X'X) \\ (X'Y_eff))\n\n# Newey-West HAC (default: Bartlett kernel, automatic bandwidth)\nV_nw = newey_west(X, residuals; bandwidth=0, kernel=:bartlett)\n\n# White heteroscedasticity-robust (HC0)\nV_w = white_vcov(X, residuals)\n\n# Driscoll-Kraay for panel data\n# V_dk = driscoll_kraay(X, residuals; bandwidth=4)\n\n# Automatic bandwidth selection\nbw = optimal_bandwidth_nw(residuals)\nprintln(\"Optimal Newey-West bandwidth: \", bw)\n\nThe Newey-West estimator is appropriate for time series with heteroscedastic and serially correlated errors — the standard choice for LP and VAR applications. The White estimator is simpler but inconsistent when errors are autocorrelated. The Driscoll-Kraay estimator extends HAC to panel settings where cross-sectional units may be correlated (e.g., country-level macro panels).","category":"section"},{"location":"manual/#Comparing-LP-and-VAR","page":"VAR","title":"Comparing LP and VAR","text":"The compare_var_lp function provides a structured comparison of VAR and LP impulse responses:\n\ncomparison = compare_var_lp(Y, 1, 20; lags=4)\n\nThis estimates both a VAR and LP model on the same data and returns the IRFs from each, facilitating visual and numerical comparison. Under correct specification, the IRFs should be close (Plagborg-Møller & Wolf 2021); substantial disagreement suggests dynamic misspecification in the VAR.\n\nReference: Newey & West (1987, 1994), Andrews (1991), Driscoll & Kraay (1998)\n\n","category":"section"},{"location":"manual/#Complete-Example","page":"VAR","title":"Complete Example","text":"This example demonstrates an end-to-end VAR workflow from lag selection through structural analysis.\n\nusing MacroEconometricModels\nusing Random\n\nRandom.seed!(42)\n\n# Generate data from a persistent VAR(1) DGP\nT, n = 200, 3\nY = zeros(T, n)\nA = [0.8 0.1 -0.1; 0.05 0.7 0.0; 0.1 0.2 0.75]\nfor t in 2:T\n    Y[t, :] = A * Y[t-1, :] + 0.3 * randn(n)\nend\n\n# Step 1: Select lag order\nsel = select_lag_order(Y, 8)\nprintln(\"AIC lag: \", sel.p_aic, \"  BIC lag: \", sel.p_bic)\n\n# Step 2: Estimate VAR\nmodel = estimate_var(Y, sel.p_bic)\nprintln(\"AIC: \", round(model.aic, digits=2),\n        \"  BIC: \", round(model.bic, digits=2))\n\n# Step 3: Check stability\nstab = is_stationary(model)\nprintln(\"Stationary: \", stab.is_stationary,\n        \"  Max modulus: \", round(stab.max_modulus, digits=4))\n\n# Step 4: Cholesky IRF with bootstrap CI\nirfs = irf(model, 20; method=:cholesky, ci_type=:bootstrap, reps=500)\nprintln(\"Impact of shock 1 on var 1: \", round(irfs.values[1, 1, 1], digits=3))\nprintln(\"After 8 periods: \", round(irfs.values[9, 1, 1], digits=3))\n\n# Step 5: FEVD\ndecomp = fevd(model, 20)\nprintln(\"Var 1 explained by shock 1 at h=1: \",\n        round(decomp.proportions[1, 1, 1] * 100, digits=1), \"%\")\nprintln(\"Var 1 explained by shock 1 at h=20: \",\n        round(decomp.proportions[20, 1, 1] * 100, digits=1), \"%\")\n\n# Step 6: Historical decomposition\nhd = historical_decomposition(model, size(model.U, 1))\nverify_decomposition(hd)  # Should return true\n\nThe lag selection criteria typically agree when the true DGP is low-order; BIC tends to be more conservative and is preferred when parsimony matters. The stability check confirms all companion matrix eigenvalues lie inside the unit circle, validating the use of standard asymptotic inference. The FEVD at long horizons reveals the unconditional variance decomposition, showing which shocks are the dominant drivers of each variable. The historical decomposition identity y_t = sum_j textHD_j(t) + textinitial(t) should hold exactly up to numerical precision.\n\n","category":"section"},{"location":"manual/#References","page":"VAR","title":"References","text":"","category":"section"},{"location":"manual/#Vector-Autoregression","page":"VAR","title":"Vector Autoregression","text":"Christiano, Lawrence J., Martin Eichenbaum, and Charles L. Evans. 1999. \"Monetary Policy Shocks: What Have We Learned and to What End?\" In Handbook of Macroeconomics, Vol. 1, edited by John B. Taylor and Michael Woodford, 65–148. Amsterdam: Elsevier. https://doi.org/10.1016/S1574-0048(99)01005-8\nHamilton, James D. 1994. Time Series Analysis. Princeton, NJ: Princeton University Press. ISBN 978-0-691-04289-3.\nLütkepohl, Helmut. 2005. New Introduction to Multiple Time Series Analysis. Berlin: Springer. ISBN 978-3-540-40172-8.\nSims, Christopher A. 1980. \"Macroeconomics and Reality.\" Econometrica 48 (1): 1–48. https://doi.org/10.2307/1912017","category":"section"},{"location":"manual/#Structural-Identification","page":"VAR","title":"Structural Identification","text":"Arias, Jonas E., Juan F. Rubio-Ramírez, and Daniel F. Waggoner. 2018. \"Inference Based on Structural Vector Autoregressions Identified with Sign and Zero Restrictions: Theory and Applications.\" Econometrica 86 (2): 685–720. https://doi.org/10.3982/ECTA14468\nAntolín-Díaz, Juan, and Juan F. Rubio-Ramírez. 2018. \"Narrative Sign Restrictions for SVARs.\" American Economic Review 108 (10): 2802–2829. https://doi.org/10.1257/aer.20161852\nBlanchard, Olivier Jean, and Danny Quah. 1989. \"The Dynamic Effects of Aggregate Demand and Supply Disturbances.\" American Economic Review 79 (4): 655–673.\nFaust, Jon. 1998. \"The Robustness of Identified VAR Conclusions about Money.\" Carnegie-Rochester Conference Series on Public Policy 49: 207–244. https://doi.org/10.1016/S0167-2231(99)00009-3\nKilian, Lutz, and Helmut Lütkepohl. 2017. Structural Vector Autoregressive Analysis. Cambridge: Cambridge University Press. https://doi.org/10.1017/9781108164818\nRubio-Ramírez, Juan F., Daniel F. Waggoner, and Tao Zha. 2010. \"Structural Vector Autoregressions: Theory of Identification and Algorithms for Inference.\" Review of Economic Studies 77 (2): 665–696. https://doi.org/10.1111/j.1467-937X.2009.00578.x\nUhlig, Harald. 2005. \"What Are the Effects of Monetary Policy on Output? Results from an Agnostic Identification Procedure.\" Journal of Monetary Economics 52 (2): 381–419. https://doi.org/10.1016/j.jmoneco.2004.05.007","category":"section"},{"location":"manual/#Bayesian-Methods","page":"VAR","title":"Bayesian Methods","text":"Bańbura, Marta, Domenico Giannone, and Lucrezia Reichlin. 2010. \"Large Bayesian Vector Auto Regressions.\" Journal of Applied Econometrics 25 (1): 71–92. https://doi.org/10.1002/jae.1137\nCarriero, Andrea, Todd E. Clark, and Massimiliano Marcellino. 2015. \"Bayesian VARs: Specification Choices and Forecast Accuracy.\" Journal of Applied Econometrics 30 (1): 46–73. https://doi.org/10.1002/jae.2272\nDoan, Thomas, Robert Litterman, and Christopher Sims. 1984. \"Forecasting and Conditional Projection Using Realistic Prior Distributions.\" Econometric Reviews 3 (1): 1–100. https://doi.org/10.1080/07474938408800053\nGiannone, Domenico, Michele Lenza, and Giorgio E. Primiceri. 2015. \"Prior Selection for Vector Autoregressions.\" Review of Economics and Statistics 97 (2): 436–451. https://doi.org/10.1162/RESTa00483\nKadiyala, K. Rao, and Sune Karlsson. 1997. \"Numerical Methods for Estimation and Inference in Bayesian VAR-Models.\" Journal of Applied Econometrics 12 (2): 99–132. https://doi.org/10.1002/(SICI)1099-1255(199703)12:2<99::AID-JAE429>3.0.CO;2-A\nLitterman, Robert B. 1986. \"Forecasting with Bayesian Vector Autoregressions—Five Years of Experience.\" Journal of Business & Economic Statistics 4 (1): 25–38. https://doi.org/10.1080/07350015.1986.10509491","category":"section"},{"location":"manual/#Inference","page":"VAR","title":"Inference","text":"Driscoll, John C., and Aart C. Kraay. 1998. \"Consistent Covariance Matrix Estimation with Spatially Dependent Panel Data.\" Review of Economics and Statistics 80 (4): 549–560. https://doi.org/10.1162/003465398557825\nAndrews, Donald W. K. 1991. \"Heteroskedasticity and Autocorrelation Consistent Covariance Matrix Estimation.\" Econometrica 59 (3): 817–858. https://doi.org/10.2307/2938229\nGelman, Andrew, John B. Carlin, Hal S. Stern, David B. Dunson, Aki Vehtari, and Donald B. Rubin. 2013. Bayesian Data Analysis. 3rd ed. Boca Raton, FL: CRC Press. ISBN 978-1-4398-4095-5.\nHoffman, Matthew D., and Andrew Gelman. 2014. \"The No-U-Turn Sampler: Adaptively Setting Path Lengths in Hamiltonian Monte Carlo.\" Journal of Machine Learning Research 15 (1): 1593–1623.\nKilian, Lutz. 1998. \"Small-Sample Confidence Intervals for Impulse Response Functions.\" Review of Economics and Statistics 80 (2): 218–230. https://doi.org/10.1162/003465398557465\nNewey, Whitney K., and Kenneth D. West. 1987. \"A Simple, Positive Semi-Definite, Heteroskedasticity and Autocorrelation Consistent Covariance Matrix.\" Econometrica 55 (3): 703–708. https://doi.org/10.2307/1913610\nNewey, Whitney K., and Kenneth D. West. 1994. \"Automatic Lag Selection in Covariance Matrix Estimation.\" Review of Economic Studies 61 (4): 631–653. https://doi.org/10.2307/2297912","category":"section"},{"location":"api/#API-Reference","page":"Overview","title":"API Reference","text":"This section provides the complete API documentation for MacroEconometricModels.jl.\n\nThe API documentation is organized into the following pages:\n\nTypes: Core type definitions for models, results, and estimators\nFunctions: Function documentation organized by module","category":"section"},{"location":"api/#Quick-Reference-Tables","page":"Overview","title":"Quick Reference Tables","text":"","category":"section"},{"location":"api/#ARIMA-Estimation-Functions","page":"Overview","title":"ARIMA Estimation Functions","text":"Function Description\nestimate_ar(y, p; method=:ols) AR(p) via OLS or MLE\nestimate_ma(y, q; method=:css_mle) MA(q) via CSS, MLE, or CSS-MLE\nestimate_arma(y, p, q; method=:css_mle) ARMA(p,q) via CSS, MLE, or CSS-MLE\nestimate_arima(y, p, d, q; method=:css_mle) ARIMA(p,d,q) via differencing + ARMA\nforecast(model, h; conf_level=0.95) Multi-step forecasting with confidence intervals\nselect_arima_order(y, max_p, max_q) Grid search for optimal ARMA order\nauto_arima(y) Automatic ARIMA order selection\nic_table(y, max_p, max_q) Information criteria comparison table","category":"section"},{"location":"api/#Multivariate-Estimation-Functions","page":"Overview","title":"Multivariate Estimation Functions","text":"Function Description\nestimate_var(Y, p) Estimate VAR(p) via OLS\nestimate_bvar(Y, p; ...) Estimate Bayesian VAR with MCMC\nestimate_lp(Y, shock_var, H; ...) Standard Local Projection\nestimate_lp_iv(Y, shock_var, Z, H; ...) LP with instrumental variables\nestimate_smooth_lp(Y, shock_var, H; ...) Smooth LP with B-splines\nestimate_state_lp(Y, shock_var, state_var, H; ...) State-dependent LP\nestimate_propensity_lp(Y, treatment, covariates, H; ...) LP with propensity scores\ndoubly_robust_lp(Y, treatment, covariates, H; ...) Doubly robust LP estimator\nestimate_factors(X, r; ...) Static factor model via PCA\nestimate_dynamic_factors(X, r, p; ...) Dynamic factor model\nestimate_gdfm(X, q; ...) Generalized dynamic factor model\nestimate_gmm(moment_fn, theta0, data; ...) GMM estimation\nstructural_lp(Y, H; method=:cholesky, ...) Structural LP with multi-shock IRFs","category":"section"},{"location":"api/#Structural-Analysis-Functions","page":"Overview","title":"Structural Analysis Functions","text":"Function Description\nirf(model, H; ...) Compute impulse response functions\nfevd(model, H; ...) Forecast error variance decomposition\nidentify_cholesky(model) Cholesky identification\nidentify_sign(model; ...) Sign restriction identification\nidentify_long_run(model) Blanchard-Quah identification\nidentify_narrative(model; ...) Narrative sign restrictions\nidentify_arias(model, restrictions, H; ...) Arias et al. (2018) sign + zero restrictions\nidentify_fastica(model; ...) FastICA SVAR identification\nidentify_jade(model; ...) JADE SVAR identification\nidentify_sobi(model; ...) SOBI SVAR identification\nidentify_dcov(model; ...) Distance covariance SVAR identification\nidentify_hsic(model; ...) HSIC SVAR identification\nidentify_student_t(model; ...) Student-t ML SVAR identification\nidentify_mixture_normal(model; ...) Mixture-normal ML SVAR identification\nidentify_pml(model; ...) Pseudo-ML SVAR identification\nidentify_skew_normal(model; ...) Skew-normal ML SVAR identification\nidentify_nongaussian_ml(model; ...) Unified non-Gaussian ML dispatcher\nidentify_markov_switching(model; ...) Markov-switching SVAR identification\nidentify_garch(model; ...) GARCH SVAR identification\nidentify_smooth_transition(model, s; ...) Smooth-transition SVAR identification\nidentify_external_volatility(model, regime) External volatility SVAR identification\nlp_fevd(slp, H; method=:r2, ...) LP-FEVD (Gorodnichenko & Lee 2019)\ncumulative_irf(lp_irfs) Cumulative IRF from LP impulse response\nhistorical_decomposition(slp) Historical decomposition from structural LP","category":"section"},{"location":"api/#LP-Forecasting-Functions","page":"Overview","title":"LP Forecasting Functions","text":"Function Description\nforecast(lp, shock_path; ...) Direct multi-step LP forecast\nforecast(slp, shock_idx, shock_path; ...) Structural LP conditional forecast","category":"section"},{"location":"api/#Unit-Root-Test-Functions","page":"Overview","title":"Unit Root Test Functions","text":"Function Description\nadf_test(y; ...) Augmented Dickey-Fuller unit root test\nkpss_test(y; ...) KPSS stationarity test\npp_test(y; ...) Phillips-Perron unit root test\nza_test(y; ...) Zivot-Andrews structural break test\nngperron_test(y; ...) Ng-Perron unit root tests (MZα, MZt, MSB, MPT)\njohansen_test(Y, p; ...) Johansen cointegration test\nis_stationary(model) Check VAR model stationarity\nunit_root_summary(y; ...) Run multiple tests with summary\ntest_all_variables(Y; ...) Apply test to all columns","category":"section"},{"location":"api/#LP-IRF-Extraction","page":"Overview","title":"LP IRF Extraction","text":"Function Description\nlp_irf(model; ...) Extract IRF from LPModel\nlp_iv_irf(model; ...) Extract IRF from LPIVModel\nsmooth_lp_irf(model; ...) Extract smoothed IRF\nstate_irf(model; ...) Extract state-dependent IRFs\npropensity_irf(model; ...) Extract ATE impulse response","category":"section"},{"location":"api/#Factor-Model-Functions","page":"Overview","title":"Factor Model Functions","text":"Function Description\nestimate_factors(X, r; ...) Estimate r-factor model\nestimate_dynamic_factors(X, r, p; ...) Dynamic factor model\nestimate_gdfm(X, q; ...) Generalized dynamic factor model\nforecast(fm, h; p=1, ci_method=:none) Static FM forecast (fits VAR(p) on factors)\nforecast(dfm, h; ci_method=:none) DFM forecast (:none/:theoretical/:bootstrap/:simulation)\nforecast(gdfm, h; ci_method=:none) GDFM forecast (:none/:theoretical/:bootstrap)\nic_criteria(X, r_max) Bai-Ng information criteria\nic_criteria_dynamic(X, max_r, max_p) DFM factor/lag selection\nic_criteria_gdfm(X, max_q) GDFM dynamic factor selection\nscree_plot_data(model) Data for scree plot\nis_stationary(dfm) Check DFM factor VAR stationarity\ncommon_variance_share(gdfm) GDFM common variance share per variable","category":"section"},{"location":"api/#Diagnostic-Functions","page":"Overview","title":"Diagnostic Functions","text":"Function Description\noptimize_hyperparameters(Y, p; ...) Optimize Minnesota prior\nposterior_mean_model(chain, p, n; ...) VARModel from posterior mean\nposterior_median_model(chain, p, n; ...) VARModel from posterior median\nweak_instrument_test(model; ...) Test for weak instruments\nsargan_test(model, h) Overidentification test\ntest_regime_difference(model; ...) Test regime differences\npropensity_diagnostics(model) Propensity score diagnostics\nj_test(model) Hansen J-test for GMM\ngmm_summary(model) Summary statistics for GMM","category":"section"},{"location":"api/#Normality-Test-Functions","page":"Overview","title":"Normality Test Functions","text":"Function Description\njarque_bera_test(model; method=:multivariate) Multivariate Jarque-Bera test\nmardia_test(model; type=:both) Mardia skewness/kurtosis tests\ndoornik_hansen_test(model) Doornik-Hansen omnibus test\nhenze_zirkler_test(model) Henze-Zirkler characteristic function test\nnormality_test_suite(model) Run all normality tests","category":"section"},{"location":"api/#Identifiability-Test-Functions","page":"Overview","title":"Identifiability Test Functions","text":"Function Description\ntest_shock_gaussianity(result) Test non-Gaussianity of recovered shocks\ntest_gaussian_vs_nongaussian(model; ...) LR test: Gaussian vs non-Gaussian\ntest_shock_independence(result; ...) Test independence of recovered shocks\ntest_identification_strength(model; ...) Bootstrap identification strength test\ntest_overidentification(model, result; ...) Overidentification test","category":"section"},{"location":"api/#Volatility-Model-Functions","page":"Overview","title":"Volatility Model Functions","text":"Function Description\nestimate_arch(y, q) ARCH(q) via MLE\nestimate_garch(y, p, q) GARCH(p,q) via MLE\nestimate_egarch(y, p, q) EGARCH(p,q) via MLE\nestimate_gjr_garch(y, p, q) GJR-GARCH(p,q) via MLE\nestimate_sv(y; variant, ...) Stochastic Volatility via MCMC\nforecast(vol_model, h) Volatility forecast with simulation CIs\narch_lm_test(y_or_model, q) ARCH-LM test for conditional heteroskedasticity\nljung_box_squared(z_or_model, K) Ljung-Box test on squared residuals\nnews_impact_curve(model) News impact curve (GARCH family)\npersistence(model) Persistence measure\nhalflife(model) Volatility half-life\nunconditional_variance(model) Unconditional variance\narch_order(model) ARCH order q\ngarch_order(model) GARCH order p","category":"section"},{"location":"api/#Display-and-Output-Functions","page":"Overview","title":"Display and Output Functions","text":"Function Description\nset_display_backend(sym) Switch output format (:text/:latex/:html)\nget_display_backend() Current display backend\nreport(result) Print comprehensive summary\ntable(result, ...) Extract results as matrix\nprint_table([io], result, ...) Print formatted table\nrefs(model; format=...) Bibliographic references\nrefs(io, :method; format=...) References by method name","category":"section"},{"location":"api/#Covariance-Functions","page":"Overview","title":"Covariance Functions","text":"Function Description\nnewey_west(X, residuals; ...) Newey-West HAC estimator\nwhite_vcov(X, residuals; ...) White heteroskedasticity-robust\ndriscoll_kraay(X, residuals; ...) Driscoll-Kraay panel-robust\nlong_run_variance(x; ...) Long-run variance estimate\nlong_run_covariance(X; ...) Long-run covariance matrix\noptimal_bandwidth_nw(residuals) Automatic bandwidth selection","category":"section"},{"location":"api/#Utility-Functions","page":"Overview","title":"Utility Functions","text":"Function Description\nconstruct_var_matrices(Y, p) Build VAR design matrices\ncompanion_matrix(B, n, p) VAR companion form\nrobust_inv(A) Robust matrix inverse\nsafe_cholesky(A; ...) Stable Cholesky decomposition","category":"section"},{"location":"factormodels/#Factor-Models","page":"Factor Models","title":"Factor Models","text":"This chapter covers static factor models for dimensionality reduction in large macroeconomic panels, including estimation via principal components and information criteria for selecting the number of factors.","category":"section"},{"location":"factormodels/#Introduction","page":"Factor Models","title":"Introduction","text":"Factor models are fundamental tools in macroeconometrics for extracting common sources of variation from large panels of economic indicators. They enable:\n\nDimensionality Reduction: Summarize N variables with r ll N factors\nForecasting: Use factors as predictors in regressions (diffusion indices)\nStructural Analysis: Identify common shocks driving multiple series\nFAVAR: Combine factors with VARs for high-dimensional structural analysis\n\nReference: Stock & Watson (2002a, 2002b), Bai & Ng (2002)","category":"section"},{"location":"factormodels/#Quick-Start","page":"Factor Models","title":"Quick Start","text":"fm = estimate_factors(X, r; standardize=true)                       # Static factor model via PCA\nic = ic_criteria(X, 10)                                             # Bai-Ng IC for factor count\ndfm = estimate_dynamic_factors(X, r, p; method=:twostep)            # Dynamic factor model\ngdfm = estimate_gdfm(X, q; kernel=:bartlett)                        # Generalized DFM (spectral)\nfc = forecast(fm, h; ci_method=:theoretical)                        # Static FM forecast with analytical CIs\nfc = forecast(dfm, h; ci_method=:bootstrap, n_boot=1000)            # DFM forecast with bootstrap CIs\nfc = forecast(gdfm, h; ci_method=:theoretical)                      # GDFM forecast with analytical CIs\n\n","category":"section"},{"location":"factormodels/#The-Static-Factor-Model","page":"Factor Models","title":"The Static Factor Model","text":"","category":"section"},{"location":"factormodels/#Model-Specification","page":"Factor Models","title":"Model Specification","text":"The static factor model decomposes an N-dimensional vector of observables x_t into common and idiosyncratic components:\n\nx_it = lambda_i F_t + e_it quad i = 1 ldots N quad t = 1 ldots T\n\nIn matrix form:\n\nX = F Lambda + E\n\nwhere:\n\nX is the T times N data matrix\nF is the T times r matrix of latent factors\nLambda is the N times r matrix of factor loadings\nE is the T times N matrix of idiosyncratic errors\nr is the number of factors (with r ll min(T N))","category":"section"},{"location":"factormodels/#Assumptions","page":"Factor Models","title":"Assumptions","text":"Factors and Loadings:\n\nEF_t = 0, textVar(F_t) = I_r (normalization)\nfrac1T sum_t F_t F_t xrightarrowp Sigma_F positive definite\nfrac1N Lambda Lambda xrightarrowp Sigma_Lambda positive definite\n\nIdiosyncratic Errors:\n\nEe_it = 0\nWeak cross-sectional and temporal dependence allowed\nWeak correlation with factors: frac1NT sum_it EF_t e_it to 0\n\nReference: Bai & Ng (2002), Bai (2003)\n\n","category":"section"},{"location":"factormodels/#Estimation-via-Principal-Components","page":"Factor Models","title":"Estimation via Principal Components","text":"","category":"section"},{"location":"factormodels/#Principal-Components-Analysis-(PCA)","page":"Factor Models","title":"Principal Components Analysis (PCA)","text":"The factors and loadings are estimated by minimizing the sum of squared idiosyncratic errors:\n\nmin_F Lambda sum_i=1^N sum_t=1^T (x_it - lambda_i F_t)^2\n\nsubject to the normalization FFT = I_r.","category":"section"},{"location":"factormodels/#Solution","page":"Factor Models","title":"Solution","text":"The solution involves the eigenvalue decomposition of XX (or XX):\n\nCase 1: T  N (short panel)\n\nCompute XX (T times T matrix)\nhatF = sqrtT times (first r eigenvectors of XX)\nhatLambda = X hatF  T\n\nCase 2: N leq T (tall panel)\n\nCompute XX (N times N matrix)\nhatLambda = sqrtN times (first r eigenvectors of XX)\nhatF = X hatLambda  N","category":"section"},{"location":"factormodels/#Data-Preprocessing","page":"Factor Models","title":"Data Preprocessing","text":"Before estimation, data is typically:\n\nDemeaned: Center each series to have zero mean\nStandardized: Scale each series to have unit variance\n\nThis prevents high-variance series from dominating the factor extraction.","category":"section"},{"location":"factormodels/#Identification","page":"Factor Models","title":"Identification","text":"The factors and loadings are identified only up to an r times r invertible rotation. If (F Lambda) is a solution, so is (FH Lambda H^-1) for any invertible H.\n\nThe normalization FFT = I_r and LambdaLambda diagonal pins down rotation up to sign.\n\nReference: Stock & Watson (2002a), Bai & Ng (2002)","category":"section"},{"location":"factormodels/#Julia-Implementation","page":"Factor Models","title":"Julia Implementation","text":"using MacroEconometricModels\n\n# X is T×N data matrix\n# Estimate r-factor model\n\nmodel = estimate_factors(X, r;\n    standardize = true,    # Standardize data\n    method = :pca          # Principal components\n)\n\n# Access results\nF = model.factors          # T×r estimated factors\nΛ = model.loadings         # N×r estimated loadings","category":"section"},{"location":"factormodels/#FactorModel-Return-Values","page":"Factor Models","title":"FactorModel Return Values","text":"Field Type Description\nX Matrix{T} Original T times N data matrix\nfactors Matrix{T} T times r estimated factor matrix\nloadings Matrix{T} N times r estimated loading matrix\neigenvalues Vector{T} Eigenvalues from PCA (in descending order)\nexplained_variance Vector{T} Fraction of variance explained by each factor\ncumulative_variance Vector{T} Cumulative fraction of variance explained\nr Int Number of factors\nstandardized Bool Whether data was standardized before estimation\n\nnote: Technical Note\nFactor models are identified only up to an r times r rotation: if (hatF hatLambda) is a solution, then (hatFH hatLambdaH^-1) is equally valid for any invertible H. The normalization FFT = I_r pins down orientation but not sign. Consequently, individual factor loadings should not be interpreted as structural parameters. To compare estimated factors with \"true\" factors (e.g., in simulations), compute absolute correlations rather than raw correlations.\n\n","category":"section"},{"location":"factormodels/#Determining-the-Number-of-Factors","page":"Factor Models","title":"Determining the Number of Factors","text":"","category":"section"},{"location":"factormodels/#The-Selection-Problem","page":"Factor Models","title":"The Selection Problem","text":"Choosing r is crucial:\n\nToo few factors: Omitted common variation, biased estimates\nToo many factors: Overfitting, including noise as signal","category":"section"},{"location":"factormodels/#Bai-and-Ng-(2002)-Information-Criteria","page":"Factor Models","title":"Bai & Ng (2002) Information Criteria","text":"Bai & Ng propose three information criteria:\n\nIC1:\n\nIC_1(r) = log hatsigma^2(r) + r cdot fracN + TNT logleft( fracNTN+T right)\n\nIC2:\n\nIC_2(r) = log hatsigma^2(r) + r cdot fracN + TNT log(C_NT^2)\n\nIC3:\n\nIC_3(r) = log hatsigma^2(r) + r cdot fraclog(C_NT^2)C_NT^2\n\nwhere:\n\nhatsigma^2(r) = frac1NT sum_it hate_it^2 is the average squared residual\nC_NT^2 = min(N T)\n\nSelection Rule: Choose hatr that minimizes IC_k(r) over r in 1 ldots r_max.\n\nProperties:\n\nIC2 and IC3 perform best in simulations\nAll three are consistent: hatr xrightarrowp r_0 as N T to infty\n\nReference: Bai & Ng (2002)","category":"section"},{"location":"factormodels/#Julia-Implementation-2","page":"Factor Models","title":"Julia Implementation","text":"using MacroEconometricModels\n\n# Compute IC for r = 1, ..., r_max\nr_max = 10\nic = ic_criteria(X, r_max)\n\n# Optimal number by each criterion\nprintln(\"IC1 selects: \", ic.r_IC1, \" factors\")\nprintln(\"IC2 selects: \", ic.r_IC2, \" factors\")\nprintln(\"IC3 selects: \", ic.r_IC3, \" factors\")\n\n# IC values for all r\nfor r in 1:r_max\n    println(\"r=$r: IC1=$(ic.IC1[r]), IC2=$(ic.IC2[r]), IC3=$(ic.IC3[r])\")\nend\n\n","category":"section"},{"location":"factormodels/#Scree-Plot-Analysis","page":"Factor Models","title":"Scree Plot Analysis","text":"","category":"section"},{"location":"factormodels/#Visual-Factor-Selection","page":"Factor Models","title":"Visual Factor Selection","text":"The scree plot displays eigenvalues (or variance explained) against factor number. The \"elbow\" in the plot suggests the number of significant factors.","category":"section"},{"location":"factormodels/#Variance-Explained","page":"Factor Models","title":"Variance Explained","text":"For each factor j:\n\nIndividual Variance:\n\ntextVarExp_j = fracmu_jsum_k=1^N mu_k\n\nCumulative Variance:\n\ntextCumVarExp_r = sum_j=1^r textVarExp_j\n\nwhere mu_j is the j-th largest eigenvalue of XXT (or XXN).","category":"section"},{"location":"factormodels/#Julia-Implementation-3","page":"Factor Models","title":"Julia Implementation","text":"using MacroEconometricModels\n\nmodel = estimate_factors(X, r)\n\n# Get scree plot data\nscree = scree_plot_data(model)\n\n# Variance explained\nfor j in 1:min(10, length(scree.factors))\n    println(\"Factor $j: $(round(scree.explained_variance[j]*100, digits=2))% \",\n            \"(cumulative: $(round(scree.cumulative_variance[j]*100, digits=2))%)\")\nend\n\n","category":"section"},{"location":"factormodels/#Model-Diagnostics","page":"Factor Models","title":"Model Diagnostics","text":"","category":"section"},{"location":"factormodels/#R-squared-for-Each-Variable","page":"Factor Models","title":"R-squared for Each Variable","text":"The R^2 measures how much of variable i's variation is explained by the common factors:\n\nR^2_i = 1 - fracsum_t hate_it^2sum_t (x_it - barx_i)^2\n\nVariables with low R^2 are mainly driven by idiosyncratic shocks.","category":"section"},{"location":"factormodels/#Julia-Implementation-4","page":"Factor Models","title":"Julia Implementation","text":"using MacroEconometricModels\n\nmodel = estimate_factors(X, r)\n\n# R² for each variable\nr2_values = r2(model)\n\n# Summary statistics\nprintln(\"Mean R²: \", round(mean(r2_values), digits=3))\nprintln(\"Median R²: \", round(median(r2_values), digits=3))\nprintln(\"Min R²: \", round(minimum(r2_values), digits=3))\nprintln(\"Max R²: \", round(maximum(r2_values), digits=3))\n\n# Variables well-explained by factors\nwell_explained = findall(r2_values .> 0.7)","category":"section"},{"location":"factormodels/#Fitted-Values-and-Residuals","page":"Factor Models","title":"Fitted Values and Residuals","text":"# Fitted values: X̂ = FΛ'\nX_fitted = predict(model)\n\n# Residuals: E = X - X̂\nresid = residuals(model)\n\n# Model statistics\nprintln(\"Number of observations: \", nobs(model))\nprintln(\"Degrees of freedom: \", dof(model))\n\n","category":"section"},{"location":"factormodels/#Applications","page":"Factor Models","title":"Applications","text":"","category":"section"},{"location":"factormodels/#Diffusion-Index-Forecasting","page":"Factor Models","title":"Diffusion Index Forecasting","text":"Use factors as predictors for forecasting a target variable y_t+h:\n\ny_t+h = alpha + beta hatF_t + gamma y_tt-p + varepsilon_t+h\n\nFactors summarize information from a large panel, improving forecast accuracy.\n\nReference: Stock & Watson (2002b)","category":"section"},{"location":"factormodels/#Factor-Augmented-VAR-(FAVAR)","page":"Factor Models","title":"Factor-Augmented VAR (FAVAR)","text":"Combine factors with key observable variables in a VAR:\n\nbeginbmatrix y_t  F_t endbmatrix = A_1 beginbmatrix y_t-1  F_t-1 endbmatrix + cdots + A_p beginbmatrix y_t-p  F_t-p endbmatrix + u_t\n\nThis allows structural analysis with high-dimensional information sets.\n\nReference: Bernanke, Boivin & Eliasz (2005)","category":"section"},{"location":"factormodels/#Example:-FAVAR-Setup","page":"Factor Models","title":"Example: FAVAR Setup","text":"using MacroEconometricModels\n\n# Estimate factors from large panel X\nfm = estimate_factors(X, r)\nF = fm.factors\n\n# Combine with key observables (e.g., FFR, GDP, inflation)\nY_key = Matrix(data[:, [:FFR, :GDP, :CPI]])\nY_favar = hcat(Y_key, F)\n\n# Estimate FAVAR\nfavar_model = estimate_var(Y_favar, p)\n\n# Structural analysis\nirf_favar = irf(favar_model, H; method=:cholesky)\n\n","category":"section"},{"location":"factormodels/#Forecasting-with-Static-Factor-Models","page":"Factor Models","title":"Forecasting with Static Factor Models","text":"","category":"section"},{"location":"factormodels/#Forecast-Method","page":"Factor Models","title":"Forecast Method","text":"The static factor model does not directly specify factor dynamics, but forecasting is possible by fitting a VAR(p) on the extracted factors:\n\nhatF_T+hT = hatA_1 hatF_T+h-1T + cdots + hatA_p hatF_T+h-pT\n\nObservable forecasts are obtained via the loading matrix:\n\nhatX_T+hT = hatLambda hatF_T+hT","category":"section"},{"location":"factormodels/#Confidence-Intervals","page":"Factor Models","title":"Confidence Intervals","text":"Theoretical CIs use the VMA(infty) representation of the factor VAR to compute the h-step forecast error covariance analytically:\n\ntextMSE_h = sum_j=0^h-1 Psi_j Sigma_eta Psi_j\n\nwhere Psi_j = J C^j are the VMA coefficient matrices from the companion form.\n\nBootstrap CIs resample factor VAR residuals to construct simulated forecast paths and compute percentile intervals.","category":"section"},{"location":"factormodels/#Julia-Implementation-5","page":"Factor Models","title":"Julia Implementation","text":"using MacroEconometricModels\n\n# Estimate static factor model\nfm = estimate_factors(X, 3)\n\n# Point forecast (fits VAR(1) on factors internally)\nfc = forecast(fm, 12)\nfc.factors       # 12×3 factor forecasts\nfc.observables   # 12×N observable forecasts\n\n# Forecast with theoretical (analytical) confidence intervals\nfc = forecast(fm, 12; ci_method=:theoretical, conf_level=0.95)\nfc.factors_lower   # 12×3 lower CI for factors\nfc.factors_upper   # 12×3 upper CI for factors\nfc.observables_se  # 12×N standard errors for observables\n\n# Forecast with bootstrap CIs\nfc = forecast(fm, 12; ci_method=:bootstrap, n_boot=1000, conf_level=0.90)\n\n# Use higher-order VAR for factor dynamics\nfc = forecast(fm, 12; p=2, ci_method=:theoretical)\n\nThe theoretical SEs increase with the forecast horizon, reflecting growing uncertainty. For stationary factor dynamics, the SEs converge to the unconditional forecast error standard deviation. Bootstrap CIs are preferred when the Gaussian assumption may not hold.\n\nReference: Stock & Watson (2002b)\n\n","category":"section"},{"location":"factormodels/#Asymptotic-Theory","page":"Factor Models","title":"Asymptotic Theory","text":"","category":"section"},{"location":"factormodels/#Consistency-of-Factor-Estimates","page":"Factor Models","title":"Consistency of Factor Estimates","text":"Under the assumptions of Bai & Ng (2002), as T N to infty:\n\nfrac1T sum_t=1^T hatF_t - H F_t^2 = O_pleft( frac1min(N T) right)\n\nwhere H is an r times r rotation matrix.\n\nThe factors are consistently estimated up to rotation at rate min(sqrtN sqrtT).","category":"section"},{"location":"factormodels/#Distribution-Theory","page":"Factor Models","title":"Distribution Theory","text":"For large N T, the factor estimates are asymptotically normal:\n\nsqrtT (hatF_t - H F_t) xrightarrowd N(0 V)\n\nwhere V depends on the cross-sectional and temporal dependence structure.\n\nReference: Bai (2003), Bai & Ng (2006)\n\n","category":"section"},{"location":"factormodels/#Comparison-with-Other-Methods","page":"Factor Models","title":"Comparison with Other Methods","text":"","category":"section"},{"location":"factormodels/#Static-vs.-Dynamic-Factor-Models","page":"Factor Models","title":"Static vs. Dynamic Factor Models","text":"Aspect Static FM Dynamic FM\nModel X_t = Lambda F_t + e_t X_t = Lambda(L) f_t + e_t\nFactors Contemporaneous May include lags\nEstimation PCA Spectral methods, Kalman filter\nUse case Large N, moderate T Time series dynamics important\n\nReference: Forni, Hallin, Lippi & Reichlin (2000)","category":"section"},{"location":"factormodels/#Maximum-Likelihood-Estimation","page":"Factor Models","title":"Maximum Likelihood Estimation","text":"ML estimation assumes Gaussian factors and errors:\n\nF_t sim N(0 I_r) quad e_t sim N(0 Psi)\n\nEstimated via EM algorithm. More efficient than PCA if model is correctly specified, but computationally intensive.\n\n","category":"section"},{"location":"factormodels/#Dynamic-Factor-Models","page":"Factor Models","title":"Dynamic Factor Models","text":"","category":"section"},{"location":"factormodels/#Model-Specification-2","page":"Factor Models","title":"Model Specification","text":"The dynamic factor model extends the static model by allowing factors to follow a VAR process:\n\nObservation Equation:\n\nX_t = Lambda F_t + e_t\n\nState Equation (Factor Dynamics):\n\nF_t = A_1 F_t-1 + A_2 F_t-2 + cdots + A_p F_t-p + eta_t\n\nwhere:\n\nF_t is the r times 1 vector of latent factors\nLambda is the N times r loading matrix\nA_1 ldots A_p are r times r autoregressive coefficient matrices\neta_t sim N(0 Sigma_eta) are factor innovations\ne_t sim N(0 Sigma_e) are idiosyncratic errors (typically diagonal)","category":"section"},{"location":"factormodels/#Estimation-Methods","page":"Factor Models","title":"Estimation Methods","text":"Two-Step Estimation:\n\nExtract factors using PCA (as in static model)\nEstimate VAR(p) on extracted factors\n\nEM Algorithm:\n\nIterates between E-step (Kalman smoother) and M-step (parameter updates)\nMore efficient but computationally intensive","category":"section"},{"location":"factormodels/#Julia-Implementation-6","page":"Factor Models","title":"Julia Implementation","text":"using MacroEconometricModels\n\n# Estimate dynamic factor model with r factors and p lags\nmodel = estimate_dynamic_factors(X, r, p;\n    method = :twostep,      # or :em\n    standardize = true,\n    diagonal_idio = true    # Diagonal idiosyncratic covariance\n)\n\n# Access results\nF = model.factors           # T×r estimated factors\nΛ = model.loadings          # N×r loadings\nA = model.A                 # Vector of r×r AR coefficient matrices\nΣ_η = model.Sigma_eta       # r×r factor innovation covariance\nΣ_e = model.Sigma_e         # N×N idiosyncratic covariance","category":"section"},{"location":"factormodels/#DynamicFactorModel-Return-Values","page":"Factor Models","title":"DynamicFactorModel Return Values","text":"Field Type Description\nX Matrix{T} Original T times N data matrix\nfactors Matrix{T} T times r estimated factors\nloadings Matrix{T} N times r loading matrix\nA Vector{Matrix{T}} r times r autoregressive coefficient matrices\nfactor_residuals Matrix{T} Factor VAR residuals\nSigma_eta Matrix{T} r times r factor innovation covariance\nSigma_e Matrix{T} N times N idiosyncratic covariance\neigenvalues Vector{T} Eigenvalues from initial PCA\nexplained_variance Vector{T} Variance explained by each factor\ncumulative_variance Vector{T} Cumulative variance explained\nr Int Number of factors\np Int Number of factor VAR lags\nmethod Symbol Estimation method (:twostep or :em)\nstandardized Bool Whether data was standardized\nconverged Bool Convergence status (relevant for :em)\niterations Int Number of iterations (relevant for :em)\nloglik T Log-likelihood value","category":"section"},{"location":"factormodels/#Model-Selection-for-DFM","page":"Factor Models","title":"Model Selection for DFM","text":"Select the number of factors r and lag order p using information criteria:\n\n# Grid search over (r, p) combinations\nic = ic_criteria_dynamic(X, max_r, max_p;\n    method = :twostep,\n    standardize = true\n)\n\nprintln(\"AIC selects: r=$(ic.r_AIC), p=$(ic.p_AIC)\")\nprintln(\"BIC selects: r=$(ic.r_BIC), p=$(ic.p_BIC)\")\n\n# View full IC matrices\nic.AIC  # r×p matrix of AIC values\nic.BIC  # r×p matrix of BIC values","category":"section"},{"location":"factormodels/#Forecasting-with-DFM","page":"Factor Models","title":"Forecasting with DFM","text":"The DFM forecast extrapolates the factor VAR dynamics forward and projects to observables via the loading matrix. Four CI methods are available:\n\nci_method Description Best for\n:none Point forecast only Quick exploration\n:theoretical Analytical VMA CIs (Gaussian) Large samples, fast\n:bootstrap Residual resampling Non-Gaussian innovations\n:simulation Monte Carlo draws from estimated model Full uncertainty propagation\n\n# Point forecasts h steps ahead\nfc = forecast(model, h)\nfc.factors       # h×r factor forecasts\nfc.observables   # h×N observable forecasts\n\n# Theoretical (analytical) confidence intervals\nfc = forecast(model, h; ci_method=:theoretical, conf_level=0.95)\nfc.factors_se           # h×r standard errors\nfc.observables_lower    # h×N lower CI bounds\nfc.observables_upper    # h×N upper CI bounds\n\n# Bootstrap confidence intervals\nfc = forecast(model, h; ci_method=:bootstrap, n_boot=1000, conf_level=0.90)\n\n# Simulation-based CIs (original method, also accessible via legacy ci=true)\nfc = forecast(model, h; ci_method=:simulation, n_boot=2000)\nfc = forecast(model, h; ci=true, ci_level=0.90)  # Legacy interface\n\nAll forecast methods return a FactorForecast struct. When ci_method=:none, the CI and SE fields are zero matrices.","category":"section"},{"location":"factormodels/#FactorForecast-Return-Values","page":"Factor Models","title":"FactorForecast Return Values","text":"Field Type Description\nfactors Matrix{T} h times r factor point forecasts\nobservables Matrix{T} h times N observable point forecasts\nfactors_lower Matrix{T} h times r lower CI bounds for factors\nfactors_upper Matrix{T} h times r upper CI bounds for factors\nobservables_lower Matrix{T} h times N lower CI bounds for observables\nobservables_upper Matrix{T} h times N upper CI bounds for observables\nfactors_se Matrix{T} h times r factor forecast standard errors\nobservables_se Matrix{T} h times N observable forecast standard errors\nhorizon Int Forecast horizon h\nconf_level T Confidence level (e.g., 0.95)\nci_method Symbol CI method used (:none, :theoretical, :bootstrap, :simulation)\n\nnote: Technical Note\nThe theoretical CIs compute the h-step forecast MSE via the VMA(infty) representation: textMSE_h = sum_j=0^h-1 Psi_j Sigma_eta Psi_j where Psi_j = J C^j with C the companion matrix and J the selector for the first r rows. Observable SEs combine factor uncertainty with idiosyncratic variance: textVar(hatX_T+h) = Lambda cdot textMSE_h cdot Lambda + Sigma_e.","category":"section"},{"location":"factormodels/#Stationarity-Check","page":"Factor Models","title":"Stationarity Check","text":"# Check if factor dynamics are stationary\nis_stationary(model)  # true if max|eigenvalue| < 1\n\n# Get companion matrix for factor VAR\nC = companion_matrix_factors(model)\neigvals(C)  # Eigenvalues determine stability\n\nReference: Stock & Watson (2002a), Doz, Giannone & Reichlin (2011)\n\n","category":"section"},{"location":"factormodels/#Generalized-Dynamic-Factor-Model-(GDFM)","page":"Factor Models","title":"Generalized Dynamic Factor Model (GDFM)","text":"","category":"section"},{"location":"factormodels/#Theoretical-Foundation","page":"Factor Models","title":"Theoretical Foundation","text":"The Generalized Dynamic Factor Model of Forni, Hallin, Lippi & Reichlin (2000, 2005) provides a fully dynamic approach to factor analysis using spectral methods. Unlike the standard DFM which uses static PCA followed by VAR, the GDFM extracts factors directly in the frequency domain.","category":"section"},{"location":"factormodels/#Model-Specification-3","page":"Factor Models","title":"Model Specification","text":"The GDFM decomposes each observable as:\n\nx_it = chi_it + xi_it\n\nwhere:\n\nchi_it is the common component driven by q common shocks\nxi_it is the idiosyncratic component\n\nThe common component has the representation:\n\nchi_it = b_i1(L) u_1t + b_i2(L) u_2t + cdots + b_iq(L) u_qt\n\nwhere b_ij(L) are square-summable filters and u_jt are orthonormal white noise shocks.","category":"section"},{"location":"factormodels/#Spectral-Representation","page":"Factor Models","title":"Spectral Representation","text":"In the frequency domain, the spectral density of X_t decomposes as:\n\nSigma_X(omega) = Sigma_chi(omega) + Sigma_xi(omega)\n\nThe key insight is that common factors produce diverging eigenvalues (growing with N) while idiosyncratic components produce bounded eigenvalues.","category":"section"},{"location":"factormodels/#Estimation-Algorithm","page":"Factor Models","title":"Estimation Algorithm","text":"Spectral Density Estimation: Estimate hatSigma_X(omega) using kernel smoothing of the periodogram\nDynamic Eigenanalysis: Compute eigenvalue decomposition at each frequency\nFactor Extraction: Select top q eigenvectors (dynamic principal components)\nCommon Component: Reconstruct chi_t via inverse Fourier transform","category":"section"},{"location":"factormodels/#Julia-Implementation-7","page":"Factor Models","title":"Julia Implementation","text":"using MacroEconometricModels\n\n# Estimate GDFM with q dynamic factors\nmodel = estimate_gdfm(X, q;\n    standardize = true,\n    bandwidth = 0,           # Auto-select: T^(1/3)\n    kernel = :bartlett,      # :bartlett, :parzen, or :tukey\n    r = 0                    # Static factors (0 = same as q)\n)\n\n# Access results\nF = model.factors                 # T×q time-domain factors\nχ = model.common_component        # T×N common component\nξ = model.idiosyncratic           # T×N idiosyncratic component\nΛ = model.loadings_spectral       # N×q×n_freq frequency-domain loadings\n\n# Variance explained by dynamic factors\nmodel.variance_explained          # q-vector of variance shares","category":"section"},{"location":"factormodels/#GeneralizedDynamicFactorModel-Return-Values","page":"Factor Models","title":"GeneralizedDynamicFactorModel Return Values","text":"Field Type Description\nX Matrix{T} Original T times N data matrix\nfactors Matrix{T} T times q time-domain factors\ncommon_component Matrix{T} T times N common component chi_t\nidiosyncratic Matrix{T} T times N idiosyncratic component xi_t\nloadings_spectral Array{Complex{T},3} N times q times n_freq frequency-domain loadings\nspectral_density_X Array{Complex{T},3} Spectral density of X_t\nspectral_density_chi Array{Complex{T},3} Spectral density of common component\neigenvalues_spectral Matrix{T} N times n_freq eigenvalues across frequencies\nfrequencies Vector{T} Frequency grid (0 to pi)\nq Int Number of dynamic factors\nr Int Number of static factors\nbandwidth Int Kernel smoothing bandwidth\nkernel Symbol Kernel type (:bartlett, :parzen, :tukey)\nstandardized Bool Whether data was standardized\nvariance_explained Vector{T} Variance share per dynamic factor","category":"section"},{"location":"factormodels/#Selecting-the-Number-of-Dynamic-Factors","page":"Factor Models","title":"Selecting the Number of Dynamic Factors","text":"The GDFM uses eigenvalue-based criteria rather than information criteria:\n\n# Compute selection criteria\nic = ic_criteria_gdfm(X, max_q;\n    standardize = true,\n    bandwidth = 0,\n    kernel = :bartlett\n)\n\n# Eigenvalue ratio criterion (Ahn & Horenstein 2013)\nprintln(\"Ratio criterion selects: q=$(ic.q_ratio)\")\n\n# Variance threshold criterion (90% of spectral variance)\nprintln(\"Variance criterion selects: q=$(ic.q_variance)\")\n\n# Diagnostic data\nic.eigenvalue_ratios      # λ_i / λ_{i+1} ratios\nic.cumulative_variance    # Cumulative variance explained\nic.avg_eigenvalues        # Average eigenvalues across frequencies","category":"section"},{"location":"factormodels/#Spectral-Diagnostics","page":"Factor Models","title":"Spectral Diagnostics","text":"# Get data for eigenvalue plots across frequencies\nplot_data = spectral_eigenvalue_plot_data(model)\nplot_data.frequencies     # Vector of frequencies (0 to π)\nplot_data.eigenvalues     # N×n_freq matrix of eigenvalues\n\n# First eigenvalue should dominate if one strong factor\n# Gap between q-th and (q+1)-th eigenvalue indicates factor count","category":"section"},{"location":"factormodels/#Common-Variance-Share","page":"Factor Models","title":"Common Variance Share","text":"# Fraction of variance explained by common component for each variable\nshares = common_variance_share(model)\n\n# Variables well-explained by common factors\nwell_explained = findall(shares .> 0.5)\n\n# Summary statistics\nprintln(\"Mean common variance share: \", round(mean(shares), digits=3))\nprintln(\"Variables with >50% common: \", length(well_explained))","category":"section"},{"location":"factormodels/#Forecasting-with-GDFM","page":"Factor Models","title":"Forecasting with GDFM","text":"The GDFM forecast uses AR(1) extrapolation of each factor series, with observable forecasts computed via the average spectral loadings. Confidence intervals are available via analytical or bootstrap methods.\n\n# Point forecast\nfc = forecast(model, h; method=:ar)\nfc.factors       # h×q factor forecasts\nfc.observables   # h×N observable forecasts\n\n# Theoretical CIs (closed-form AR(1) variance)\nfc = forecast(model, h; ci_method=:theoretical, conf_level=0.95)\nfc.factors_se           # h×q SEs (non-decreasing with horizon)\nfc.observables_lower    # h×N lower CI bounds\nfc.observables_upper    # h×N upper CI bounds\n\n# Bootstrap CIs (resample AR(1) residuals per factor)\nfc = forecast(model, h; ci_method=:bootstrap, n_boot=1000)\n\nThe theoretical CIs use the closed-form AR(1) forecast variance: textVar(hatF_T+hi) = sigma_i^2 sum_j=0^h-1 phi_i^2j where phi_i and sigma_i^2 are the AR(1) coefficient and innovation variance for factor i. Observable SEs combine factor uncertainty with idiosyncratic variance.","category":"section"},{"location":"factormodels/#Comparison:-DFM-vs-GDFM","page":"Factor Models","title":"Comparison: DFM vs GDFM","text":"Aspect Dynamic Factor Model Generalized DFM\nApproach Time domain (PCA + VAR) Frequency domain (spectral)\nFactor dynamics Explicit VAR structure Implicit through spectral density\nEstimation Two-step or EM Kernel-smoothed periodogram\nComputational cost Moderate Higher (FFT at each frequency)\nAsymptotics T to infty N T to infty jointly\nBest for Moderate N, focus on forecasting Large N, structural decomposition","category":"section"},{"location":"factormodels/#Example:-Complete-GDFM-Workflow","page":"Factor Models","title":"Example: Complete GDFM Workflow","text":"using MacroEconometricModels\n\n# Load large macroeconomic panel (e.g., FRED-MD)\nX = load_data()  # T×N matrix\n\n# Step 1: Select number of factors\nic = ic_criteria_gdfm(X, 10)\nq = ic.q_ratio\nprintln(\"Selected q = $q dynamic factors\")\n\n# Step 2: Estimate GDFM\nmodel = estimate_gdfm(X, q; kernel=:parzen)\n\n# Step 3: Diagnostics\nprintln(\"Variance explained: \", round.(model.variance_explained, digits=3))\nprintln(\"Mean R²: \", round(mean(r2(model)), digits=3))\n\n# Step 4: Extract common component for further analysis\nχ = model.common_component  # Use in FAVAR, forecasting, etc.\n\n# Step 5: Identify variables driven by common vs idiosyncratic shocks\nshares = common_variance_share(model)\ncommon_driven = findall(shares .> 0.7)\nidio_driven = findall(shares .< 0.3)\n\nReferences:\n\nForni, M., Hallin, M., Lippi, M., & Reichlin, L. (2000). \"The Generalized Dynamic-Factor Model: Identification and Estimation.\"\nForni, M., Hallin, M., Lippi, M., & Reichlin, L. (2005). \"The Generalized Dynamic Factor Model: One-Sided Estimation and Forecasting.\"\nHallin, M., & Liška, R. (2007). \"Determining the Number of Factors in the General Dynamic Factor Model.\"\n\n","category":"section"},{"location":"factormodels/#References","page":"Factor Models","title":"References","text":"","category":"section"},{"location":"factormodels/#Core-References","page":"Factor Models","title":"Core References","text":"Bai, Jushan. 2003. \"Inferential Theory for Factor Models of Large Dimensions.\" Econometrica 71 (1): 135–171. https://doi.org/10.1111/1468-0262.00392\nBai, Jushan, and Serena Ng. 2002. \"Determining the Number of Factors in Approximate Factor Models.\" Econometrica 70 (1): 191–221. https://doi.org/10.1111/1468-0262.00273\nBai, Jushan, and Serena Ng. 2006. \"Confidence Intervals for Diffusion Index Forecasts and Inference for Factor-Augmented Regressions.\" Econometrica 74 (4): 1133–1150. https://doi.org/10.1111/j.1468-0262.2006.00696.x\nStock, James H., and Mark W. Watson. 2002a. \"Forecasting Using Principal Components from a Large Number of Predictors.\" Journal of the American Statistical Association 97 (460): 1167–1179. https://doi.org/10.1198/016214502388618960\nStock, James H., and Mark W. Watson. 2002b. \"Macroeconomic Forecasting Using Diffusion Indexes.\" Journal of Business & Economic Statistics 20 (2): 147–162. https://doi.org/10.1198/073500102317351921","category":"section"},{"location":"factormodels/#Dynamic-Factor-Models-2","page":"Factor Models","title":"Dynamic Factor Models","text":"Doz, Catherine, Domenico Giannone, and Lucrezia Reichlin. 2011. \"A Two-Step Estimator for Large Approximate Dynamic Factor Models Based on Kalman Filtering.\" Journal of Econometrics 164 (1): 188–205. https://doi.org/10.1016/j.jeconom.2011.02.012\nDoz, Catherine, Domenico Giannone, and Lucrezia Reichlin. 2012. \"A Quasi-Maximum Likelihood Approach for Large, Approximate Dynamic Factor Models.\" Review of Economics and Statistics 94 (4): 1014–1024. https://doi.org/10.1162/RESTa00225\nForni, Mario, Marc Hallin, Marco Lippi, and Lucrezia Reichlin. 2000. \"The Generalized Dynamic-Factor Model: Identification and Estimation.\" Review of Economics and Statistics 82 (4): 540–554. https://doi.org/10.1162/003465300559037\nForni, Mario, Marc Hallin, Marco Lippi, and Lucrezia Reichlin. 2005. \"The Generalized Dynamic Factor Model: One-Sided Estimation and Forecasting.\" Journal of the American Statistical Association 100 (471): 830–840. https://doi.org/10.1198/016214504000002050\nHallin, Marc, and Roman Liška. 2007. \"Determining the Number of Factors in the General Dynamic Factor Model.\" Journal of the American Statistical Association 102 (478): 603–617. https://doi.org/10.1198/016214506000001275","category":"section"},{"location":"factormodels/#Applications-2","page":"Factor Models","title":"Applications","text":"Bernanke, Ben S., Jean Boivin, and Piotr Eliasz. 2005. \"Measuring the Effects of Monetary Policy: A Factor-Augmented Vector Autoregressive (FAVAR) Approach.\" Quarterly Journal of Economics 120 (1): 387–422. https://doi.org/10.1162/0033553053327452\nMcCracken, Michael W., and Serena Ng. 2016. \"FRED-MD: A Monthly Database for Macroeconomic Research.\" Journal of Business & Economic Statistics 34 (4): 574–589. https://doi.org/10.1080/07350015.2015.1086655","category":"section"},{"location":"hypothesis_tests/#Hypothesis-Tests","page":"Unit Root & Cointegration","title":"Hypothesis Tests","text":"This chapter covers statistical hypothesis tests for time series analysis, including unit root tests for stationarity detection, cointegration tests for multivariate relationships, and VAR stability diagnostics.","category":"section"},{"location":"hypothesis_tests/#Introduction","page":"Unit Root & Cointegration","title":"Introduction","text":"Before fitting dynamic models like VARs or Local Projections, it is essential to understand the stationarity properties of the data. Non-stationary series (those with unit roots) require different treatment than stationary series, as standard regression methods can lead to spurious results.\n\nMacroEconometricModels.jl provides a comprehensive suite of unit root and stationarity tests.","category":"section"},{"location":"hypothesis_tests/#Quick-Start","page":"Unit Root & Cointegration","title":"Quick Start","text":"adf_result = adf_test(y; lags=:aic, regression=:constant)          # ADF unit root test\nkpss_result = kpss_test(y; regression=:constant)                    # KPSS stationarity test\npp_result = pp_test(y; regression=:constant)                        # Phillips-Perron test\nza_result = za_test(y; regression=:both, trim=0.15)                 # Zivot-Andrews (structural break)\njohansen_result = johansen_test(Y, 2; deterministic=:constant)      # Johansen cointegration","category":"section"},{"location":"hypothesis_tests/#Univariate-Tests","page":"Unit Root & Cointegration","title":"Univariate Tests","text":"ADF (Augmented Dickey-Fuller): Tests the null of a unit root against stationarity\nKPSS: Tests the null of stationarity against a unit root\nPhillips-Perron: Non-parametric unit root test with autocorrelation correction\nZivot-Andrews: Unit root test allowing for endogenous structural break\nNg-Perron: Modified tests with improved size properties","category":"section"},{"location":"hypothesis_tests/#Multivariate-Tests","page":"Unit Root & Cointegration","title":"Multivariate Tests","text":"Johansen Cointegration: Tests for cointegrating relationships among variables","category":"section"},{"location":"hypothesis_tests/#Model-Diagnostics","page":"Unit Root & Cointegration","title":"Model Diagnostics","text":"VAR Stationarity: Check if an estimated VAR model is stable\n\n","category":"section"},{"location":"hypothesis_tests/#Augmented-Dickey-Fuller-Test","page":"Unit Root & Cointegration","title":"Augmented Dickey-Fuller Test","text":"","category":"section"},{"location":"hypothesis_tests/#Theory","page":"Unit Root & Cointegration","title":"Theory","text":"The Augmented Dickey-Fuller (ADF) test examines whether a time series has a unit root. Consider the autoregressive model:\n\ny_t = rho y_t-1 + u_t\n\nThe null hypothesis is H_0 rho = 1 (unit root) against H_1 rho  1 (stationary).\n\nThe test is performed via the regression:\n\nDelta y_t = alpha + beta t + gamma y_t-1 + sum_j=1^p delta_j Delta y_t-j + varepsilon_t\n\nwhere:\n\ngamma = rho - 1 is the coefficient of interest\nalpha is an optional constant\nbeta t is an optional linear trend\nLagged differences are included to control for serial correlation\n\nThe ADF statistic is the t-ratio tau = hatgamma  textse(hatgamma).\n\nCritical values depend on the specification (none, constant, or trend) and are tabulated using MacKinnon (1994, 2010) response surfaces.\n\nReference: Dickey & Fuller (1979), MacKinnon (2010)","category":"section"},{"location":"hypothesis_tests/#Julia-Implementation","page":"Unit Root & Cointegration","title":"Julia Implementation","text":"using MacroEconometricModels\n\n# Generate a random walk (has unit root)\ny = cumsum(randn(200))\n\n# ADF test with automatic lag selection via AIC\nresult = adf_test(y; lags=:aic, regression=:constant)\n\n# The result displays with publication-quality formatting:\n# - Test statistic and significance stars\n# - Critical values at 1%, 5%, 10% levels\n# - Automatic conclusion","category":"section"},{"location":"hypothesis_tests/#Function-Signature","page":"Unit Root & Cointegration","title":"Function Signature","text":"","category":"section"},{"location":"hypothesis_tests/#Options","page":"Unit Root & Cointegration","title":"Options","text":"Argument Description Default\nlags Number of augmenting lags, or :aic/:bic/:hqic for automatic selection :aic\nmax_lags Maximum lags for automatic selection floor(12*(T/100)^0.25)\nregression Deterministic terms: :none, :constant, or :trend :constant","category":"section"},{"location":"hypothesis_tests/#ADFResult-Return-Values","page":"Unit Root & Cointegration","title":"ADFResult Return Values","text":"Field Type Description\nstatistic T ADF test statistic (tau-ratio)\npvalue T Asymptotic p-value (MacKinnon response surface)\nlags Int Number of augmenting lags used\nregression Symbol Deterministic specification (:none, :constant, :trend)\ncritical_values Dict{Int,T} Critical values at 1%, 5%, 10% significance levels\nnobs Int Number of observations used","category":"section"},{"location":"hypothesis_tests/#Interpreting-Results","page":"Unit Root & Cointegration","title":"Interpreting Results","text":"Reject H₀ (p-value < 0.05): Evidence against unit root; series appears stationary\nFail to reject H₀ (p-value > 0.05): Cannot reject unit root; series may be non-stationary\n\n","category":"section"},{"location":"hypothesis_tests/#KPSS-Stationarity-Test","page":"Unit Root & Cointegration","title":"KPSS Stationarity Test","text":"","category":"section"},{"location":"hypothesis_tests/#Theory-2","page":"Unit Root & Cointegration","title":"Theory","text":"The KPSS test (Kwiatkowski, Phillips, Schmidt & Shin, 1992) reverses the hypotheses of the ADF test:\n\nH_0: Series is stationary (level or trend stationary)\nH_1: Series has a unit root\n\nThis complementary approach is valuable because failure to reject in the ADF test does not confirm stationarity—it may simply reflect low power.\n\nThe test decomposes the series:\n\ny_t = xi t + r_t + varepsilon_t\n\nwhere r_t = r_t-1 + u_t is a random walk. Under H_0, the variance of u_t is zero.\n\nThe KPSS statistic is:\n\ntextKPSS = fracsum_t=1^T S_t^2T^2 hatsigma^2_LR\n\nwhere S_t = sum_s=1^t hate_s are partial sums of residuals and hatsigma^2_LR is the long-run variance estimated using a Bartlett kernel.\n\nReference: Kwiatkowski et al. (1992)","category":"section"},{"location":"hypothesis_tests/#Julia-Implementation-2","page":"Unit Root & Cointegration","title":"Julia Implementation","text":"using MacroEconometricModels\n\n# Stationary series\ny = randn(200)\nresult = kpss_test(y; regression=:constant)\n\n# For trend stationarity\nresult_trend = kpss_test(y; regression=:trend)","category":"section"},{"location":"hypothesis_tests/#Function-Signature-2","page":"Unit Root & Cointegration","title":"Function Signature","text":"","category":"section"},{"location":"hypothesis_tests/#Options-2","page":"Unit Root & Cointegration","title":"Options","text":"Argument Description Default\nregression Stationarity type: :constant (level) or :trend :constant\nbandwidth Bartlett kernel bandwidth, or :auto for Newey-West selection :auto","category":"section"},{"location":"hypothesis_tests/#KPSSResult-Return-Values","page":"Unit Root & Cointegration","title":"KPSSResult Return Values","text":"Field Type Description\nstatistic T KPSS test statistic\npvalue T Asymptotic p-value\nregression Symbol Stationarity type (:constant or :trend)\ncritical_values Dict{Int,T} Critical values at 1%, 5%, 10%\nbandwidth Int Bartlett kernel bandwidth used\nnobs Int Number of observations","category":"section"},{"location":"hypothesis_tests/#Interpreting-Results-2","page":"Unit Root & Cointegration","title":"Interpreting Results","text":"Reject H₀ (p-value < 0.05): Evidence against stationarity; series has a unit root\nFail to reject H₀ (p-value > 0.05): Cannot reject stationarity","category":"section"},{"location":"hypothesis_tests/#Combining-ADF-and-KPSS","page":"Unit Root & Cointegration","title":"Combining ADF and KPSS","text":"Using both tests together provides stronger inference:\n\nADF Result KPSS Result Conclusion\nReject H₀ (stationary) Fail to reject H₀ (stationary) Stationary\nFail to reject H₀ (unit root) Reject H₀ (unit root) Unit root\nReject H₀ Reject H₀ Conflicting (possible structural break)\nFail to reject H₀ Fail to reject H₀ Inconclusive\n\n","category":"section"},{"location":"hypothesis_tests/#Phillips-Perron-Test","page":"Unit Root & Cointegration","title":"Phillips-Perron Test","text":"","category":"section"},{"location":"hypothesis_tests/#Theory-3","page":"Unit Root & Cointegration","title":"Theory","text":"The Phillips-Perron (PP) test is a non-parametric alternative to the ADF test. Instead of augmenting with lagged differences, the PP test corrects the t-statistic for serial correlation using Newey-West standard errors.\n\nThe regression is:\n\ny_t = alpha + rho y_t-1 + u_t\n\nThe PP Z_t statistic adjusts the OLS t-ratio:\n\nZ_t = sqrtfrachatgamma_0hatlambda^2 t_rho - frachatlambda^2 - hatgamma_02hatlambda cdot textse(hatrho) cdot sqrtT\n\nwhere hatgamma_0 is the short-run variance and hatlambda^2 is the long-run variance.\n\nAdvantage: Does not require specifying the number of lags.\n\nReference: Phillips & Perron (1988)","category":"section"},{"location":"hypothesis_tests/#Julia-Implementation-3","page":"Unit Root & Cointegration","title":"Julia Implementation","text":"using MacroEconometricModels\n\ny = cumsum(randn(200))\nresult = pp_test(y; regression=:constant)","category":"section"},{"location":"hypothesis_tests/#Function-Signature-3","page":"Unit Root & Cointegration","title":"Function Signature","text":"","category":"section"},{"location":"hypothesis_tests/#Options-3","page":"Unit Root & Cointegration","title":"Options","text":"Argument Description Default\nregression Deterministic terms: :none, :constant, or :trend :constant\nbandwidth Newey-West bandwidth, or :auto :auto","category":"section"},{"location":"hypothesis_tests/#PPResult-Return-Values","page":"Unit Root & Cointegration","title":"PPResult Return Values","text":"Field Type Description\nstatistic T Phillips-Perron Z_t test statistic\npvalue T Asymptotic p-value\nregression Symbol Deterministic specification\ncritical_values Dict{Int,T} Critical values at 1%, 5%, 10%\nbandwidth Int Newey-West bandwidth used\nnobs Int Number of observations\n\n","category":"section"},{"location":"hypothesis_tests/#Zivot-Andrews-Test","page":"Unit Root & Cointegration","title":"Zivot-Andrews Test","text":"","category":"section"},{"location":"hypothesis_tests/#Theory-4","page":"Unit Root & Cointegration","title":"Theory","text":"The Zivot-Andrews test extends the ADF test by allowing for an endogenous structural break in the series. This is important because standard unit root tests have low power against stationary alternatives with structural breaks.\n\nThree specifications are available:\n\nBreak in intercept (:constant):\n\nDelta y_t = alpha + beta t + theta DU_t + gamma y_t-1 + sum_j delta_j Delta y_t-j + varepsilon_t\n\nBreak in trend (:trend):\n\nDelta y_t = alpha + beta t + phi DT_t + gamma y_t-1 + sum_j delta_j Delta y_t-j + varepsilon_t\n\nBreak in both (:both):\n\nDelta y_t = alpha + beta t + theta DU_t + phi DT_t + gamma y_t-1 + sum_j delta_j Delta y_t-j + varepsilon_t\n\nwhere:\n\nDU_t = 1 if t  T_B (level shift dummy)\nDT_t = t - T_B if t  T_B (trend shift dummy)\nT_B is the break point, selected to minimize the t-statistic on gamma\n\nReference: Zivot & Andrews (1992)","category":"section"},{"location":"hypothesis_tests/#Julia-Implementation-4","page":"Unit Root & Cointegration","title":"Julia Implementation","text":"using MacroEconometricModels\n\n# Series with structural break\ny = vcat(randn(100), randn(100) .+ 2)  # Level shift at t=100\nresult = za_test(y; regression=:constant)\n\n# Access break point\nprintln(\"Break detected at observation: \", result.break_index)\nprintln(\"Break location: \", result.break_fraction * 100, \"% of sample\")","category":"section"},{"location":"hypothesis_tests/#Function-Signature-4","page":"Unit Root & Cointegration","title":"Function Signature","text":"","category":"section"},{"location":"hypothesis_tests/#Options-4","page":"Unit Root & Cointegration","title":"Options","text":"Argument Description Default\nregression Break type: :constant, :trend, or :both :both\ntrim Trimming fraction for break search 0.15\nlags Augmenting lags, or :aic/:bic :aic","category":"section"},{"location":"hypothesis_tests/#ZAResult-Return-Values","page":"Unit Root & Cointegration","title":"ZAResult Return Values","text":"Field Type Description\nstatistic T Minimum ADF t-statistic over break candidates\npvalue T Asymptotic p-value\nbreak_index Int Estimated break point (observation index)\nbreak_fraction T Break location as fraction of sample (0 to 1)\nregression Symbol Break type (:constant, :trend, :both)\ncritical_values Dict{Int,T} Critical values at 1%, 5%, 10%\nlags Int Number of augmenting lags\nnobs Int Number of observations\n\n","category":"section"},{"location":"hypothesis_tests/#Ng-Perron-Tests","page":"Unit Root & Cointegration","title":"Ng-Perron Tests","text":"","category":"section"},{"location":"hypothesis_tests/#Theory-5","page":"Unit Root & Cointegration","title":"Theory","text":"The Ng-Perron tests (2001) are modified unit root tests with improved size and power properties, especially in small samples. They use GLS detrending and report four test statistics:\n\nMZα: Modified Phillips Zα statistic\nMZt: Modified Phillips Zt statistic (most commonly used)\nMSB: Modified Sargan-Bhargava statistic\nMPT: Modified Point-optimal statistic\n\nThe GLS detrending uses the quasi-difference:\n\ntildey_t = y_t - barcT cdot y_t-1\n\nwhere barc = -7 (constant) or barc = -135 (trend).\n\nAdvantage: Better size properties than ADF when the initial condition is far from zero.\n\nReference: Ng & Perron (2001)","category":"section"},{"location":"hypothesis_tests/#Julia-Implementation-5","page":"Unit Root & Cointegration","title":"Julia Implementation","text":"using MacroEconometricModels\n\ny = cumsum(randn(100))\nresult = ngperron_test(y; regression=:constant)\n\n# All four statistics are reported\nprintln(\"MZα: \", result.MZa)\nprintln(\"MZt: \", result.MZt)\nprintln(\"MSB: \", result.MSB)\nprintln(\"MPT: \", result.MPT)","category":"section"},{"location":"hypothesis_tests/#Function-Signature-5","page":"Unit Root & Cointegration","title":"Function Signature","text":"","category":"section"},{"location":"hypothesis_tests/#NgPerronResult-Return-Values","page":"Unit Root & Cointegration","title":"NgPerronResult Return Values","text":"Field Type Description\nMZa T Modified Phillips Z_alpha statistic\nMZt T Modified Phillips Z_t statistic (most commonly reported)\nMSB T Modified Sargan-Bhargava statistic\nMPT T Modified Point-optimal statistic\nregression Symbol Deterministic specification\ncritical_values Dict{Symbol,Dict{Int,T}} Critical values keyed by statistic name (:MZa, :MZt, :MSB, :MPT)\nnobs Int Number of observations\n\nnote: Technical Note\nThe Ng-Perron tests use GLS detrending which provides substantially better size properties than the standard ADF test in small samples (T  100). When the ADF test has borderline results, the Ng-Perron MZt statistic is a more reliable indicator. However, ADF remains preferable when the data-generating process has a large negative MA root, as GLS-based tests can be oversized in that case (Perron & Ng, 1996).\n\n","category":"section"},{"location":"hypothesis_tests/#Johansen-Cointegration-Test","page":"Unit Root & Cointegration","title":"Johansen Cointegration Test","text":"","category":"section"},{"location":"hypothesis_tests/#Theory-6","page":"Unit Root & Cointegration","title":"Theory","text":"The Johansen test examines whether multiple I(1) series share common stochastic trends, i.e., are cointegrated. Consider a VAR(p) in levels:\n\ny_t = A_1 y_t-1 + cdots + A_p y_t-p + u_t\n\nThis can be rewritten in Vector Error Correction Model (VECM) form:\n\nDelta y_t = Pi y_t-1 + sum_i=1^p-1 Gamma_i Delta y_t-i + u_t\n\nwhere Pi = alpha beta is the long-run matrix:\n\nbeta: Cointegrating vectors (equilibrium relationships)\nalpha: Adjustment coefficients (speed of adjustment to equilibrium)\ntextrank(Pi) = r: Number of cointegrating relationships\n\nTwo test statistics are computed:\n\nTrace Test: Tests H_0 textrank leq r against H_1 textrank  r\n\nlambda_trace(r) = -T sum_i=r+1^n ln(1 - hatlambda_i)\n\nMaximum Eigenvalue Test: Tests H_0 textrank = r against H_1 textrank = r+1\n\nlambda_max(r) = -T ln(1 - hatlambda_r+1)\n\nReference: Johansen (1991), Osterwald-Lenum (1992)","category":"section"},{"location":"hypothesis_tests/#Julia-Implementation-6","page":"Unit Root & Cointegration","title":"Julia Implementation","text":"using MacroEconometricModels\n\n# Generate cointegrated system\nT, n = 200, 3\nY = randn(T, n)\nY[:, 2] = Y[:, 1] + 0.1 * randn(T)  # Y2 cointegrated with Y1\nY[:, 3] = cumsum(randn(T))           # Y3 independent I(1)\n\n# Johansen test with 2 lags in VECM\nresult = johansen_test(Y, 2; deterministic=:constant)\n\n# Access results\nprintln(\"Estimated cointegration rank: \", result.rank)\nprintln(\"Cointegrating vectors:\\n\", result.eigenvectors[:, 1:result.rank])\nprintln(\"Adjustment coefficients:\\n\", result.adjustment)","category":"section"},{"location":"hypothesis_tests/#Function-Signature-6","page":"Unit Root & Cointegration","title":"Function Signature","text":"","category":"section"},{"location":"hypothesis_tests/#Options-5","page":"Unit Root & Cointegration","title":"Options","text":"Argument Description Default\np Lags in VECM representation Required\ndeterministic :none, :constant, or :trend :constant","category":"section"},{"location":"hypothesis_tests/#JohansenResult-Return-Values","page":"Unit Root & Cointegration","title":"JohansenResult Return Values","text":"Field Type Description\ntrace_stats Vector{T} Trace test statistics for each rank hypothesis\ntrace_pvalues Vector{T} P-values for trace statistics\nmax_eigen_stats Vector{T} Maximum eigenvalue test statistics\nmax_eigen_pvalues Vector{T} P-values for max eigenvalue statistics\nrank Int Estimated cointegration rank\neigenvectors Matrix{T} n times n matrix of cointegrating vectors (columns)\nadjustment Matrix{T} n times n adjustment (loading) matrix alpha\neigenvalues Vector{T} Ordered eigenvalues from reduced-rank regression\ncritical_values_trace Matrix{T} n times 3 critical values for trace test (1%, 5%, 10%)\ncritical_values_max Matrix{T} n times 3 critical values for max eigenvalue test\ndeterministic Symbol Deterministic specification (:none, :constant, :trend)\nlags Int Number of VECM lags\nnobs Int Number of observations","category":"section"},{"location":"hypothesis_tests/#Interpreting-Results-3","page":"Unit Root & Cointegration","title":"Interpreting Results","text":"The test sequentially tests:\n\nH_0 r = 0 (no cointegration)\nH_0 r leq 1\nH_0 r leq 2, etc.\n\nStop at the first non-rejected hypothesis; that gives the cointegration rank.\n\n","category":"section"},{"location":"hypothesis_tests/#VAR-Stationarity-Check","page":"Unit Root & Cointegration","title":"VAR Stationarity Check","text":"","category":"section"},{"location":"hypothesis_tests/#Theory-7","page":"Unit Root & Cointegration","title":"Theory","text":"A VAR(p) model is stable (stationary) if and only if all eigenvalues of the companion matrix lie strictly inside the unit circle:\n\nF = beginbmatrix\nA_1  A_2  cdots  A_p-1  A_p \nI_n  0  cdots  0  0 \n0  I_n  cdots  0  0 \nvdots   ddots   vdots \n0  0  cdots  I_n  0\nendbmatrix\n\nStability Condition: lambda_i  1 for all eigenvalues lambda_i of F.\n\nIf violated, the VAR is explosive or contains unit roots, and standard asymptotic theory does not apply.","category":"section"},{"location":"hypothesis_tests/#Julia-Implementation-7","page":"Unit Root & Cointegration","title":"Julia Implementation","text":"using MacroEconometricModels\n\n# Estimate VAR\nY = randn(200, 3)\nmodel = fit(VARModel, Y, 2)\n\n# Check stationarity\nresult = is_stationary(model)\n\nif result.is_stationary\n    println(\"VAR is stationary\")\n    println(\"Maximum eigenvalue modulus: \", result.max_modulus)\nelse\n    println(\"WARNING: VAR is non-stationary!\")\n    println(\"Maximum eigenvalue modulus: \", result.max_modulus)\n    println(\"Consider differencing or VECM specification\")\nend","category":"section"},{"location":"hypothesis_tests/#Function-Signature-7","page":"Unit Root & Cointegration","title":"Function Signature","text":"","category":"section"},{"location":"hypothesis_tests/#VARStationarityResult-Return-Values","page":"Unit Root & Cointegration","title":"VARStationarityResult Return Values","text":"Field Type Description\nis_stationary Bool true if all eigenvalues lie inside unit circle\neigenvalues Vector{E} Eigenvalues of the companion matrix (may be complex)\nmax_modulus T Maximum eigenvalue modulus (should be  1 for stability)\ncompanion_matrix Matrix{T} np times np companion matrix\n\n","category":"section"},{"location":"hypothesis_tests/#Convenience-Functions","page":"Unit Root & Cointegration","title":"Convenience Functions","text":"","category":"section"},{"location":"hypothesis_tests/#Summary-of-Multiple-Tests","page":"Unit Root & Cointegration","title":"Summary of Multiple Tests","text":"using MacroEconometricModels\n\ny = cumsum(randn(200))\n\n# Run multiple tests and get summary\nsummary = unit_root_summary(y; tests=[:adf, :kpss, :pp])\n\n# Access individual results\nsummary.results[:adf]\nsummary.results[:kpss]\n\n# Overall conclusion\nprintln(summary.conclusion)","category":"section"},{"location":"hypothesis_tests/#Test-All-Variables","page":"Unit Root & Cointegration","title":"Test All Variables","text":"using MacroEconometricModels\n\nY = randn(200, 5)\nY[:, 1] = cumsum(Y[:, 1])  # Make first column non-stationary\n\n# Apply ADF test to all columns\nresults = test_all_variables(Y; test=:adf)\n\n# Check which variables have unit roots\nfor (i, r) in enumerate(results)\n    status = r.pvalue > 0.05 ? \"I(1)\" : \"I(0)\"\n    println(\"Variable $i: p=$(round(r.pvalue, digits=3)) → $status\")\nend","category":"section"},{"location":"hypothesis_tests/#Function-Signatures","page":"Unit Root & Cointegration","title":"Function Signatures","text":"","category":"section"},{"location":"hypothesis_tests/#Result-Types","page":"Unit Root & Cointegration","title":"Result Types","text":"All unit root test results inherit from AbstractUnitRootTest and implement the StatsAPI interface:\n\nusing StatsAPI\n\nresult = adf_test(y)\n\n# StatsAPI interface\nnobs(result)    # Number of observations\ndof(result)     # Degrees of freedom\npvalue(result)  # P-value","category":"section"},{"location":"hypothesis_tests/#Type-Hierarchy","page":"Unit Root & Cointegration","title":"Type Hierarchy","text":"All unit root test results inherit from AbstractUnitRootTest and implement the StatsAPI interface. See the API Reference for detailed type documentation.\n\nADFResult - Augmented Dickey-Fuller test result\nKPSSResult - KPSS stationarity test result\nPPResult - Phillips-Perron test result\nZAResult - Zivot-Andrews structural break test result\nNgPerronResult - Ng-Perron test result (MZα, MZt, MSB, MPT)\nJohansenResult - Johansen cointegration test result\nVARStationarityResult - VAR model stationarity check result\n\n","category":"section"},{"location":"hypothesis_tests/#Practical-Workflow","page":"Unit Root & Cointegration","title":"Practical Workflow","text":"","category":"section"},{"location":"hypothesis_tests/#Step-by-Step-Unit-Root-Analysis","page":"Unit Root & Cointegration","title":"Step-by-Step Unit Root Analysis","text":"using MacroEconometricModels\n\n# 1. Load/generate data\ny = your_time_series\n\n# 2. Visual inspection (plot the series)\n# Look for trends, structural breaks, etc.\n\n# 3. Test for unit root with ADF\nadf_result = adf_test(y; regression=:constant)\n\n# 4. Confirm with KPSS (opposite null)\nkpss_result = kpss_test(y; regression=:constant)\n\n# 5. If structural break suspected, use Zivot-Andrews\nza_result = za_test(y; regression=:both)\n\n# 6. For small samples, use Ng-Perron\nnp_result = ngperron_test(y; regression=:constant)\n\n# 7. Decision matrix\nif pvalue(adf_result) < 0.05 && pvalue(kpss_result) > 0.05\n    println(\"Series is stationary - proceed with VAR in levels\")\nelseif pvalue(adf_result) > 0.05 && pvalue(kpss_result) < 0.05\n    println(\"Series has unit root - consider differencing or VECM\")\nelse\n    println(\"Inconclusive - examine further or use robust methods\")\nend","category":"section"},{"location":"hypothesis_tests/#Pre-VAR-Analysis","page":"Unit Root & Cointegration","title":"Pre-VAR Analysis","text":"using MacroEconometricModels\n\n# Multi-variable dataset\nY = your_data_matrix\n\n# 1. Test each variable for unit root\nresults = test_all_variables(Y; test=:adf)\nn_nonstationary = sum(r.pvalue > 0.05 for r in results)\nprintln(\"Variables with unit roots: $n_nonstationary / $(size(Y, 2))\")\n\n# 2. If all I(1), test for cointegration\nif n_nonstationary == size(Y, 2)\n    johansen_result = johansen_test(Y, 2)\n\n    if johansen_result.rank > 0\n        println(\"Cointegration detected! Use VECM with rank=$(johansen_result.rank)\")\n    else\n        println(\"No cointegration - use VAR in first differences\")\n    end\nend\n\n# 3. If mixed I(0)/I(1), be cautious\n# Consider ARDL bounds test or transform I(1) variables\n\n","category":"section"},{"location":"hypothesis_tests/#References","page":"Unit Root & Cointegration","title":"References","text":"","category":"section"},{"location":"hypothesis_tests/#Unit-Root-Tests","page":"Unit Root & Cointegration","title":"Unit Root Tests","text":"Dickey, David A., and Wayne A. Fuller. 1979. \"Distribution of the Estimators for Autoregressive Time Series with a Unit Root.\" Journal of the American Statistical Association 74 (366): 427–431. https://doi.org/10.1080/01621459.1979.10482531\nKwiatkowski, Denis, Peter C. B. Phillips, Peter Schmidt, and Yongcheol Shin. 1992. \"Testing the Null Hypothesis of Stationarity Against the Alternative of a Unit Root.\" Journal of Econometrics 54 (1–3): 159–178. https://doi.org/10.1016/0304-4076(92)90104-Y\nMacKinnon, James G. 2010. \"Critical Values for Cointegration Tests.\" Queen's Economics Department Working Paper No. 1227.\nNg, Serena, and Pierre Perron. 2001. \"Lag Length Selection and the Construction of Unit Root Tests with Good Size and Power.\" Econometrica 69 (6): 1519–1554. https://doi.org/10.1111/1468-0262.00256\nPhillips, Peter C. B., and Pierre Perron. 1988. \"Testing for a Unit Root in Time Series Regression.\" Biometrika 75 (2): 335–346. https://doi.org/10.1093/biomet/75.2.335\nZivot, Eric, and Donald W. K. Andrews. 1992. \"Further Evidence on the Great Crash, the Oil-Price Shock, and the Unit-Root Hypothesis.\" Journal of Business & Economic Statistics 10 (3): 251–270. https://doi.org/10.1080/07350015.1992.10509904","category":"section"},{"location":"hypothesis_tests/#Cointegration","page":"Unit Root & Cointegration","title":"Cointegration","text":"Johansen, Søren. 1991. \"Estimation and Hypothesis Testing of Cointegration Vectors in Gaussian Vector Autoregressive Models.\" Econometrica 59 (6): 1551–1580. https://doi.org/10.2307/2938278\nJohansen, Søren. 1995. Likelihood-Based Inference in Cointegrated Vector Autoregressive Models. Oxford: Oxford University Press. ISBN 978-0-19-877450-5.\nOsterwald-Lenum, Michael. 1992. \"A Note with Quantiles of the Asymptotic Distribution of the Maximum Likelihood Cointegration Rank Test Statistics.\" Oxford Bulletin of Economics and Statistics 54 (3): 461–472. https://doi.org/10.1111/j.1468-0084.1992.tb00013.x","category":"section"},{"location":"hypothesis_tests/#Textbooks","page":"Unit Root & Cointegration","title":"Textbooks","text":"Hamilton, James D. 1994. Time Series Analysis. Princeton, NJ: Princeton University Press. ISBN 978-0-691-04289-3.\nLütkepohl, Helmut. 2005. New Introduction to Multiple Time Series Analysis. Berlin: Springer. ISBN 978-3-540-40172-8.\nEnders, Walter. 2014. Applied Econometric Time Series. 4th ed. Hoboken, NJ: Wiley. ISBN 978-1-118-80856-6.","category":"section"},{"location":"hypothesis_tests/#MacroEconometricModels.adf_test","page":"Unit Root & Cointegration","title":"MacroEconometricModels.adf_test","text":"adf_test(y; lags=:aic, max_lags=nothing, regression=:constant) -> ADFResult\n\nAugmented Dickey-Fuller test for unit root.\n\nTests H₀: y has a unit root (non-stationary) against H₁: y is stationary.\n\nArguments\n\ny: Time series vector\nlags: Number of augmenting lags, or :aic/:bic/:hqic for automatic selection\nmax_lags: Maximum lags for automatic selection (default: floor(12*(T/100)^0.25))\nregression: Deterministic terms - :none, :constant (default), or :trend\n\nReturns\n\nADFResult containing test statistic, p-value, critical values, etc.\n\nExample\n\ny = cumsum(randn(200))  # Random walk (has unit root)\nresult = adf_test(y)\nresult.pvalue > 0.05  # Should fail to reject H₀\n\nReferences\n\nDickey, D. A., & Fuller, W. A. (1979). Distribution of the estimators for autoregressive time series with a unit root. JASA, 74(366), 427-431.\nMacKinnon, J. G. (2010). Critical values for cointegration tests. Queen's Economics Department Working Paper No. 1227.\n\n\n\n\n\n","category":"function"},{"location":"hypothesis_tests/#MacroEconometricModels.kpss_test","page":"Unit Root & Cointegration","title":"MacroEconometricModels.kpss_test","text":"kpss_test(y; regression=:constant, bandwidth=:auto) -> KPSSResult\n\nKwiatkowski-Phillips-Schmidt-Shin test for stationarity.\n\nTests H₀: y is stationary against H₁: y has a unit root.\n\nArguments\n\ny: Time series vector\nregression: :constant (level stationarity) or :trend (trend stationarity)\nbandwidth: Bartlett kernel bandwidth, or :auto for Newey-West selection\n\nReturns\n\nKPSSResult containing test statistic, p-value, critical values, etc.\n\nExample\n\ny = randn(200)  # Stationary series\nresult = kpss_test(y)\nresult.pvalue > 0.05  # Should fail to reject H₀ (stationarity)\n\nReferences\n\nKwiatkowski, D., Phillips, P. C., Schmidt, P., & Shin, Y. (1992). Testing the null hypothesis of stationarity against the alternative of a unit root. Journal of Econometrics, 54(1-3), 159-178.\n\n\n\n\n\n","category":"function"},{"location":"hypothesis_tests/#MacroEconometricModels.pp_test","page":"Unit Root & Cointegration","title":"MacroEconometricModels.pp_test","text":"pp_test(y; regression=:constant, bandwidth=:auto) -> PPResult\n\nPhillips-Perron test for unit root with non-parametric correction.\n\nTests H₀: y has a unit root against H₁: y is stationary.\n\nArguments\n\ny: Time series vector\nregression: :none, :constant (default), or :trend\nbandwidth: Newey-West bandwidth, or :auto for automatic selection\n\nReturns\n\nPPResult containing test statistic (Zt), p-value, critical values, etc.\n\nExample\n\ny = cumsum(randn(200))  # Random walk\nresult = pp_test(y)\nresult.pvalue > 0.05  # Should fail to reject H₀\n\nReferences\n\nPhillips, P. C., & Perron, P. (1988). Testing for a unit root in time series regression. Biometrika, 75(2), 335-346.\n\n\n\n\n\n","category":"function"},{"location":"hypothesis_tests/#MacroEconometricModels.za_test","page":"Unit Root & Cointegration","title":"MacroEconometricModels.za_test","text":"za_test(y; regression=:both, trim=0.15, lags=:aic, max_lags=nothing) -> ZAResult\n\nZivot-Andrews test for unit root with endogenous structural break.\n\nTests H₀: y has a unit root without break against H₁: y is stationary with break.\n\nArguments\n\ny: Time series vector\nregression: Type of break - :constant (intercept), :trend (slope), or :both\ntrim: Trimming fraction for break search (default 0.15)\nlags: Number of augmenting lags, or :aic/:bic for automatic selection\nmax_lags: Maximum lags for selection\n\nReturns\n\nZAResult containing minimum t-statistic, break point, p-value, etc.\n\nExample\n\n# Series with structural break\ny = vcat(randn(100), randn(100) .+ 2)\nresult = za_test(y; regression=:constant)\n\nReferences\n\nZivot, E., & Andrews, D. W. K. (1992). Further evidence on the great crash, the oil-price shock, and the unit-root hypothesis. JBES, 10(3), 251-270.\n\n\n\n\n\n","category":"function"},{"location":"hypothesis_tests/#MacroEconometricModels.ngperron_test","page":"Unit Root & Cointegration","title":"MacroEconometricModels.ngperron_test","text":"ngperron_test(y; regression=:constant) -> NgPerronResult\n\nNg-Perron unit root tests with GLS detrending (MZα, MZt, MSB, MPT).\n\nTests H₀: y has a unit root against H₁: y is stationary. These tests have better size properties than ADF/PP in small samples.\n\nArguments\n\ny: Time series vector\nregression: :constant (default) or :trend\n\nReturns\n\nNgPerronResult containing MZα, MZt, MSB, MPT statistics and critical values.\n\nExample\n\ny = cumsum(randn(100))\nresult = ngperron_test(y)\n# Check if MZt rejects at 5%\nresult.MZt < result.critical_values[:MZt][5]\n\nReferences\n\nNg, S., & Perron, P. (2001). Lag length selection and the construction of unit root tests with good size and power. Econometrica, 69(6), 1519-1554.\n\n\n\n\n\n","category":"function"},{"location":"hypothesis_tests/#MacroEconometricModels.johansen_test","page":"Unit Root & Cointegration","title":"MacroEconometricModels.johansen_test","text":"johansen_test(Y, p; deterministic=:constant) -> JohansenResult\n\nJohansen cointegration test for VAR system.\n\nTests for the number of cointegrating relationships among variables using trace and maximum eigenvalue tests.\n\nArguments\n\nY: Data matrix (T × n)\np: Number of lags in the VECM representation\ndeterministic: Specification for deterministic terms\n:none - No deterministic terms\n:constant - Constant in cointegrating relation (default)\n:trend - Linear trend in levels\n\nReturns\n\nJohansenResult containing trace and max-eigenvalue statistics, cointegrating vectors, adjustment coefficients, and estimated rank.\n\nExample\n\n# Generate cointegrated system\nn, T = 3, 200\nY = randn(T, n)\nY[:, 2] = Y[:, 1] + 0.1 * randn(T)  # Y2 cointegrated with Y1\n\nresult = johansen_test(Y, 2)\nresult.rank  # Should detect 1 or 2 cointegrating relations\n\nReferences\n\nJohansen, S. (1991). Estimation and hypothesis testing of cointegration vectors in Gaussian vector autoregressive models. Econometrica, 59(6), 1551-1580.\nOsterwald-Lenum, M. (1992). A note with quantiles of the asymptotic distribution of the ML cointegration rank test statistics. Oxford BEJM.\n\n\n\n\n\n","category":"function"},{"location":"hypothesis_tests/#MacroEconometricModels.is_stationary","page":"Unit Root & Cointegration","title":"MacroEconometricModels.is_stationary","text":"is_stationary(model::VARModel) -> VARStationarityResult\n\nCheck if estimated VAR model is stationary.\n\nA VAR(p) is stationary if and only if all eigenvalues of the companion matrix have modulus strictly less than 1.\n\nReturns\n\nVARStationarityResult with:\n\nis_stationary: Boolean indicating stationarity\neigenvalues: Complex eigenvalues of companion matrix\nmax_modulus: Maximum eigenvalue modulus\ncompanion_matrix: The (np × np) companion form matrix\n\nExample\n\nmodel = estimate_var(Y, 2)\nresult = is_stationary(model)\nif !result.is_stationary\n    println(\"Warning: VAR is non-stationary, max modulus = \", result.max_modulus)\nend\n\n\n\n\n\nis_stationary(model::DynamicFactorModel) -> Bool\n\nCheck if factor dynamics are stationary (max |eigenvalue| < 1).\n\n\n\n\n\n","category":"function"},{"location":"hypothesis_tests/#MacroEconometricModels.unit_root_summary","page":"Unit Root & Cointegration","title":"MacroEconometricModels.unit_root_summary","text":"unit_root_summary(y; tests=[:adf, :kpss, :pp], kwargs...) -> NamedTuple\n\nRun multiple unit root tests and return summary with PrettyTables output.\n\nArguments\n\ny: Time series vector\ntests: Vector of test symbols to run (default: [:adf, :kpss, :pp])\nkwargs...: Additional arguments passed to individual tests\n\nReturns\n\nNamedTuple with test results, conclusion, and summary table.\n\nExample\n\ny = cumsum(randn(200))\nsummary = unit_root_summary(y)\nsummary.conclusion  # Overall conclusion\n\n\n\n\n\n","category":"function"},{"location":"hypothesis_tests/#MacroEconometricModels.test_all_variables","page":"Unit Root & Cointegration","title":"MacroEconometricModels.test_all_variables","text":"test_all_variables(Y; test=:adf, kwargs...) -> Vector\n\nApply unit root test to each column of Y.\n\nArguments\n\nY: Data matrix (T × n)\ntest: Test to apply (:adf, :kpss, :pp, :za, :ngperron)\nkwargs...: Additional arguments passed to the test\n\nReturns\n\nVector of test results, one per variable.\n\nExample\n\nY = randn(200, 3)\nY[:, 1] = cumsum(Y[:, 1])  # Make first column non-stationary\nresults = test_all_variables(Y; test=:adf)\n[r.pvalue for r in results]  # P-values for each variable\n\n\n\n\n\n","category":"function"},{"location":"innovation_accounting/#Innovation-Accounting","page":"Innovation Accounting","title":"Innovation Accounting","text":"Innovation accounting refers to the collection of tools for analyzing the dynamic effects of structural shocks in VAR models. This includes Impulse Response Functions (IRF), Forecast Error Variance Decomposition (FEVD), and Historical Decomposition (HD).","category":"section"},{"location":"innovation_accounting/#Quick-Start","page":"Innovation Accounting","title":"Quick Start","text":"irfs = irf(model, 20; method=:cholesky)                          # Frequentist IRF\nirfs_ci = irf(model, 20; ci_type=:bootstrap, reps=1000)          # With bootstrap CI\nbirfs = irf(chain, p, n, 20; method=:cholesky)                   # Bayesian IRF\ndecomp = fevd(model, 20)                                         # FEVD\nhd = historical_decomposition(model, 198)                        # Historical decomposition\nreport(irfs)                                                      # Publication-quality summary\n\n","category":"section"},{"location":"innovation_accounting/#Impulse-Response-Functions-(IRF)","page":"Innovation Accounting","title":"Impulse Response Functions (IRF)","text":"","category":"section"},{"location":"innovation_accounting/#Definition","page":"Innovation Accounting","title":"Definition","text":"The impulse response function Theta_h measures the effect of a one-unit structural shock at time t on the endogenous variables at time t+h:\n\nTheta_h = fracpartial y_t+hpartial varepsilon_t\n\nwhere\n\nTheta_h is the n times n impulse response matrix at horizon h\ny_t+h is the n times 1 vector of endogenous variables at time t+h\nvarepsilon_t is the n times 1 vector of structural shocks at time t\n\nFor a VAR, the IRF at horizon h is computed recursively:\n\nTheta_h = sum_i=1^min(hp) A_i Theta_h-i\n\nwhere\n\nA_i are the n times n VAR coefficient matrices for lag i\nTheta_0 = B_0 is the n times n structural impact matrix\np is the VAR lag order","category":"section"},{"location":"innovation_accounting/#Companion-Form-Representation","page":"Innovation Accounting","title":"Companion Form Representation","text":"Using the companion form, IRFs can be computed as:\n\nTheta_h = J F^h J B_0\n\nwhere\n\nJ = I_n 0 ldots 0 is the n times np selection matrix\nF is the np times np companion matrix\nB_0 is the n times n structural impact matrix","category":"section"},{"location":"innovation_accounting/#Cumulative-IRF","page":"Innovation Accounting","title":"Cumulative IRF","text":"The cumulative impulse response up to horizon H is:\n\nTheta^cum_H = sum_h=0^H Theta_h\n\nwhere Theta^cum_H accumulates the impulse responses from impact through horizon H, measuring the total cumulative effect of a structural shock. This is particularly relevant for variables in growth rates, where the cumulative IRF represents the effect on the level.","category":"section"},{"location":"innovation_accounting/#Confidence-Intervals","page":"Innovation Accounting","title":"Confidence Intervals","text":"Bootstrap (Frequentist): Residual bootstrap of Kilian (1998):\n\nEstimate the VAR and save residuals hatu_t\nGenerate bootstrap sample by resampling residuals with replacement\nRe-estimate the VAR and compute IRFs\nRepeat B times to build the distribution\n\nCredible Intervals (Bayesian): For each MCMC draw, compute IRFs and report posterior quantiles (e.g., 16th and 84th percentiles for 68% intervals).","category":"section"},{"location":"innovation_accounting/#Usage","page":"Innovation Accounting","title":"Usage","text":"using MacroEconometricModels\n\nY = randn(200, 3)\nmodel = estimate_var(Y, 2)\n\n# Basic IRF (Cholesky identification)\nirf_result = irf(model, 20)\n\n# With bootstrap confidence intervals\nirf_ci = irf(model, 20; ci_type=:bootstrap, reps=1000)\n\n# Sign restrictions\nsign_constraints = [1 1 0; -1 0 0; 0 0 1]\nirf_sign = irf(model, 20; method=:sign, sign_restrictions=sign_constraints)\n\nThe basic irf(model, 20) call uses Cholesky identification by default. Adding ci_type=:bootstrap generates pointwise confidence bands via Kilian's (1998) residual bootstrap — reps=1000 draws are recommended for publication-quality bands. Sign restrictions produce a set of admissible IRFs satisfying the constraints; the returned values are the median (or a representative draw), with the set-identified nature reflected in wider credible bands.\n\nnote: Technical Note\nThe ci_lower and ci_upper arrays are only populated when ci_type=:bootstrap (frequentist) or when using the Bayesian irf(chain, ...) method. With ci_type=:none (the default), these arrays contain zeros. Always check irf_result.ci_type before interpreting confidence bands.","category":"section"},{"location":"innovation_accounting/#ImpulseResponse-Return-Values","page":"Innovation Accounting","title":"ImpulseResponse Return Values","text":"Field Type Description\nvalues Array{T,3} (H+1) times n times n IRF array: values[h+1, i, j] = response of variable i to shock j at horizon h\nci_lower Array{T,3} Lower confidence bound (same shape as values)\nci_upper Array{T,3} Upper confidence bound\nhorizon Int Maximum IRF horizon H\nvariables Vector{String} Variable names\nshocks Vector{String} Shock names\nci_type Symbol CI method used (:bootstrap, :none, etc.)","category":"section"},{"location":"innovation_accounting/#BayesianImpulseResponse-Return-Values","page":"Innovation Accounting","title":"BayesianImpulseResponse Return Values","text":"Field Type Description\nquantiles Array{T,4} (H+1) times n times n times 3: dimension 4 = [16th pctl, median, 84th pctl]\nmean Array{T,3} (H+1) times n times n posterior mean IRF\nhorizon Int Maximum IRF horizon\nvariables Vector{String} Variable names\nshocks Vector{String} Shock names\nquantile_levels Vector{T} Quantile levels (e.g., [0.16, 0.5, 0.84])\n\nReference: Kilian (1998), Lütkepohl (2005, Chapter 3)\n\n","category":"section"},{"location":"innovation_accounting/#Forecast-Error-Variance-Decomposition-(FEVD)","page":"Innovation Accounting","title":"Forecast Error Variance Decomposition (FEVD)","text":"","category":"section"},{"location":"innovation_accounting/#Definition-2","page":"Innovation Accounting","title":"Definition","text":"The FEVD measures the proportion of the h-step ahead forecast error variance of variable i attributable to structural shock j:\n\ntextFEVD_ij(h) = fracsum_s=0^h-1 (Theta_s)_ij^2sum_s=0^h-1 sum_k=1^n (Theta_s)_ik^2\n\nwhere\n\ntextFEVD_ij(h) is the share of variable i's h-step forecast error variance due to shock j\n(Theta_s)_ij is the (ij) element of the impulse response matrix at horizon s\nThe numerator sums the squared contributions of shock j through horizon h-1\nThe denominator sums contributions from all n shocks, ensuring sum_j textFEVD_ij(h) = 1","category":"section"},{"location":"innovation_accounting/#Properties","page":"Innovation Accounting","title":"Properties","text":"0 leq textFEVD_ij(h) leq 1 for all i j h\nsum_j=1^n textFEVD_ij(h) = 1 for all i h\nAs h to infty, FEVD converges to the unconditional variance decomposition","category":"section"},{"location":"innovation_accounting/#Usage-2","page":"Innovation Accounting","title":"Usage","text":"# Basic FEVD\nfevd_result = fevd(model, 20)\n\n# With bootstrap CI\nfevd_ci = fevd(model, 20; ci_type=:bootstrap, reps=500)\n\n# Access decomposition for variable 1\nfevd_var1 = fevd_result.decomposition[:, 1, :]  # horizons × shocks\n\nThe proportions array satisfies sum_j textproportionsh i j = 1 for all horizons h and variables i. At short horizons, own shocks typically dominate (large diagonal entries). As h to infty, the FEVD converges to the unconditional variance decomposition, revealing which shocks are the dominant long-run drivers of each variable's fluctuations. Adding ci_type=:bootstrap produces bootstrap CIs that quantify estimation uncertainty in the FEVD shares.","category":"section"},{"location":"innovation_accounting/#FEVD-Return-Values","page":"Innovation Accounting","title":"FEVD Return Values","text":"Field Type Description\ndecomposition Array{T,3} H times n times n raw variance contributions\nproportions Array{T,3} H times n times n proportion of FEV: proportions[h, i, j] = share of variable i's FEV due to shock j at horizon h","category":"section"},{"location":"innovation_accounting/#BayesianFEVD-Return-Values","page":"Innovation Accounting","title":"BayesianFEVD Return Values","text":"Field Type Description\nquantiles Array{T,4} H times n times n times 3: dimension 4 = [16th pctl, median, 84th pctl]\nmean Array{T,3} H times n times n posterior mean FEVD proportions\nhorizon Int Maximum horizon\nvariables Vector{String} Variable names\nshocks Vector{String} Shock names\nquantile_levels Vector{T} Quantile levels\n\nReference: Lütkepohl (2005, Section 2.3.3)\n\n","category":"section"},{"location":"innovation_accounting/#Historical-Decomposition-(HD)","page":"Innovation Accounting","title":"Historical Decomposition (HD)","text":"","category":"section"},{"location":"innovation_accounting/#Definition-3","page":"Innovation Accounting","title":"Definition","text":"Historical decomposition decomposes observed variable movements into contributions from individual structural shocks over time:\n\ny_t = sum_s=0^t-1 Theta_s varepsilon_t-s + textinitial conditions\n\nwhere\n\ny_t is the n times 1 vector of observed variables at time t\nTheta_s = Phi_s P are the n times n structural MA coefficients at lag s\nPhi_s are the reduced-form MA coefficients (from the VMA representation)\nP = L Q is the n times n impact matrix (Cholesky factor L times rotation Q)\nvarepsilon_t = Q L^-1 u_t are the n times 1 structural shocks\nThe initial conditions capture the contribution of pre-sample values","category":"section"},{"location":"innovation_accounting/#Contribution-of-Shock-j-to-Variable-i-at-Time-t","page":"Innovation Accounting","title":"Contribution of Shock j to Variable i at Time t","text":"textHD_ij(t) = sum_s=0^t-1 (Theta_s)_ij  varepsilon_j(t-s)\n\nwhere\n\ntextHD_ij(t) is the contribution of shock j to variable i at time t\n(Theta_s)_ij is the (ij) element of the structural MA coefficient at lag s\nvarepsilon_j(t-s) is the realized structural shock j at time t-s\n\nThe decomposition satisfies the identity:\n\ny_t = sum_j=1^n textHD_ij(t) + textinitial_i(t)","category":"section"},{"location":"innovation_accounting/#Usage-3","page":"Innovation Accounting","title":"Usage","text":"# Basic historical decomposition\nhd = historical_decomposition(model, 198)\n\n# Verify decomposition identity\nverify_decomposition(hd)  # returns true if identity holds\n\n# Get contribution of shock 1 to variable 2\ncontrib = contribution(hd, 2, 1)\n\n# Total shock contribution (excluding initial conditions)\ntotal = total_shock_contribution(hd, 1)\n\n# With different identification\nhd_sign = historical_decomposition(model, 198; method=:sign,\n    sign_restrictions=sign_constraints)\n\nThe contributions[t, i, j] array gives the contribution of shock j to variable i at time t. Summing across shocks plus the initial conditions recovers the actual data: verify_decomposition(hd) checks this identity holds to numerical precision. The total_shock_contribution(hd, i) function sums all shock contributions for variable i, providing the \"shock-driven\" component of the series with initial conditions removed.","category":"section"},{"location":"innovation_accounting/#HistoricalDecomposition-Return-Values","page":"Innovation Accounting","title":"HistoricalDecomposition Return Values","text":"Field Type Description\ncontributions Array{T,3} T_eff times n times n shock contributions: contributions[t, i, j] = contribution of shock j to variable i at time t\ninitial_conditions Matrix{T} T_eff times n initial condition component\nactual Matrix{T} T_eff times n actual data values\nshocks Matrix{T} T_eff times n structural shocks\nT_eff Int Effective number of time periods\nvariables Vector{String} Variable names\nshock_names Vector{String} Shock names\nmethod Symbol Identification method (:cholesky, :sign, etc.)","category":"section"},{"location":"innovation_accounting/#BayesianHistoricalDecomposition-Return-Values","page":"Innovation Accounting","title":"BayesianHistoricalDecomposition Return Values","text":"Field Type Description\nquantiles Array{T,4} T_eff times n times n times n_q contribution quantiles\nmean Array{T,3} T_eff times n times n mean contributions\ninitial_quantiles Array{T,3} T_eff times n times n_q initial condition quantiles\ninitial_mean Matrix{T} T_eff times n mean initial conditions\nshocks_mean Matrix{T} T_eff times n mean structural shocks\nactual Matrix{T} T_eff times n actual data values\nT_eff Int Effective number of time periods\nvariables Vector{String} Variable names\nshock_names Vector{String} Shock names\nquantile_levels Vector{T} Quantile levels\nmethod Symbol Identification method\n\nReference: Kilian & Lütkepohl (2017, Chapter 4)\n\n","category":"section"},{"location":"innovation_accounting/#LP-Based-Innovation-Accounting","page":"Innovation Accounting","title":"LP-Based Innovation Accounting","text":"Structural Local Projections provide the same innovation accounting tools (IRF, FEVD, HD) as standard VAR, but via LP estimation. This offers robustness to VAR dynamic misspecification at the cost of some efficiency. For full theoretical background, see Local Projections.","category":"section"},{"location":"innovation_accounting/#IRF-from-Structural-LP","page":"Innovation Accounting","title":"IRF from Structural LP","text":"The irf() function dispatches on StructuralLP to return the pre-computed 3D impulse response:\n\nslp = structural_lp(Y, 20; method=:cholesky, lags=4)\nirf_result = irf(slp)   # Returns ImpulseResponse from the StructuralLP\n\n# Access: irf_result.values[h, i, j] = response of var i to shock j at horizon h\nprintln(\"Impact of shock 1 on var 2: \", irf_result.values[1, 2, 1])\n\nThe LP-based IRFs are numerically close to VAR-based IRFs under correct specification (Plagborg-Møller & Wolf 2021), but the LP standard errors stored in slp.se are wider because each horizon is estimated independently without imposing cross-horizon restrictions.","category":"section"},{"location":"innovation_accounting/#FEVD-from-Structural-LP","page":"Innovation Accounting","title":"FEVD from Structural LP","text":"The fevd() method for StructuralLP dispatches to the R²-based LP-FEVD of Gorodnichenko & Lee (2019):\n\ndecomp = fevd(slp, 20)  # Returns LPFEVD\n\n# Bias-corrected shares\nprintln(\"Var 1 explained by Shock 1 at h=8: \",\n        round(decomp.bias_corrected[1, 1, 8] * 100, digits=1), \"%\")\n\nUnlike VMA-based FEVD, LP-FEVD estimates variance shares directly via R² regressions, so they do not depend on the invertibility of the VAR lag polynomial. See LP-Based FEVD for the three estimator variants (R², LP-A, LP-B) and bias correction details.","category":"section"},{"location":"innovation_accounting/#Historical-Decomposition-from-Structural-LP","page":"Innovation Accounting","title":"Historical Decomposition from Structural LP","text":"hd = historical_decomposition(slp)\nverify_decomposition(hd)  # Check additive identity\n\nThe LP-based historical decomposition uses the structural shocks recovered from the VAR identification step (hatvarepsilon_t = QL^-1hatu_t) combined with LP-estimated IRF coefficients to decompose observed variable movements into shock contributions.","category":"section"},{"location":"innovation_accounting/#Cumulative-IRF-2","page":"Innovation Accounting","title":"Cumulative IRF","text":"For variables measured in growth rates (e.g., log-differenced GDP), the cumulative IRF shows the effect on the level:\n\nlp_model = estimate_lp(Y, 1, 20; lags=4)\nlp_irfs = lp_irf(lp_model)\ncum_irfs = cumulative_irf(lp_irfs)\n\nThe cumulative_irf function sums the pointwise IRF from horizon 0 through h, propagating standard errors via the delta method. This is especially useful for comparing LP and VAR results in levels versus differences.\n\n","category":"section"},{"location":"innovation_accounting/#Summary-Tables","page":"Innovation Accounting","title":"Summary Tables","text":"The package provides publication-quality summary tables using a unified interface with multiple dispatch.","category":"section"},{"location":"innovation_accounting/#Functions","page":"Innovation Accounting","title":"Functions","text":"Function Description\nreport(obj) Print comprehensive summary to stdout\ntable(obj, ...) Extract results as a matrix\nprint_table(io, obj, ...) Print formatted table to IO stream","category":"section"},{"location":"innovation_accounting/#Usage-Examples","page":"Innovation Accounting","title":"Usage Examples","text":"using MacroEconometricModels\n\nY = randn(200, 3)\nmodel = estimate_var(Y, 2)\nirf_result = irf(model, 20)\nfevd_result = fevd(model, 20)\nhd_result = historical_decomposition(model, 198)\n\n# Print summaries\nreport(model)\nreport(irf_result)\nreport(fevd_result)\nreport(hd_result)\n\n# Extract as DataFrames for further analysis\ndf_irf = table(irf_result, 1, 1)                    # response of var 1 to shock 1\ndf_irf_sel = table(irf_result, 1, 1; horizons=[1, 4, 8, 12, 20])\n\ndf_fevd = table(fevd_result, 1)                     # FEVD for variable 1\ndf_fevd_sel = table(fevd_result, 1; horizons=[1, 4, 8, 12])\n\ndf_hd = table(hd_result, 1)                         # HD for variable 1\ndf_hd_sel = table(hd_result, 1; periods=180:198)    # specific periods\n\n# Print formatted tables to stdout or file\nprint_table(stdout, irf_result, 1, 1; horizons=[1, 4, 8, 12])\nprint_table(stdout, fevd_result, 1; horizons=[1, 4, 8, 12])\nprint_table(stdout, hd_result, 1; periods=190:198)\n\n# Write to file\nopen(\"results.txt\", \"w\") do io\n    print_table(io, irf_result, 1, 1)\n    print_table(io, fevd_result, 1)\nend","category":"section"},{"location":"innovation_accounting/#String-Indexing","page":"Innovation Accounting","title":"String Indexing","text":"Variables and shocks can be indexed by name:\n\n# If variable names are set\ndf = table(irf_result, \"GDP\", \"Monetary Shock\")\ndf = table(fevd_result, \"Inflation\")\ndf = table(hd_result, \"Output\")\n\n","category":"section"},{"location":"innovation_accounting/#Display-Backends","page":"Innovation Accounting","title":"Display Backends","text":"All show, print_table, and report methods route through a unified PrettyTables backend. Switching from terminal text to LaTeX or HTML output requires a single call:\n\n# Switch output format globally\nset_display_backend(:text)    # Terminal-friendly (default)\nset_display_backend(:latex)   # LaTeX \\begin{tabular} output\nset_display_backend(:html)    # HTML <table> output\n\n# Check current backend\nget_display_backend()         # :text\n\nThis applies to all innovation accounting results — IRF, FEVD, and HD tables:\n\n# IRF table in LaTeX for a paper\nset_display_backend(:latex)\nopen(\"tables/irf_table.tex\", \"w\") do io\n    print_table(io, irfs, 1, 1; horizons=[1, 4, 8, 12, 20])\nend\n\n# FEVD table in HTML for slides\nset_display_backend(:html)\nopen(\"slides/fevd.html\", \"w\") do io\n    print_table(io, fevd_result, 1; horizons=[1, 4, 8, 12, 20])\nend\n\n# Reset to text for interactive work\nset_display_backend(:text)\n\nFor a comprehensive display backend workflow, see Example 12: Table Output.\n\n","category":"section"},{"location":"innovation_accounting/#Bibliographic-References","page":"Innovation Accounting","title":"Bibliographic References","text":"The refs() function returns bibliographic references for any model or identification method. This integrates with innovation accounting results:\n\n# References for a VAR model\nrefs(model)                           # AEA text format (default)\nrefs(model; format=:bibtex)           # BibTeX format\nrefs(model; format=:latex)            # LaTeX \\bibitem format\n\n# References by identification method\nrefs(:cholesky)                       # Cholesky decomposition references\nrefs(:fastica)                        # FastICA references\nrefs(:sign)                           # Sign restriction references\n\n# Write BibTeX to file\nopen(\"references.bib\", \"w\") do io\n    refs(io, model; format=:bibtex)\nend\n\nFor a comprehensive references workflow, see Example 13: Bibliographic References.\n\nnote: Technical Note\nFor non-Gaussian identification methods (ICA, ML, heteroskedasticity-based), see Non-Gaussian Structural Identification. All 18 identification methods work seamlessly with irf(), fevd(), and historical_decomposition() via the method keyword.\n\n","category":"section"},{"location":"innovation_accounting/#Complete-Example","page":"Innovation Accounting","title":"Complete Example","text":"This example combines IRF, FEVD, and HD for a three-variable VAR.\n\nusing MacroEconometricModels\nusing Random\n\nRandom.seed!(42)\n\n# Simulate a 3-variable VAR(2)\nT, n, p = 200, 3, 2\nY = randn(T, n)\nfor t in 2:T\n    Y[t, :] = 0.5 * Y[t-1, :] + 0.3 * randn(n)\nend\n\nmodel = estimate_var(Y, p)\n\n# IRF with bootstrap confidence intervals\nH = 20\nirfs = irf(model, H; method=:cholesky, ci_type=:bootstrap, reps=500)\nprintln(\"Shock 1 → Var 1 at h=0: \", round(irfs.values[1, 1, 1], digits=3))\nprintln(\"Shock 1 → Var 1 at h=8: \", round(irfs.values[9, 1, 1], digits=3))\n\n# FEVD\ndecomp = fevd(model, H)\nprintln(\"\\nFEVD for Var 1 at h=1: shock shares = \",\n        round.(decomp.proportions[1, 1, :] .* 100, digits=1), \"%\")\nprintln(\"FEVD for Var 1 at h=20: shock shares = \",\n        round.(decomp.proportions[20, 1, :] .* 100, digits=1), \"%\")\n\n# Historical decomposition\nhd = historical_decomposition(model, size(model.U, 1))\nprintln(\"\\nDecomposition identity holds: \", verify_decomposition(hd))\n\n# Summary tables\ndf_irf = table(irfs, 1, 1; horizons=[0, 4, 8, 12, 20])\ndf_fevd = table(decomp, 1; horizons=[1, 4, 8, 20])\n\nThe IRF values show the dynamic propagation of structural shocks through the system. At impact (h=0), the Cholesky identification imposes a lower-triangular structure, so shock 1 affects only the first variable contemporaneously. By h=8, cross-variable transmission is visible. The FEVD reveals whether the first variable's forecast uncertainty is dominated by its own shocks or by spillovers from other variables. At short horizons own shocks typically dominate; as h to infty, the FEVD converges to the unconditional variance decomposition. The HD passes the verification check, confirming the additive identity y_t = sum_j textHD_j(t) + textinitial(t) holds to numerical precision.\n\n","category":"section"},{"location":"innovation_accounting/#References","page":"Innovation Accounting","title":"References","text":"Kilian, Lutz. 1998. \"Small-Sample Confidence Intervals for Impulse Response Functions.\" Review of Economics and Statistics 80 (2): 218–230. https://doi.org/10.1162/003465398557465\nKilian, Lutz, and Helmut Lütkepohl. 2017. Structural Vector Autoregressive Analysis. Cambridge: Cambridge University Press. https://doi.org/10.1017/9781108164818\nLütkepohl, Helmut. 2005. New Introduction to Multiple Time Series Analysis. Berlin: Springer. ISBN 978-3-540-40172-8.","category":"section"},{"location":"api_types/#api_types","page":"Types","title":"API Types","text":"This page documents all core types in MacroEconometricModels.jl.","category":"section"},{"location":"api_types/#Module","page":"Types","title":"Module","text":"","category":"section"},{"location":"api_types/#ARIMA-Models","page":"Types","title":"ARIMA Models","text":"","category":"section"},{"location":"api_types/#VAR-Models","page":"Types","title":"VAR Models","text":"","category":"section"},{"location":"api_types/#Impulse-Response-and-FEVD","page":"Types","title":"Impulse Response and FEVD","text":"","category":"section"},{"location":"api_types/#Historical-Decomposition","page":"Types","title":"Historical Decomposition","text":"","category":"section"},{"location":"api_types/#Factor-Models","page":"Types","title":"Factor Models","text":"","category":"section"},{"location":"api_types/#Local-Projections","page":"Types","title":"Local Projections","text":"","category":"section"},{"location":"api_types/#GMM-Types","page":"Types","title":"GMM Types","text":"","category":"section"},{"location":"api_types/#Prior-Types","page":"Types","title":"Prior Types","text":"","category":"section"},{"location":"api_types/#Covariance-Estimators","page":"Types","title":"Covariance Estimators","text":"","category":"section"},{"location":"api_types/#Unit-Root-Test-Types","page":"Types","title":"Unit Root Test Types","text":"","category":"section"},{"location":"api_types/#SVAR-Identification-Types","page":"Types","title":"SVAR Identification Types","text":"","category":"section"},{"location":"api_types/#Volatility-Models","page":"Types","title":"Volatility Models","text":"","category":"section"},{"location":"api_types/#Non-Gaussian-SVAR-Types","page":"Types","title":"Non-Gaussian SVAR Types","text":"","category":"section"},{"location":"api_types/#Type-Hierarchy","page":"Types","title":"Type Hierarchy","text":"AbstractARIMAModel <: StatsAPI.RegressionModel\n├── ARModel{T}\n├── MAModel{T}\n├── ARMAModel{T}\n└── ARIMAModel{T}\n\nAbstractVARModel\n└── VARModel{T}\n\nAbstractImpulseResponse\n├── ImpulseResponse{T}\n├── BayesianImpulseResponse{T}\n└── AbstractLPImpulseResponse\n    └── LPImpulseResponse{T}\n\nAbstractFEVD\n├── FEVD{T}\n├── BayesianFEVD{T}\n└── LPFEVD{T}\n\nAbstractHistoricalDecomposition\n├── HistoricalDecomposition{T}\n└── BayesianHistoricalDecomposition{T}\n\nAbstractFactorModel\n├── FactorModel{T}\n├── DynamicFactorModel{T}\n└── GeneralizedDynamicFactorModel{T}\n\nFactorForecast{T}\n\nAbstractLPModel\n├── LPModel{T}\n├── LPIVModel{T}\n├── SmoothLPModel{T}\n├── StateLPModel{T}\n└── PropensityLPModel{T}\n\nStructuralLP{T}\nLPForecast{T}\n\nAbstractCovarianceEstimator\n├── NeweyWestEstimator{T}\n├── WhiteEstimator\n└── DriscollKraayEstimator{T}\n\nAbstractGMMModel\n└── GMMModel{T}\n\nAbstractPrior\n└── MinnesotaHyperparameters{T}\n\nAbstractUnitRootTest <: StatsAPI.HypothesisTest\n├── ADFResult{T}\n├── KPSSResult{T}\n├── PPResult{T}\n├── ZAResult{T}\n├── NgPerronResult{T}\n└── JohansenResult{T}\n\nVARStationarityResult{T}\n\nAbstractNormalityTest <: StatsAPI.HypothesisTest\n└── NormalityTestResult{T}\n\nNormalityTestSuite{T}\n\nAbstractNonGaussianSVAR\n├── ICASVARResult{T}\n├── NonGaussianMLResult{T}\n├── MarkovSwitchingSVARResult{T}\n├── GARCHSVARResult{T}\n├── SmoothTransitionSVARResult{T}\n└── ExternalVolatilitySVARResult{T}\n\nIdentifiabilityTestResult{T}\n\nAbstractVolatilityModel <: StatsAPI.RegressionModel\n├── ARCHModel{T}\n├── GARCHModel{T}\n├── EGARCHModel{T}\n├── GJRGARCHModel{T}\n└── SVModel{T}\n\nVolatilityForecast{T}","category":"section"},{"location":"api_types/#MacroEconometricModels.MacroEconometricModels","page":"Types","title":"MacroEconometricModels.MacroEconometricModels","text":"MacroEconometricModels\n\nA Julia package for macroeconomic time series analysis, providing tools for:\n\nVector Autoregression (VAR) estimation\nBayesian VAR (BVAR) with Minnesota priors\nStructural identification (Cholesky, sign restrictions, narrative, long-run)\nImpulse Response Functions (IRF)\nForecast Error Variance Decomposition (FEVD)\nFactor models via Principal Component Analysis\nLocal Projections (LP) with various extensions:\nHAC standard errors (Jordà 2005)\nInstrumental Variables (Stock & Watson 2018)\nSmooth IRF via B-splines (Barnichon & Brownlees 2019)\nState-dependent LP (Auerbach & Gorodnichenko 2013)\nPropensity Score Matching (Angrist et al. 2018)\nARIMA/ARMA model estimation, forecasting, and order selection\nGeneralized Method of Moments (GMM) estimation\n\nQuick Start\n\nusing MacroEconometricModels\n\n# Estimate a VAR model\nY = randn(100, 3)\nmodel = estimate_var(Y, 2)\n\n# Compute IRFs with bootstrap confidence intervals\nirf_result = irf(model, 20; ci_type=:bootstrap)\n\n# Local Projection IRFs with HAC standard errors\nlp_result = estimate_lp(Y, 1, 20; cov_type=:newey_west)\nlp_irf_result = lp_irf(lp_result)\n\n# Bayesian estimation\nchain = estimate_bvar(Y, 2; prior=:minnesota)\n\nReferences\n\nBańbura, M., Giannone, D., & Reichlin, L. (2010). Large Bayesian vector auto regressions.\nLütkepohl, H. (2005). New Introduction to Multiple Time Series Analysis.\nRubio-Ramírez, J. F., Waggoner, D. F., & Zha, T. (2010). Structural vector autoregressions.\nJordà, Ò. (2005). Estimation and Inference of Impulse Responses by Local Projections.\nStock, J. H., & Watson, M. W. (2018). Identification and Estimation of Dynamic Causal Effects.\nBarnichon, R., & Brownlees, C. (2019). Impulse Response Estimation by Smooth Local Projections.\nAuerbach, A. J., & Gorodnichenko, Y. (2013). Fiscal Multipliers in Recession and Expansion.\nAngrist, J. D., Jordà, Ò., & Kuersteiner, G. M. (2018). Semiparametric Estimates of Monetary Policy Effects.\n\n\n\n\n\n","category":"module"},{"location":"api_types/#MacroEconometricModels.AbstractARIMAModel","page":"Types","title":"MacroEconometricModels.AbstractARIMAModel","text":"AbstractARIMAModel <: StatsAPI.RegressionModel\n\nAbstract supertype for all univariate ARIMA-class models.\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.ARModel","page":"Types","title":"MacroEconometricModels.ARModel","text":"ARModel{T} <: AbstractARIMAModel\n\nAutoregressive AR(p) model: yₜ = c + φ₁yₜ₋₁ + ... + φₚyₜ₋ₚ + εₜ\n\nFields\n\ny::Vector{T}: Original data\np::Int: AR order\nc::T: Intercept\nphi::Vector{T}: AR coefficients [φ₁, ..., φₚ]\nsigma2::T: Innovation variance\nresiduals::Vector{T}: Estimated residuals\nfitted::Vector{T}: Fitted values\nloglik::T: Log-likelihood\naic::T: Akaike Information Criterion\nbic::T: Bayesian Information Criterion\nmethod::Symbol: Estimation method (:ols, :mle)\nconverged::Bool: Whether optimization converged\niterations::Int: Number of iterations (0 for OLS)\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.MAModel","page":"Types","title":"MacroEconometricModels.MAModel","text":"MAModel{T} <: AbstractARIMAModel\n\nMoving average MA(q) model: yₜ = c + εₜ + θ₁εₜ₋₁ + ... + θqεₜ₋q\n\nFields\n\ny::Vector{T}: Original data\nq::Int: MA order\nc::T: Intercept\ntheta::Vector{T}: MA coefficients [θ₁, ..., θq]\nsigma2::T: Innovation variance\nresiduals::Vector{T}: Estimated residuals\nfitted::Vector{T}: Fitted values\nloglik::T: Log-likelihood\naic::T: Akaike Information Criterion\nbic::T: Bayesian Information Criterion\nmethod::Symbol: Estimation method (:css, :mle, :css_mle)\nconverged::Bool: Whether optimization converged\niterations::Int: Number of iterations\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.ARMAModel","page":"Types","title":"MacroEconometricModels.ARMAModel","text":"ARMAModel{T} <: AbstractARIMAModel\n\nAutoregressive moving average ARMA(p,q) model: yₜ = c + φ₁yₜ₋₁ + ... + φₚyₜ₋ₚ + εₜ + θ₁εₜ₋₁ + ... + θqεₜ₋q\n\nFields\n\ny::Vector{T}: Original data\np::Int: AR order\nq::Int: MA order\nc::T: Intercept\nphi::Vector{T}: AR coefficients [φ₁, ..., φₚ]\ntheta::Vector{T}: MA coefficients [θ₁, ..., θq]\nsigma2::T: Innovation variance\nresiduals::Vector{T}: Estimated residuals\nfitted::Vector{T}: Fitted values\nloglik::T: Log-likelihood\naic::T: Akaike Information Criterion\nbic::T: Bayesian Information Criterion\nmethod::Symbol: Estimation method (:css, :mle, :css_mle)\nconverged::Bool: Whether optimization converged\niterations::Int: Number of iterations\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.ARIMAModel","page":"Types","title":"MacroEconometricModels.ARIMAModel","text":"ARIMAModel{T} <: AbstractARIMAModel\n\nAutoregressive integrated moving average ARIMA(p,d,q) model. The model is fit to the d-times differenced series as ARMA(p,q).\n\nFields\n\ny::Vector{T}: Original (undifferenced) data\ny_diff::Vector{T}: Differenced series\np::Int: AR order\nd::Int: Integration order (number of differences)\nq::Int: MA order\nc::T: Intercept (on differenced series)\nphi::Vector{T}: AR coefficients\ntheta::Vector{T}: MA coefficients\nsigma2::T: Innovation variance\nresiduals::Vector{T}: Estimated residuals\nfitted::Vector{T}: Fitted values (on differenced series)\nloglik::T: Log-likelihood\naic::T: Akaike Information Criterion\nbic::T: Bayesian Information Criterion\nmethod::Symbol: Estimation method\nconverged::Bool: Whether optimization converged\niterations::Int: Number of iterations\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.ARIMAForecast","page":"Types","title":"MacroEconometricModels.ARIMAForecast","text":"ARIMAForecast{T}\n\nForecast result from an ARIMA-class model.\n\nFields\n\nforecast::Vector{T}: Point forecasts\nci_lower::Vector{T}: Lower confidence interval bound\nci_upper::Vector{T}: Upper confidence interval bound\nse::Vector{T}: Standard errors of forecasts\nhorizon::Int: Forecast horizon\nconf_level::T: Confidence level (e.g., 0.95)\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.ARIMAOrderSelection","page":"Types","title":"MacroEconometricModels.ARIMAOrderSelection","text":"ARIMAOrderSelection{T}\n\nResult from automatic ARIMA order selection.\n\nFields\n\nbest_p_aic::Int: Best AR order by AIC\nbest_q_aic::Int: Best MA order by AIC\nbest_p_bic::Int: Best AR order by BIC\nbest_q_bic::Int: Best MA order by BIC\naic_matrix::Matrix{T}: AIC values for all (p,q) combinations\nbic_matrix::Matrix{T}: BIC values for all (p,q) combinations\nbest_model_aic::AbstractARIMAModel: Fitted model with best AIC\nbest_model_bic::AbstractARIMAModel: Fitted model with best BIC\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.VARModel","page":"Types","title":"MacroEconometricModels.VARModel","text":"VARModel{T} <: AbstractVARModel\n\nVAR model estimated via OLS.\n\nFields: Y (data), p (lags), B (coefficients), U (residuals), Sigma (covariance), aic, bic, hqic.\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.AbstractVARModel","page":"Types","title":"MacroEconometricModels.AbstractVARModel","text":"Abstract supertype for Vector Autoregression models.\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.ImpulseResponse","page":"Types","title":"MacroEconometricModels.ImpulseResponse","text":"ImpulseResponse{T} <: AbstractImpulseResponse\n\nIRF results with optional confidence intervals.\n\nFields: values (H×n×n), cilower, ciupper, horizon, variables, shocks, ci_type.\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.BayesianImpulseResponse","page":"Types","title":"MacroEconometricModels.BayesianImpulseResponse","text":"BayesianImpulseResponse{T} <: AbstractImpulseResponse\n\nBayesian IRF with posterior quantiles.\n\nFields: quantiles (H×n×n×q), mean (H×n×n), horizon, variables, shocks, quantile_levels.\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.AbstractImpulseResponse","page":"Types","title":"MacroEconometricModels.AbstractImpulseResponse","text":"Abstract supertype for impulse response function results.\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.FEVD","page":"Types","title":"MacroEconometricModels.FEVD","text":"FEVD results: decomposition (n×n×H) and proportions.\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.BayesianFEVD","page":"Types","title":"MacroEconometricModels.BayesianFEVD","text":"Bayesian FEVD with posterior quantiles.\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.AbstractFEVD","page":"Types","title":"MacroEconometricModels.AbstractFEVD","text":"Abstract supertype for forecast error variance decomposition results.\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.HistoricalDecomposition","page":"Types","title":"MacroEconometricModels.HistoricalDecomposition","text":"HistoricalDecomposition{T} <: AbstractHistoricalDecomposition\n\nFrequentist historical decomposition result.\n\nFields:\n\ncontributions: Shock contributions (Teff × nvars × n_shocks)\ninitial_conditions: Initial condition component (Teff × nvars)\nactual: Actual data values (Teff × nvars)\nshocks: Structural shocks (Teff × nshocks)\nT_eff: Effective number of time periods\nvariables: Variable names\nshock_names: Shock names\nmethod: Identification method used\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.BayesianHistoricalDecomposition","page":"Types","title":"MacroEconometricModels.BayesianHistoricalDecomposition","text":"BayesianHistoricalDecomposition{T} <: AbstractHistoricalDecomposition\n\nBayesian historical decomposition with posterior quantiles.\n\nFields:\n\nquantiles: Contribution quantiles (Teff × nvars × nshocks × nquantiles)\nmean: Mean contributions (Teff × nvars × n_shocks)\ninitial_quantiles: Initial condition quantiles (Teff × nvars × n_quantiles)\ninitial_mean: Mean initial conditions (Teff × nvars)\nshocks_mean: Mean structural shocks (Teff × nshocks)\nactual: Actual data values (Teff × nvars)\nT_eff: Effective number of time periods\nvariables: Variable names\nshock_names: Shock names\nquantile_levels: Quantile levels (e.g., [0.16, 0.5, 0.84])\nmethod: Identification method used\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.AbstractHistoricalDecomposition","page":"Types","title":"MacroEconometricModels.AbstractHistoricalDecomposition","text":"Abstract supertype for historical decomposition results.\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.FactorModel","page":"Types","title":"MacroEconometricModels.FactorModel","text":"FactorModel{T} <: AbstractFactorModel\n\nStatic factor model via PCA: Xₜ = Λ Fₜ + eₜ.\n\nFields: X, factors, loadings, eigenvalues, explainedvariance, cumulativevariance, r, standardized.\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.DynamicFactorModel","page":"Types","title":"MacroEconometricModels.DynamicFactorModel","text":"DynamicFactorModel{T} <: AbstractFactorModel\n\nDynamic factor model: Xₜ = Λ Fₜ + eₜ, Fₜ = Σᵢ Aᵢ Fₜ₋ᵢ + ηₜ.\n\nFields: X, factors, loadings, A (VAR coefficients), factorresiduals, Sigmaeta, Sigmae, eigenvalues, explainedvariance, cumulative_variance, r, p, method, standardized, converged, iterations, loglik.\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.GeneralizedDynamicFactorModel","page":"Types","title":"MacroEconometricModels.GeneralizedDynamicFactorModel","text":"GeneralizedDynamicFactorModel{T} <: AbstractFactorModel\n\nGDFM with frequency-dependent loadings: Xₜ = χₜ + ξₜ.\n\nFields: X, factors, commoncomponent, idiosyncratic, loadingsspectral, spectraldensityX, spectraldensitychi, eigenvaluesspectral, frequencies, q (dynamic factors), r (static factors), bandwidth, kernel, standardized, varianceexplained.\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.FactorForecast","page":"Types","title":"MacroEconometricModels.FactorForecast","text":"FactorForecast{T<:AbstractFloat}\n\nResult of factor model forecasting with optional confidence intervals.\n\nFields: factors, observables, factorslower, factorsupper, observableslower, observablesupper, factorsse, observablesse, horizon, conflevel, cimethod.\n\nWhen ci_method == :none, CI and SE fields are zero matrices.\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.AbstractFactorModel","page":"Types","title":"MacroEconometricModels.AbstractFactorModel","text":"Abstract supertype for factor models (static and dynamic).\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.AbstractLPImpulseResponse","page":"Types","title":"MacroEconometricModels.AbstractLPImpulseResponse","text":"Abstract supertype for LP impulse response results.\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.AbstractLPModel","page":"Types","title":"MacroEconometricModels.AbstractLPModel","text":"Abstract supertype for Local Projection models.\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.BSplineBasis","page":"Types","title":"MacroEconometricModels.BSplineBasis","text":"BSplineBasis{T} <: Any\n\nB-spline basis for smooth LP (Barnichon & Brownlees 2019).\n\nFields:\n\ndegree: Spline degree (typically 3 for cubic)\nninteriorknots: Number of interior knots\nknots: Full knot vector including boundary knots\nbasismatrix: Precomputed basis matrix at horizon points (H+1 × nbasis)\nhorizons: Horizon points where basis is evaluated\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.LPFEVD","page":"Types","title":"MacroEconometricModels.LPFEVD","text":"LPFEVD{T} <: AbstractFEVD\n\nLP-based Forecast Error Variance Decomposition (Gorodnichenko & Lee 2019).\n\nUses R²-based estimator: regress estimated LP forecast errors on identified structural shocks to measure the share of forecast error variance attributable to each shock. Includes VAR-based bootstrap bias correction and CIs.\n\nFields\n\nproportions: Raw FEVD estimates (n × n × H), [i,j,h] = share of variable i's h-step forecast error variance due to shock j\nbias_corrected: Bias-corrected FEVD (n × n × H)\nse: Bootstrap standard errors (n × n × H)\nci_lower: Lower CI bounds (n × n × H)\nci_upper: Upper CI bounds (n × n × H)\nmethod: Estimator (:r2, :lpa, :lpb)\nhorizon: Maximum FEVD horizon\nn_boot: Number of bootstrap replications used\nconf_level: Confidence level for CIs\nbias_correction: Whether bias correction was applied\n\nReference\n\nGorodnichenko, Y. & Lee, B. (2019). \"Forecast Error Variance Decompositions with Local Projections.\" JBES, 38(4), 921–933.\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.LPForecast","page":"Types","title":"MacroEconometricModels.LPForecast","text":"LPForecast{T}\n\nDirect multi-step LP forecast result.\n\nEach horizon h uses its own regression coefficients directly (no recursion), producing ŷ{T+h} = αh + βh·shockh + Γh·controlsT.\n\nFields:\n\nforecasts: Point forecasts (H × n_response)\nci_lower: Lower CI bounds (H × n_response)\nci_upper: Upper CI bounds (H × n_response)\nse: Standard errors (H × n_response)\nhorizon: Maximum forecast horizon\nresponse_vars: Response variable indices\nshock_var: Shock variable index\nshock_path: Assumed shock trajectory\nconf_level: Confidence level\nci_method: CI method (:analytical, :bootstrap, :none)\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.LPIVModel","page":"Types","title":"MacroEconometricModels.LPIVModel","text":"LPIVModel{T} <: AbstractLPModel\n\nLocal Projection with Instrumental Variables (Stock & Watson 2018). Uses 2SLS estimation at each horizon.\n\nFields:\n\nY: Response data matrix\nshock_var: Index of endogenous shock variable\nresponse_vars: Indices of response variables\ninstruments: Instrument matrix (T × n_instruments)\nhorizon: Maximum horizon\nlags: Number of control lags\nB: 2SLS coefficient matrices per horizon\nresiduals: Residuals per horizon\nvcov: Robust covariance matrices per horizon\nfirststageF: First-stage F-statistics per horizon (for weak IV test)\nfirststagecoef: First-stage coefficients per horizon\nT_eff: Effective sample sizes\ncov_estimator: Covariance estimator used\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.LPImpulseResponse","page":"Types","title":"MacroEconometricModels.LPImpulseResponse","text":"LPImpulseResponse{T} <: AbstractLPImpulseResponse\n\nLP-based impulse response function with confidence intervals from robust standard errors.\n\nFields:\n\nvalues: Point estimates (H+1 × n_response)\nci_lower: Lower CI bounds\nci_upper: Upper CI bounds\nse: Standard errors\nhorizon: Maximum horizon\nresponse_vars: Names of response variables\nshock_var: Name of shock variable\ncov_type: Covariance estimator type\nconf_level: Confidence level used\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.LPModel","page":"Types","title":"MacroEconometricModels.LPModel","text":"LPModel{T} <: AbstractLPModel\n\nLocal Projection model estimated via OLS with robust standard errors (Jordà 2005).\n\nThe LP regression for horizon h:     y{t+h} = αh + βh * shockt + Γh * controlst + ε_{t+h}\n\nFields:\n\nY: Response data matrix (Tobs × nvars)\nshock_var: Index of shock variable in Y\nresponse_vars: Indices of response variables (default: all)\nhorizon: Maximum IRF horizon H\nlags: Number of control lags included\nB: Vector of coefficient matrices, one per horizon h=0,...,H\nresiduals: Vector of residual matrices per horizon\nvcov: Vector of robust covariance matrices per horizon\nT_eff: Effective sample sizes per horizon\ncov_estimator: Covariance estimator used\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.PropensityLPModel","page":"Types","title":"MacroEconometricModels.PropensityLPModel","text":"PropensityLPModel{T} <: AbstractLPModel\n\nLocal Projection with Inverse Propensity Weighting (Angrist et al. 2018).\n\nEstimates Average Treatment Effect (ATE) at each horizon using IPW.\n\nFields:\n\nY: Response data matrix\ntreatment: Binary treatment indicator vector\nresponse_vars: Response variable indices\ncovariates: Covariate matrix for propensity model\nhorizon: Maximum horizon\npropensity_scores: Estimated propensity scores P(D=1|X)\nipw_weights: Inverse propensity weights\nB: IPW-weighted regression coefficients per horizon\nresiduals: Residuals per horizon\nvcov: Robust covariance matrices per horizon\nate: Average treatment effects per horizon (for each response var)\nate_se: Standard errors of ATE\nconfig: Propensity score configuration\nT_eff: Effective sample sizes\ncov_estimator: Covariance estimator used\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.PropensityScoreConfig","page":"Types","title":"MacroEconometricModels.PropensityScoreConfig","text":"PropensityScoreConfig{T} <: Any\n\nConfiguration for propensity score estimation and IPW.\n\nFields:\n\nmethod: Propensity model (:logit, :probit)\ntrimming: (lower, upper) bounds for propensity scores\nnormalize: Normalize weights to sum to 1 within groups\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.SmoothLPModel","page":"Types","title":"MacroEconometricModels.SmoothLPModel","text":"SmoothLPModel{T} <: AbstractLPModel\n\nSmooth Local Projection with B-spline basis (Barnichon & Brownlees 2019).\n\nThe IRF is parameterized as: β(h) = Σj θj Bj(h) where Bj are B-spline basis functions.\n\nFields:\n\nY: Response data matrix\nshock_var: Shock variable index\nresponse_vars: Response variable indices\nhorizon: Maximum horizon\nlags: Number of control lags\nspline_basis: B-spline basis configuration\ntheta: Spline coefficients (nbasis × nresponse)\nvcov_theta: Covariance of theta (vectorized)\nlambda: Smoothing penalty parameter\nirfvalues: Smoothed IRF point estimates (H+1 × nresponse)\nirf_se: Standard errors of smoothed IRF\nresiduals: Pooled residuals\nT_eff: Effective sample size\ncov_estimator: Covariance estimator used\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.StateLPModel","page":"Types","title":"MacroEconometricModels.StateLPModel","text":"StateLPModel{T} <: AbstractLPModel\n\nState-dependent Local Projection (Auerbach & Gorodnichenko 2013).\n\nModel: y{t+h} = F(zt)[αE + βE * shockt + ...] + (1-F(zt))[αR + βR * shock_t + ...]\n\nF(z) is a smooth transition function, typically logistic. State E = expansion (high z), State R = recession (low z).\n\nFields:\n\nY: Response data matrix\nshock_var: Shock variable index\nresponse_vars: Response variable indices\nhorizon: Maximum horizon\nlags: Number of control lags\nstate: StateTransition configuration\nB_expansion: Coefficients in expansion state (per horizon)\nB_recession: Coefficients in recession state (per horizon)\nresiduals: Residuals per horizon\nvcov_expansion: Covariance in expansion (per horizon)\nvcov_recession: Covariance in recession (per horizon)\nvcov_diff: Covariance of difference (per horizon)\nT_eff: Effective sample sizes\ncov_estimator: Covariance estimator used\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.StateTransition","page":"Types","title":"MacroEconometricModels.StateTransition","text":"StateTransition{T} <: Any\n\nSmooth state transition function for state-dependent LP.\n\nF(zt) = exp(-γ(zt - c)) / (1 + exp(-γ(z_t - c)))\n\nFields:\n\nstate_var: State variable values (standardized)\ngamma: Transition smoothness parameter (higher = sharper)\nthreshold: Transition threshold c\nmethod: Transition function type (:logistic, :exponential, :indicator)\nF_values: Precomputed transition function values\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.StructuralLP","page":"Types","title":"MacroEconometricModels.StructuralLP","text":"StructuralLP{T} <: AbstractFrequentistResult\n\nStructural Local Projection result combining VAR-based identification with LP estimation.\n\nEstimates multi-shock IRFs by computing orthogonalized structural shocks from a VAR model and using them as regressors in LP regressions (Plagborg-Møller & Wolf 2021).\n\nFields:\n\nirf: 3D impulse responses (H × n × n) — reuses ImpulseResponse{T}\nstructural_shocks: Structural shocks (T_eff × n)\nvar_model: Underlying VAR model used for identification\nQ: Rotation/identification matrix\nmethod: Identification method used (:cholesky, :sign, :long_run, :fastica, etc.)\nlags: Number of LP control lags\ncov_type: HAC estimator type\nse: Standard errors (H × n × n)\nlp_models: Individual LP model per shock\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.AbstractGMMModel","page":"Types","title":"MacroEconometricModels.AbstractGMMModel","text":"Abstract supertype for GMM models.\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.GMMModel","page":"Types","title":"MacroEconometricModels.GMMModel","text":"GMMModel{T} <: AbstractGMMModel\n\nGeneralized Method of Moments estimator.\n\nMinimizes: g(θ)'W g(θ) where g(θ) = (1/n) Σᵢ gᵢ(θ)\n\nFields:\n\ntheta: Parameter estimates\nvcov: Asymptotic covariance matrix\nn_moments: Number of moment conditions\nn_params: Number of parameters\nn_obs: Number of observations\nweighting: Weighting specification\nW: Final weighting matrix\ng_bar: Sample moment vector at solution\nJ_stat: Hansen's J-test statistic\nJ_pvalue: p-value for J-test\nconverged: Convergence flag\niterations: Number of iterations\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.GMMWeighting","page":"Types","title":"MacroEconometricModels.GMMWeighting","text":"GMMWeighting{T} <: Any\n\nGMM weighting matrix specification.\n\nFields:\n\nmethod: Weighting method (:identity, :optimal, :two_step, :iterated)\nmax_iter: Maximum iterations for iterated GMM\ntol: Convergence tolerance\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.MinnesotaHyperparameters","page":"Types","title":"MacroEconometricModels.MinnesotaHyperparameters","text":"MinnesotaHyperparameters{T} <: AbstractPrior\n\nMinnesota prior hyperparameters: tau (tightness), decay, lambda (sum-of-coef), mu (co-persistence), omega (covariance).\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.AbstractPrior","page":"Types","title":"MacroEconometricModels.AbstractPrior","text":"Abstract supertype for Bayesian prior specifications.\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.AbstractCovarianceEstimator","page":"Types","title":"MacroEconometricModels.AbstractCovarianceEstimator","text":"Abstract supertype for covariance estimators.\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.NeweyWestEstimator","page":"Types","title":"MacroEconometricModels.NeweyWestEstimator","text":"NeweyWestEstimator{T} <: AbstractCovarianceEstimator\n\nNewey-West HAC covariance estimator configuration.\n\nFields:\n\nbandwidth: Truncation lag (0 = automatic via Newey-West 1994 formula)\nkernel: Kernel function (:bartlett, :parzen, :quadraticspectral, :tukeyhanning)\nprewhiten: Use AR(1) prewhitening\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.WhiteEstimator","page":"Types","title":"MacroEconometricModels.WhiteEstimator","text":"WhiteEstimator <: AbstractCovarianceEstimator\n\nWhite heteroscedasticity-robust covariance estimator (HC0). Does not correct for serial correlation.\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.DriscollKraayEstimator","page":"Types","title":"MacroEconometricModels.DriscollKraayEstimator","text":"DriscollKraayEstimator{T} <: AbstractCovarianceEstimator\n\nDriscoll-Kraay standard errors for panel data with cross-sectional dependence.\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.AbstractUnitRootTest","page":"Types","title":"MacroEconometricModels.AbstractUnitRootTest","text":"Abstract supertype for all unit root test results.\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.ADFResult","page":"Types","title":"MacroEconometricModels.ADFResult","text":"ADFResult{T} <: AbstractUnitRootTest\n\nAugmented Dickey-Fuller test result.\n\nFields:\n\nstatistic: ADF test statistic (t-ratio on γ)\npvalue: Approximate p-value (MacKinnon 1994, 2010)\nlags: Number of augmenting lags used\nregression: Regression specification (:none, :constant, :trend)\ncritical_values: Critical values at 1%, 5%, 10% levels\nnobs: Effective number of observations\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.KPSSResult","page":"Types","title":"MacroEconometricModels.KPSSResult","text":"KPSSResult{T} <: AbstractUnitRootTest\n\nKPSS stationarity test result.\n\nFields:\n\nstatistic: KPSS test statistic\npvalue: Approximate p-value\nregression: Regression specification (:constant, :trend)\ncritical_values: Critical values at 1%, 5%, 10% levels\nbandwidth: Bartlett kernel bandwidth used\nnobs: Number of observations\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.PPResult","page":"Types","title":"MacroEconometricModels.PPResult","text":"PPResult{T} <: AbstractUnitRootTest\n\nPhillips-Perron test result.\n\nFields:\n\nstatistic: PP test statistic (Zt or Zα)\npvalue: Approximate p-value\nregression: Regression specification (:none, :constant, :trend)\ncritical_values: Critical values at 1%, 5%, 10% levels\nbandwidth: Newey-West bandwidth used\nnobs: Effective number of observations\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.ZAResult","page":"Types","title":"MacroEconometricModels.ZAResult","text":"ZAResult{T} <: AbstractUnitRootTest\n\nZivot-Andrews structural break unit root test result.\n\nFields:\n\nstatistic: Minimum t-statistic across all break points\npvalue: Approximate p-value\nbreak_index: Index of estimated structural break\nbreak_fraction: Break point as fraction of sample\nregression: Break specification (:constant, :trend, :both)\ncritical_values: Critical values at 1%, 5%, 10% levels\nlags: Number of augmenting lags\nnobs: Effective number of observations\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.NgPerronResult","page":"Types","title":"MacroEconometricModels.NgPerronResult","text":"NgPerronResult{T} <: AbstractUnitRootTest\n\nNg-Perron unit root test result (MZα, MZt, MSB, MPT).\n\nFields:\n\nMZa: Modified Zα statistic\nMZt: Modified Zt statistic\nMSB: Modified Sargan-Bhargava statistic\nMPT: Modified Point-optimal statistic\nregression: Regression specification (:constant, :trend)\ncritical_values: Dict mapping statistic name to critical values\nnobs: Effective number of observations\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.JohansenResult","page":"Types","title":"MacroEconometricModels.JohansenResult","text":"JohansenResult{T} <: AbstractUnitRootTest\n\nJohansen cointegration test result.\n\nFields:\n\ntrace_stats: Trace test statistics for each rank\ntrace_pvalues: P-values for trace tests\nmax_eigen_stats: Maximum eigenvalue test statistics\nmax_eigen_pvalues: P-values for max eigenvalue tests\nrank: Estimated cointegration rank (at 5% level)\neigenvectors: Cointegrating vectors (β), columns are vectors\nadjustment: Adjustment coefficients (α)\neigenvalues: Eigenvalues from reduced rank regression\ncritical_values_trace: Critical values for trace test (rows: ranks, cols: 10%, 5%, 1%)\ncritical_values_max: Critical values for max eigenvalue test\ndeterministic: Deterministic specification\nlags: Number of lags in VECM\nnobs: Effective number of observations\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.VARStationarityResult","page":"Types","title":"MacroEconometricModels.VARStationarityResult","text":"VARStationarityResult{T}\n\nVAR model stationarity check result.\n\nFields:\n\nis_stationary: true if all eigenvalues have modulus < 1\neigenvalues: Eigenvalues of companion matrix (may be real or complex)\nmax_modulus: Maximum eigenvalue modulus\ncompanion_matrix: The companion form matrix F\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.ZeroRestriction","page":"Types","title":"MacroEconometricModels.ZeroRestriction","text":"Zero restriction: variable doesn't respond to shock at horizon.\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.SignRestriction","page":"Types","title":"MacroEconometricModels.SignRestriction","text":"Sign restriction: variable response to shock has required sign at horizon.\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.SVARRestrictions","page":"Types","title":"MacroEconometricModels.SVARRestrictions","text":"Container for SVAR restrictions.\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.AriasSVARResult","page":"Types","title":"MacroEconometricModels.AriasSVARResult","text":"Result from Arias et al. (2018) identification.\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.AbstractVolatilityModel","page":"Types","title":"MacroEconometricModels.AbstractVolatilityModel","text":"Abstract supertype for univariate volatility models (ARCH/GARCH/SV).\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.ARCHModel","page":"Types","title":"MacroEconometricModels.ARCHModel","text":"ARCHModel{T} <: AbstractVolatilityModel\n\nARCH(q) model (Engle 1982): εₜ = σₜ zₜ, σ²ₜ = ω + α₁ε²ₜ₋₁ + ... + αqε²ₜ₋q\n\nFields\n\ny::Vector{T}: Original data\nq::Int: ARCH order\nmu::T: Mean (intercept)\nomega::T: Variance intercept (ω > 0)\nalpha::Vector{T}: ARCH coefficients [α₁, ..., αq]\nconditional_variance::Vector{T}: Estimated conditional variances σ²ₜ\nstandardized_residuals::Vector{T}: Standardized residuals zₜ = εₜ/σₜ\nresiduals::Vector{T}: Raw residuals εₜ = yₜ - μ\nfitted::Vector{T}: Fitted values (mean)\nloglik::T: Log-likelihood\naic::T: Akaike Information Criterion\nbic::T: Bayesian Information Criterion\nmethod::Symbol: Estimation method\nconverged::Bool: Whether optimization converged\niterations::Int: Number of iterations\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.GARCHModel","page":"Types","title":"MacroEconometricModels.GARCHModel","text":"GARCHModel{T} <: AbstractVolatilityModel\n\nGARCH(p,q) model (Bollerslev 1986): σ²ₜ = ω + α₁ε²ₜ₋₁ + ... + αqε²ₜ₋q + β₁σ²ₜ₋₁ + ... + βpσ²ₜ₋p\n\nFields\n\ny::Vector{T}: Original data\np::Int: GARCH order (lagged variances)\nq::Int: ARCH order (lagged squared residuals)\nmu::T: Mean (intercept)\nomega::T: Variance intercept (ω > 0)\nalpha::Vector{T}: ARCH coefficients [α₁, ..., αq]\nbeta::Vector{T}: GARCH coefficients [β₁, ..., βp]\nconditional_variance::Vector{T}: Estimated conditional variances σ²ₜ\nstandardized_residuals::Vector{T}: Standardized residuals zₜ = εₜ/σₜ\nresiduals::Vector{T}: Raw residuals εₜ = yₜ - μ\nfitted::Vector{T}: Fitted values (mean)\nloglik::T: Log-likelihood\naic::T: Akaike Information Criterion\nbic::T: Bayesian Information Criterion\nmethod::Symbol: Estimation method\nconverged::Bool: Whether optimization converged\niterations::Int: Number of iterations\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.EGARCHModel","page":"Types","title":"MacroEconometricModels.EGARCHModel","text":"EGARCHModel{T} <: AbstractVolatilityModel\n\nEGARCH(p,q) model (Nelson 1991): log(σ²ₜ) = ω + Σαᵢ(|zₜ₋ᵢ| - E|zₜ₋ᵢ|) + Σγᵢzₜ₋ᵢ + Σβⱼlog(σ²ₜ₋ⱼ)\n\nThe log specification ensures σ² > 0 without parameter constraints, and γᵢ captures leverage effects (typically γ < 0).\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.GJRGARCHModel","page":"Types","title":"MacroEconometricModels.GJRGARCHModel","text":"GJRGARCHModel{T} <: AbstractVolatilityModel\n\nGJR-GARCH(p,q) model (Glosten, Jagannathan & Runkle 1993): σ²ₜ = ω + Σ(αᵢ + γᵢI(εₜ₋ᵢ < 0))ε²ₜ₋ᵢ + Σβⱼσ²ₜ₋ⱼ\n\nγᵢ > 0 means negative shocks increase variance more than positive shocks.\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.SVModel","page":"Types","title":"MacroEconometricModels.SVModel","text":"SVModel{T} <: AbstractVolatilityModel\n\nStochastic Volatility model (Taylor 1986), estimated via Bayesian MCMC:     yₜ = exp(hₜ/2) εₜ,       εₜ ~ N(0,1)     hₜ = μ + φ(hₜ₋₁ - μ) + σ_η ηₜ,  ηₜ ~ N(0,1)\n\nFields\n\ny::Vector{T}: Original data\nchain::Chains: Full MCMC chain\nmu_post::Vector{T}: Posterior draws of μ (log-variance level)\nphi_post::Vector{T}: Posterior draws of φ (persistence)\nsigma_eta_post::Vector{T}: Posterior draws of σ_η (volatility of volatility)\nvolatility_mean::Vector{T}: Posterior mean of exp(hₜ) at each time t\nvolatility_quantiles::Matrix{T}: Quantiles of exp(hₜ) (T × n_quantiles)\nquantile_levels::Vector{T}: Quantile levels (e.g., [0.025, 0.5, 0.975])\ndist::Symbol: Error distribution (:normal or :studentt)\nleverage::Bool: Whether leverage effect was estimated\nn_samples::Int: Number of posterior samples\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.VolatilityForecast","page":"Types","title":"MacroEconometricModels.VolatilityForecast","text":"VolatilityForecast{T}\n\nForecast result from a volatility model.\n\nFields\n\nforecast::Vector{T}: Point forecasts of conditional variance\nci_lower::Vector{T}: Lower confidence interval bound\nci_upper::Vector{T}: Upper confidence interval bound\nse::Vector{T}: Standard errors of forecasts\nhorizon::Int: Forecast horizon\nconf_level::T: Confidence level (e.g., 0.95)\nmodel_type::Symbol: Source model type (:arch, :garch, :egarch, :gjr_garch, :sv)\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.AbstractNormalityTest","page":"Types","title":"MacroEconometricModels.AbstractNormalityTest","text":"Abstract supertype for multivariate normality test results.\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.AbstractNonGaussianSVAR","page":"Types","title":"MacroEconometricModels.AbstractNonGaussianSVAR","text":"Abstract supertype for non-Gaussian SVAR identification results.\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.NormalityTestResult","page":"Types","title":"MacroEconometricModels.NormalityTestResult","text":"NormalityTestResult{T} <: AbstractNormalityTest\n\nResult of a multivariate normality test.\n\nFields:\n\ntest_name::Symbol — :jarque_bera, :mardia_skewness, :mardia_kurtosis, :doornik_hansen, :henze_zirkler\nstatistic::T — test statistic\npvalue::T — p-value\ndf::Int — degrees of freedom (for chi-squared tests)\nn_vars::Int — number of variables\nn_obs::Int — number of observations\ncomponents::Union{Nothing, Vector{T}} — per-component statistics (for component-wise tests)\ncomponent_pvalues::Union{Nothing, Vector{T}} — per-component p-values\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.NormalityTestSuite","page":"Types","title":"MacroEconometricModels.NormalityTestSuite","text":"NormalityTestSuite{T}\n\nCollection of normality test results from normality_test_suite.\n\nFields:\n\nresults::Vector{NormalityTestResult{T}} — individual test results\nresiduals::Matrix{T} — the residual matrix tested\nn_vars::Int\nn_obs::Int\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.ICASVARResult","page":"Types","title":"MacroEconometricModels.ICASVARResult","text":"ICASVARResult{T} <: AbstractNonGaussianSVAR\n\nResult from ICA-based SVAR identification.\n\nFields:\n\nB0::Matrix{T} — structural impact matrix (n × n): ut = B₀ εt\nW::Matrix{T} — unmixing matrix (n × n): εt = W ut\nQ::Matrix{T} — rotation matrix for compute_Q integration\nshocks::Matrix{T} — recovered structural shocks (T_eff × n)\nmethod::Symbol — :fastica, :jade, :sobi, :dcov, :hsic\nconverged::Bool\niterations::Int\nobjective::T — final objective value\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.NonGaussianMLResult","page":"Types","title":"MacroEconometricModels.NonGaussianMLResult","text":"NonGaussianMLResult{T} <: AbstractNonGaussianSVAR\n\nResult from non-Gaussian maximum likelihood SVAR identification.\n\nFields:\n\nB0::Matrix{T} — structural impact matrix (n × n)\nQ::Matrix{T} — rotation matrix\nshocks::Matrix{T} — structural shocks (T_eff × n)\ndistribution::Symbol — :student_t, :mixture_normal, :pml, :skew_normal\nloglik::T — log-likelihood at MLE\nloglik_gaussian::T — Gaussian log-likelihood (for LR test)\ndist_params::Dict{Symbol, Any} — distribution parameters\nvcov::Matrix{T} — asymptotic covariance of B₀ elements\nse::Matrix{T} — standard errors for B₀\nconverged::Bool\niterations::Int\naic::T\nbic::T\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.MarkovSwitchingSVARResult","page":"Types","title":"MacroEconometricModels.MarkovSwitchingSVARResult","text":"MarkovSwitchingSVARResult{T} <: AbstractNonGaussianSVAR\n\nResult from Markov-switching heteroskedasticity SVAR identification.\n\nFields:\n\nB0::Matrix{T} — structural impact matrix\nQ::Matrix{T} — rotation matrix\nSigma_regimes::Vector{Matrix{T}} — covariance per regime\nLambda::Vector{Vector{T}} — relative variances per regime\nregime_probs::Matrix{T} — smoothed regime probabilities (T × K)\ntransition_matrix::Matrix{T} — Markov transition probabilities (K × K)\nloglik::T\nconverged::Bool\niterations::Int\nn_regimes::Int\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.GARCHSVARResult","page":"Types","title":"MacroEconometricModels.GARCHSVARResult","text":"GARCHSVARResult{T} <: AbstractNonGaussianSVAR\n\nResult from GARCH-based SVAR identification.\n\nFields:\n\nB0::Matrix{T} — structural impact matrix\nQ::Matrix{T} — rotation matrix\ngarch_params::Matrix{T} — (n × 3): [ω, α, β] per shock\ncond_var::Matrix{T} — (T_eff × n) conditional variances\nshocks::Matrix{T} — structural shocks\nloglik::T\nconverged::Bool\niterations::Int\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.SmoothTransitionSVARResult","page":"Types","title":"MacroEconometricModels.SmoothTransitionSVARResult","text":"SmoothTransitionSVARResult{T} <: AbstractNonGaussianSVAR\n\nResult from smooth-transition heteroskedasticity SVAR identification.\n\nFields:\n\nB0::Matrix{T} — structural impact matrix\nQ::Matrix{T} — rotation matrix\nSigma_regimes::Vector{Matrix{T}} — covariance matrices for extreme regimes\nLambda::Vector{Vector{T}} — relative variances per regime\ngamma::T — transition speed parameter\nthreshold::T — transition location parameter\ntransition_var::Vector{T} — transition variable values\nG_values::Vector{T} — transition function G(s_t) values\nloglik::T\nconverged::Bool\niterations::Int\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.ExternalVolatilitySVARResult","page":"Types","title":"MacroEconometricModels.ExternalVolatilitySVARResult","text":"ExternalVolatilitySVARResult{T} <: AbstractNonGaussianSVAR\n\nResult from external volatility instrument SVAR identification.\n\nFields:\n\nB0::Matrix{T} — structural impact matrix\nQ::Matrix{T} — rotation matrix\nSigma_regimes::Vector{Matrix{T}} — covariance per regime\nLambda::Vector{Vector{T}} — relative variances per regime\nregime_indices::Vector{Vector{Int}} — observation indices per regime\nloglik::T\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.IdentifiabilityTestResult","page":"Types","title":"MacroEconometricModels.IdentifiabilityTestResult","text":"IdentifiabilityTestResult{T}\n\nResult from an identifiability or specification test.\n\nFields:\n\ntest_name::Symbol — test identifier\nstatistic::T — test statistic\npvalue::T — p-value\nidentified::Bool — whether identification appears to hold\ndetails::Dict{Symbol, Any} — method-specific details\n\n\n\n\n\n","category":"type"},{"location":"nongaussian/#Non-Gaussian-Structural-Identification","page":"Non-Gaussian Structural Identification","title":"Non-Gaussian Structural Identification","text":"This page covers identification of structural VAR models using non-Gaussian distributional assumptions, heteroskedasticity, and ICA methods. These methods provide identification without requiring the recursive ordering of Cholesky or the a priori sign/zero restrictions of traditional SVAR.","category":"section"},{"location":"nongaussian/#Quick-Start","page":"Non-Gaussian Structural Identification","title":"Quick Start","text":"using MacroEconometricModels\n\n# Multivariate normality tests (diagnostics)\nsuite = normality_test_suite(model)                # Run all 7 tests\njb = jarque_bera_test(model)                       # Multivariate Jarque-Bera\n\n# ICA-based SVAR identification\nica = identify_fastica(model)                      # FastICA (Hyvärinen 1999)\njade = identify_jade(model)                        # JADE (Cardoso 1993)\n\n# Non-Gaussian ML identification\nml = identify_student_t(model)                     # Student-t shocks\nml = identify_nongaussian_ml(model; distribution=:mixture_normal)\n\n# Heteroskedasticity identification\nms = identify_markov_switching(model; n_regimes=2) # Markov-switching (Lanne & Lütkepohl 2008)\nev = identify_external_volatility(model, regime)   # Known volatility regimes (Rigobon 2003)\n\n# Identifiability tests\ntest_shock_gaussianity(ica)                        # Are shocks non-Gaussian?\ntest_gaussian_vs_nongaussian(model)                # LR test: Gaussian vs non-Gaussian\ntest_shock_independence(ica)                       # Are shocks independent?\n\n# Integration with existing IRF pipeline\nirfs = irf(model, 20; method=:fastica)             # Works automatically via compute_Q\n\n","category":"section"},{"location":"nongaussian/#Multivariate-Normality-Tests","page":"Non-Gaussian Structural Identification","title":"Multivariate Normality Tests","text":"Before applying non-Gaussian SVAR methods, it is essential to verify that the VAR residuals are indeed non-Gaussian. If residuals are Gaussian, non-Gaussian identification will not work (the problem is unidentified).","category":"section"},{"location":"nongaussian/#Multivariate-Jarque-Bera-Test","page":"Non-Gaussian Structural Identification","title":"Multivariate Jarque-Bera Test","text":"The multivariate Jarque-Bera test extends the univariate JB test to vector residuals. Under the null hypothesis of multivariate normality, the test statistic is:\n\nJB = T cdot fracb_1k6 + T cdot frac(b_2k - k(k+2))^224k\n\nwhere b_1k is the multivariate skewness measure and b_2k is the multivariate kurtosis measure (Lütkepohl 2005, §4.5).\n\nusing MacroEconometricModels, Random\nRandom.seed!(42)\nY = randn(300, 3)\nmodel = estimate_var(Y, 2)\n\n# Joint test\njb = jarque_bera_test(model)\nprintln(\"Statistic: $(round(jb.statistic, digits=4)), p-value: $(round(jb.pvalue, digits=4))\")\n\n# Component-wise test on standardized residuals\njb_comp = jarque_bera_test(model; method=:component)\nprintln(\"Component p-values: \", round.(jb_comp.component_pvalues, digits=4))\n\nWith Gaussian data, we expect p-values above 0.05 — failure to reject normality.","category":"section"},{"location":"nongaussian/#Mardia's-Tests","page":"Non-Gaussian Structural Identification","title":"Mardia's Tests","text":"Mardia (1970) proposed separate tests for multivariate skewness and kurtosis:\n\nb_1k = frac1T^2 sum_ij (u_i Sigma^-1 u_j)^3 quad text(skewness)\n\nb_2k = frac1T sum_i (u_i Sigma^-1 u_i)^2 quad text(kurtosis)\n\nUnder H₀: T cdot b_1k6 sim chi^2(k(k+1)(k+2)6) and (b_2k - k(k+2))  sqrt8k(k+2)T sim N(01).\n\nskew_test = mardia_test(model; type=:skewness)\nkurt_test = mardia_test(model; type=:kurtosis)\nboth_test = mardia_test(model; type=:both)\n\nThe :both option combines both tests into a single chi-squared statistic.\n\nReference: Mardia (1970)","category":"section"},{"location":"nongaussian/#Doornik-Hansen-Test","page":"Non-Gaussian Structural Identification","title":"Doornik-Hansen Test","text":"The Doornik-Hansen (2008) omnibus test applies the Bowman-Shenton transformation to each component's skewness and kurtosis, producing approximately standard normal transforms z_1 and z_2. The test statistic is:\n\nDH = sum_j=1^k (z_1j^2 + z_2j^2) sim chi^2(2k)\n\ndh = doornik_hansen_test(model)","category":"section"},{"location":"nongaussian/#Henze-Zirkler-Test","page":"Non-Gaussian Structural Identification","title":"Henze-Zirkler Test","text":"The Henze-Zirkler (1990) test is based on the empirical characteristic function and is consistent against all alternatives. The test statistic uses a smoothing parameter beta that depends on the sample size and dimension.\n\nhz = henze_zirkler_test(model)","category":"section"},{"location":"nongaussian/#Normality-Test-Suite","page":"Non-Gaussian Structural Identification","title":"Normality Test Suite","text":"Run all tests at once with normality_test_suite:\n\nsuite = normality_test_suite(model)\nprintln(suite)\n\nThis runs 7 tests: multivariate JB, component-wise JB, Mardia skewness, Mardia kurtosis, Mardia combined, Doornik-Hansen, and Henze-Zirkler.","category":"section"},{"location":"nongaussian/#Return-Values","page":"Non-Gaussian Structural Identification","title":"Return Values","text":"Field Type Description\ntest_name Symbol Test identifier\nstatistic T Test statistic value\npvalue T p-value\ndf Int Degrees of freedom\nn_vars Int Number of variables\nn_obs Int Number of observations\ncomponents Vector{T} or nothing Per-component statistics\ncomponent_pvalues Vector{T} or nothing Per-component p-values\n\n","category":"section"},{"location":"nongaussian/#ICA-based-SVAR-Identification","page":"Non-Gaussian Structural Identification","title":"ICA-based SVAR Identification","text":"Independent Component Analysis (ICA) identifies the structural impact matrix B_0 by finding the rotation Q that makes the recovered shocks varepsilon_t = (B_0)^-1 u_t maximally independent and non-Gaussian.","category":"section"},{"location":"nongaussian/#Model-Specification","page":"Non-Gaussian Structural Identification","title":"Model Specification","text":"The structural VAR has the decomposition:\n\nu_t = B_0 varepsilon_t quad Sigma = B_0 B_0\n\nwhere\n\nu_t is the n times 1 vector of reduced-form residuals\nvarepsilon_t is the n times 1 vector of structural shocks, assumed mutually independent and non-Gaussian\nB_0 = L Q where L = textchol(Sigma) and Q is orthogonal\n\nIdentification condition: At most one structural shock may be Gaussian (Lanne, Meitz & Saikkonen 2017). If all shocks are non-Gaussian, B_0 is unique up to column permutation and sign.","category":"section"},{"location":"nongaussian/#FastICA","page":"Non-Gaussian Structural Identification","title":"FastICA","text":"FastICA (Hyvärinen 1999) finds the unmixing matrix by maximizing a measure of non-Gaussianity (negentropy) via a fixed-point algorithm.\n\n# Default: logcosh contrast, deflation approach\nica = identify_fastica(model)\n\n# Symmetric approach with exponential contrast\nica = identify_fastica(model; approach=:symmetric, contrast=:exp)\n\nThree contrast functions are available:\n\n:logcosh (default) — robust, good general-purpose choice: G(u) = logcosh(u)\n:exp — better for super-Gaussian sources: G(u) = -exp(-u^22)\n:kurtosis — classical kurtosis-based: G(u) = u^44\n\nTwo extraction approaches:\n\n:deflation — extracts components one at a time (deflation approach)\n:symmetric — extracts all components simultaneously\n\nReference: Hyvärinen (1999)","category":"section"},{"location":"nongaussian/#JADE","page":"Non-Gaussian Structural Identification","title":"JADE","text":"JADE (Joint Approximate Diagonalization of Eigenmatrices) uses fourth-order cumulant matrices and joint diagonalization via Jacobi rotations.\n\njade = identify_jade(model)\n\nJADE computes the fourth-order cumulant matrices C_ijkl = textcum(z_k z_l z_i z_j) and finds the orthogonal matrix that simultaneously diagonalizes all of them.\n\nReference: Cardoso & Souloumiac (1993)","category":"section"},{"location":"nongaussian/#SOBI","page":"Non-Gaussian Structural Identification","title":"SOBI","text":"SOBI (Second-Order Blind Identification) exploits temporal structure via autocovariance matrices at multiple lags.\n\nsobi = identify_sobi(model; lags=1:12)\n\nUnlike FastICA and JADE which use higher-order statistics, SOBI only uses second-order statistics (autocovariances), making it suitable when temporal dependence is the main source of identifiability.\n\nReference: Belouchrani et al. (1997)","category":"section"},{"location":"nongaussian/#Distance-Covariance","page":"Non-Gaussian Structural Identification","title":"Distance Covariance","text":"Minimizes the sum of pairwise distance covariances between recovered shocks. Distance covariance (Székely et al. 2007) is zero if and only if variables are independent.\n\ndcov = identify_dcov(model)\n\nReference: Matteson & Tsay (2017)","category":"section"},{"location":"nongaussian/#HSIC","page":"Non-Gaussian Structural Identification","title":"HSIC","text":"Minimizes the Hilbert-Schmidt Independence Criterion using a Gaussian kernel. Like distance covariance, HSIC with a characteristic kernel is zero iff variables are independent.\n\nhsic = identify_hsic(model; sigma=1.0)\n\nThe bandwidth parameter sigma defaults to the median pairwise distance heuristic.\n\nReference: Gretton et al. (2005)","category":"section"},{"location":"nongaussian/#ICA-Result-Fields","page":"Non-Gaussian Structural Identification","title":"ICA Result Fields","text":"Field Type Description\nB0 Matrix{T} Structural impact matrix (n times n)\nW Matrix{T} Unmixing matrix: varepsilon_t = W u_t\nQ Matrix{T} Rotation matrix: B_0 = L Q\nshocks Matrix{T} Recovered structural shocks (T times n)\nmethod Symbol Method used\nconverged Bool Whether the algorithm converged\niterations Int Number of iterations\nobjective T Final objective value\n\n","category":"section"},{"location":"nongaussian/#Non-Gaussian-Maximum-Likelihood","page":"Non-Gaussian Structural Identification","title":"Non-Gaussian Maximum Likelihood","text":"Instead of the two-step ICA approach, ML methods estimate B_0 and the shock distribution parameters jointly by maximizing the log-likelihood.","category":"section"},{"location":"nongaussian/#Model-Specification-2","page":"Non-Gaussian Structural Identification","title":"Model Specification","text":"The log-likelihood under non-Gaussian shocks is:\n\nell(theta) = sum_t=1^T left logdet(B_0^-1) + sum_j=1^n log f_j(varepsilon_jt theta_j) right\n\nwhere\n\nvarepsilon_t = B_0^-1 u_t are the structural shocks\nf_j(cdot theta_j) is the marginal density of shock j\ntheta_j are distribution-specific parameters (e.g., degrees of freedom for Student-t)","category":"section"},{"location":"nongaussian/#Student-t-Shocks","page":"Non-Gaussian Structural Identification","title":"Student-t Shocks","text":"Assumes each shock follows a (standardized) Student-t distribution with shock-specific degrees of freedom nu_j:\n\nml = identify_student_t(model)\nprintln(\"Degrees of freedom: \", ml.dist_params[:nu])\n\nLow nu indicates heavy tails. When nu to infty, the shock approaches Gaussianity. Identification requires that at most one shock has nu = infty.\n\nReference: Lanne, Meitz & Saikkonen (2017)","category":"section"},{"location":"nongaussian/#Mixture-of-Normals","page":"Non-Gaussian Structural Identification","title":"Mixture of Normals","text":"Each shock follows a mixture of two normals: varepsilon_j sim p_j N(0 sigma_1j^2) + (1-p_j) N(0 sigma_2j^2) with the unit variance constraint p_j sigma_1j^2 + (1-p_j) sigma_2j^2 = 1.\n\nml = identify_mixture_normal(model)\nprintln(\"Mixing probabilities: \", ml.dist_params[:p_mix])\n\nReference: Lanne & Lütkepohl (2010)","category":"section"},{"location":"nongaussian/#Pseudo-Maximum-Likelihood-(PML)","page":"Non-Gaussian Structural Identification","title":"Pseudo Maximum Likelihood (PML)","text":"Uses Pearson Type IV distributions, allowing both skewness and excess kurtosis.\n\nml = identify_pml(model)\n\nReference: Herwartz (2018)","category":"section"},{"location":"nongaussian/#Skew-Normal-Shocks","page":"Non-Gaussian Structural Identification","title":"Skew-Normal Shocks","text":"Each shock follows a skew-normal distribution with pdf f(x) = 2phi(x)Phi(alpha_j x).\n\nml = identify_skew_normal(model)\nprintln(\"Skewness parameters: \", ml.dist_params[:alpha])\n\nReference: Azzalini (1985)","category":"section"},{"location":"nongaussian/#Unified-Dispatcher","page":"Non-Gaussian Structural Identification","title":"Unified Dispatcher","text":"Use identify_nongaussian_ml to select the distribution at runtime:\n\nfor dist in [:student_t, :mixture_normal, :pml, :skew_normal]\n    ml = identify_nongaussian_ml(model; distribution=dist)\n    println(\"$dist: logL=$(round(ml.loglik, digits=2)), AIC=$(round(ml.aic, digits=2))\")\nend\n\nCompare AIC/BIC across distributions to select the best-fitting specification.","category":"section"},{"location":"nongaussian/#ML-Result-Fields","page":"Non-Gaussian Structural Identification","title":"ML Result Fields","text":"Field Type Description\nB0 Matrix{T} Structural impact matrix\nQ Matrix{T} Rotation matrix\nshocks Matrix{T} Structural shocks\ndistribution Symbol Distribution used\nloglik T Log-likelihood at MLE\nloglik_gaussian T Gaussian log-likelihood (for LR test)\ndist_params Dict{Symbol,Any} Distribution parameters\nvcov Matrix{T} Asymptotic covariance of parameters\nse Matrix{T} Standard errors for B_0\nconverged Bool Convergence status\naic T Akaike information criterion\nbic T Bayesian information criterion\n\n","category":"section"},{"location":"nongaussian/#Heteroskedasticity-Based-Identification","page":"Non-Gaussian Structural Identification","title":"Heteroskedasticity-Based Identification","text":"These methods identify B_0 from changes in the error covariance across volatility regimes, without requiring non-Gaussianity.","category":"section"},{"location":"nongaussian/#Eigendecomposition-Identification","page":"Non-Gaussian Structural Identification","title":"Eigendecomposition Identification","text":"The core idea (Rigobon 2003): given two regime covariance matrices Sigma_1 and Sigma_2, the eigendecomposition of Sigma_1^-1Sigma_2 yields:\n\nSigma_1^-1Sigma_2 = V D V^-1\n\nwhere\n\nV contains the eigenvectors\nD = textdiag(lambda_1 ldots lambda_n) contains the relative variance ratios\nB_0 = Sigma_1^12 V (with normalization)\n\nIdentification condition: The eigenvalues lambda_j must be distinct.","category":"section"},{"location":"nongaussian/#Markov-Switching-Volatility","page":"Non-Gaussian Structural Identification","title":"Markov-Switching Volatility","text":"Estimates regime-specific covariance matrices via the Hamilton (1989) filter with EM algorithm:\n\nms = identify_markov_switching(model; n_regimes=2)\nprintln(\"Transition matrix:\")\nprintln(round.(ms.transition_matrix, digits=3))\nprintln(\"Regime probabilities (first 5 obs):\")\nprintln(round.(ms.regime_probs[1:5, :], digits=3))\n\nThe EM algorithm iterates:\n\nE-step: Hamilton filter (forward) + Kim smoother (backward) → regime probabilities\nM-step: Update regime covariances and transition matrix given probabilities\n\nReference: Lanne & Lütkepohl (2008)","category":"section"},{"location":"nongaussian/#GARCH-Based-Identification","page":"Non-Gaussian Structural Identification","title":"GARCH-Based Identification","text":"Uses GARCH(1,1) conditional heteroskedasticity in the structural shocks for identification:\n\nh_jt = omega_j + alpha_j varepsilon_jt-1^2 + beta_j h_jt-1\n\ngarch = identify_garch(model)\nprintln(\"GARCH parameters (ω, α, β):\")\nfor j in 1:size(garch.garch_params, 1)\n    println(\"  Shock $j: \", round.(garch.garch_params[j, :], digits=4))\nend\n\nReference: Normandin & Phaneuf (2004)","category":"section"},{"location":"nongaussian/#Smooth-Transition","page":"Non-Gaussian Structural Identification","title":"Smooth Transition","text":"The covariance varies smoothly between two regimes via a logistic transition function:\n\nSigma_t = B_0 I + G(s_t)(Lambda - I) B_0\n\nwhere G(s_t) = 1(1 + exp(-gamma(s_t - c))) is the logistic transition function.\n\n# Use a lagged variable as the transition variable\ns = Y[2:end, 1]  # first variable, lagged\nst = identify_smooth_transition(model, s)\nprintln(\"Transition speed γ = $(round(st.gamma, digits=3))\")\nprintln(\"Threshold c = $(round(st.threshold, digits=3))\")\n\nReference: Lütkepohl & Netšunajev (2017)","category":"section"},{"location":"nongaussian/#External-Volatility-Instruments","page":"Non-Gaussian Structural Identification","title":"External Volatility Instruments","text":"When volatility regimes are known a priori (e.g., NBER recession dates, financial crisis indicators):\n\n# Binary regime indicator\nregime = vcat(fill(1, 100), fill(2, 100))  # first half = regime 1\nev = identify_external_volatility(model, regime)\n\nThis is the simplest heteroskedasticity method — it just splits the sample and applies eigendecomposition identification.\n\nReference: Rigobon (2003)\n\n","category":"section"},{"location":"nongaussian/#Identifiability-and-Specification-Tests","page":"Non-Gaussian Structural Identification","title":"Identifiability and Specification Tests","text":"","category":"section"},{"location":"nongaussian/#Shock-Gaussianity-Test","page":"Non-Gaussian Structural Identification","title":"Shock Gaussianity Test","text":"Tests whether recovered structural shocks are non-Gaussian using univariate Jarque-Bera tests on each shock. Non-Gaussian identification requires at most one Gaussian shock.\n\nica = identify_fastica(model)\nresult = test_shock_gaussianity(ica)\nprintln(\"Number of Gaussian shocks: \", result.details[:n_gaussian])\nprintln(\"Identified: \", result.identified)","category":"section"},{"location":"nongaussian/#Gaussian-vs-Non-Gaussian-LR-Test","page":"Non-Gaussian Structural Identification","title":"Gaussian vs Non-Gaussian LR Test","text":"Likelihood ratio test: H_0: Gaussian shocks vs H_1: non-Gaussian shocks.\n\nLR = 2(ell_1 - ell_0) sim chi^2(p)\n\nwhere p is the number of extra distribution parameters.\n\nlr = test_gaussian_vs_nongaussian(model; distribution=:student_t)\nprintln(\"LR statistic: $(round(lr.statistic, digits=4))\")\nprintln(\"p-value: $(round(lr.pvalue, digits=4))\")\n\nRejecting H_0 supports the use of non-Gaussian identification.","category":"section"},{"location":"nongaussian/#Shock-Independence-Test","page":"Non-Gaussian Structural Identification","title":"Shock Independence Test","text":"Tests whether recovered shocks are mutually independent using both cross-correlation (portmanteau) and distance covariance tests, combined via Fisher's method.\n\nresult = test_shock_independence(ica; max_lag=10)\nprintln(\"Independent: \", result.identified)  # fail-to-reject = independent","category":"section"},{"location":"nongaussian/#Identification-Strength","page":"Non-Gaussian Structural Identification","title":"Identification Strength","text":"Bootstrap test of identification robustness: resamples residuals and measures the stability of the estimated B_0.\n\nresult = test_identification_strength(model; method=:fastica, n_bootstrap=499)\nprintln(\"Median Procrustes distance: $(round(result.statistic, digits=4))\")\n\nSmall distances indicate strong identification.","category":"section"},{"location":"nongaussian/#Overidentification-Test","page":"Non-Gaussian Structural Identification","title":"Overidentification Test","text":"Tests consistency of additional restrictions beyond non-Gaussianity.\n\nresult = test_overidentification(model, ica; n_bootstrap=499)\nprintln(\"p-value: $(round(result.pvalue, digits=4))\")\n\n","category":"section"},{"location":"nongaussian/#Integration-with-IRF-Pipeline","page":"Non-Gaussian Structural Identification","title":"Integration with IRF Pipeline","text":"All ICA and ML methods integrate seamlessly with the existing irf, fevd, and historical_decomposition functions via compute_Q:\n\n# Any non-Gaussian method works as an irf method\nirfs_ica = irf(model, 20; method=:fastica)\nirfs_ml  = irf(model, 20; method=:student_t)\nirfs_ms  = irf(model, 20; method=:markov_switching)\n\n# FEVD and HD also work\ndecomp = fevd(model, 20; method=:fastica)\n\nSupported method symbols: :fastica, :jade, :sobi, :dcov, :hsic, :student_t, :mixture_normal, :pml, :skew_normal, :markov_switching, :garch.\n\n","category":"section"},{"location":"nongaussian/#Complete-Example","page":"Non-Gaussian Structural Identification","title":"Complete Example","text":"using MacroEconometricModels, Random\nRandom.seed!(42)\n\n# Generate VAR data\nT_obs, n = 300, 3\nY = randn(T_obs, n)\nfor t in 3:T_obs\n    Y[t, :] = 0.5 * Y[t-1, :] + 0.2 * Y[t-2, :] + 0.8 * randn(n)\nend\nmodel = estimate_var(Y, 2)\n\n# Step 1: Test for non-Gaussianity\nsuite = normality_test_suite(model)\nprintln(suite)\n\n# Step 2: Try ICA identification\nica = identify_fastica(model)\nprintln(\"\\nFastICA result:\")\nprintln(\"  Converged: \", ica.converged)\nprintln(\"  Q orthogonal: \", round(norm(ica.Q' * ica.Q - I), digits=8))\n\n# Step 3: Verify identification\ngauss = test_shock_gaussianity(ica)\nprintln(\"\\nShock Gaussianity Test:\")\nprintln(\"  Number of Gaussian shocks: \", gauss.details[:n_gaussian])\nprintln(\"  JB p-values: \", round.(gauss.details[:jb_pvals], digits=4))\n\nindep = test_shock_independence(ica; max_lag=5)\nprintln(\"\\nShock Independence Test:\")\nprintln(\"  Independent: \", indep.identified)\nprintln(\"  Fisher p-value: \", round(indep.pvalue, digits=4))\n\n# Step 4: Compare with ML approach\nml = identify_student_t(model)\nprintln(\"\\nStudent-t ML:\")\nprintln(\"  ν = \", round.(ml.dist_params[:nu], digits=2))\nprintln(\"  AIC = $(round(ml.aic, digits=2)), BIC = $(round(ml.bic, digits=2))\")\n\nlr = test_gaussian_vs_nongaussian(model)\nprintln(\"\\nGaussian vs Non-Gaussian LR test:\")\nprintln(\"  LR = $(round(lr.statistic, digits=4)), p = $(round(lr.pvalue, digits=4))\")\n\n# Step 5: Compute IRFs\nirfs = irf(model, 20; method=:fastica)\nprintln(\"\\nIRF size: \", size(irfs.values))\n\n","category":"section"},{"location":"nongaussian/#References","page":"Non-Gaussian Structural Identification","title":"References","text":"","category":"section"},{"location":"nongaussian/#Multivariate-Normality-Tests-2","page":"Non-Gaussian Structural Identification","title":"Multivariate Normality Tests","text":"Jarque, Carlos M., and Anil K. Bera. 1980. \"Efficient Tests for Normality, Homoscedasticity and Serial Independence of Regression Residuals.\" Economics Letters 6 (3): 255–259. https://doi.org/10.1016/0165-1765(80)90024-5\nMardia, Kanti V. 1970. \"Measures of Multivariate Skewness and Kurtosis with Applications.\" Biometrika 57 (3): 519–530. https://doi.org/10.1093/biomet/57.3.519\nDoornik, Jurgen A., and Henrik Hansen. 2008. \"An Omnibus Test for Univariate and Multivariate Normality.\" Oxford Bulletin of Economics and Statistics 70: 927–939. https://doi.org/10.1111/j.1468-0084.2008.00537.x\nHenze, Norbert, and Bernhard Zirkler. 1990. \"A Class of Invariant Consistent Tests for Multivariate Normality.\" Communications in Statistics - Theory and Methods 19 (10): 3595–3617. https://doi.org/10.1080/03610929008830400\nLütkepohl, Helmut. 2005. New Introduction to Multiple Time Series Analysis. Berlin: Springer. ISBN 978-3-540-40172-8.","category":"section"},{"location":"nongaussian/#ICA-based-Identification","page":"Non-Gaussian Structural Identification","title":"ICA-based Identification","text":"Hyvärinen, Aapo. 1999. \"Fast and Robust Fixed-Point Algorithms for Independent Component Analysis.\" IEEE Transactions on Neural Networks 10 (3): 626–634. https://doi.org/10.1109/72.761722\nCardoso, Jean-François, and Antoine Souloumiac. 1993. \"Blind Beamforming for Non-Gaussian Signals.\" IEE Proceedings-F 140 (6): 362–370. https://doi.org/10.1049/ip-f-2.1993.0054\nBelouchrani, Adel, Karim Abed-Meraim, Jean-François Cardoso, and Eric Moulines. 1997. \"A Blind Source Separation Technique Using Second-Order Statistics.\" IEEE Transactions on Signal Processing 45 (2): 434–444. https://doi.org/10.1109/78.554307","category":"section"},{"location":"nongaussian/#Non-Gaussian-ML","page":"Non-Gaussian Structural Identification","title":"Non-Gaussian ML","text":"Lanne, Markku, Mika Meitz, and Pentti Saikkonen. 2017. \"Identification and Estimation of Non-Gaussian Structural Vector Autoregressions.\" Journal of Econometrics 196 (2): 288–304. https://doi.org/10.1016/j.jeconom.2016.06.002\nLanne, Markku, and Helmut Lütkepohl. 2010. \"Structural Vector Autoregressions with Nonnormal Residuals.\" Journal of Business & Economic Statistics 28 (1): 159–168. https://doi.org/10.1198/jbes.2009.06003\nHerwartz, Helmut. 2018. \"Hodges-Lehmann Detection of Structural Shocks: An Analysis of Macroeconomic Dynamics in the Euro Area.\" Oxford Bulletin of Economics and Statistics 80 (4): 736–754. https://doi.org/10.1111/obes.12237\nAzzalini, Adelchi. 1985. \"A Class of Distributions Which Includes the Normal Ones.\" Scandinavian Journal of Statistics 12 (2): 171–178. https://doi.org/10.1111/j.1467-9469.1985.tb01174.x","category":"section"},{"location":"nongaussian/#Heteroskedasticity-based-Identification","page":"Non-Gaussian Structural Identification","title":"Heteroskedasticity-based Identification","text":"Rigobon, Roberto. 2003. \"Identification through Heteroskedasticity.\" Review of Economics and Statistics 85 (4): 777–792. https://doi.org/10.1162/003465303772815727\nLanne, Markku, and Helmut Lütkepohl. 2008. \"Identifying Monetary Policy Shocks via Changes in Volatility.\" Journal of Money, Credit and Banking 40 (6): 1131–1149. https://doi.org/10.1111/j.1538-4616.2008.00151.x\nNormandin, Michel, and Louis Phaneuf. 2004. \"Monetary Policy Shocks: Testing Identification Conditions under Time-Varying Conditional Volatility.\" Journal of Monetary Economics 51 (6): 1217–1243. https://doi.org/10.1016/j.jmoneco.2003.11.002\nLütkepohl, Helmut, and Aleksei Netšunajev. 2017. \"Structural Vector Autoregressions with Smooth Transition in Variances.\" Journal of Economic Dynamics and Control 84: 43–57. https://doi.org/10.1016/j.jedc.2017.09.001","category":"section"},{"location":"nongaussian/#Independence-Measures","page":"Non-Gaussian Structural Identification","title":"Independence Measures","text":"Székely, Gábor J., Maria L. Rizzo, and Nail K. Bakirov. 2007. \"Measuring and Testing Dependence by Correlation of Distances.\" Annals of Statistics 35 (6): 2769–2794. https://doi.org/10.1214/009053607000000505\nGretton, Arthur, Olivier Bousquet, Alex Smola, and Bernhard Schölkopf. 2005. \"Measuring Statistical Dependence with Hilbert-Schmidt Norms.\" In Algorithmic Learning Theory, edited by Sanjay Jain, Hans Ulrich Simon, and Etsuji Tomita, 63–77. Berlin: Springer. https://doi.org/10.1007/11564089_7\nMatteson, David S., and Ruey S. Tsay. 2017. \"Independent Component Analysis via Distance Covariance.\" Journal of the American Statistical Association 112 (518): 623–637. https://doi.org/10.1080/01621459.2016.1150851","category":"section"},{"location":"examples/#Examples","page":"Examples","title":"Examples","text":"This chapter provides comprehensive worked examples demonstrating the main functionality of MacroEconometricModels.jl. Each example includes complete code, economic interpretation, and best practices. The examples follow the natural empirical workflow: test data properties, estimate models, try alternatives, and report results.","category":"section"},{"location":"examples/#Quick-Reference","page":"Examples","title":"Quick Reference","text":"# Example Key Functions Description\n1 ARIMA Models estimate_ar, estimate_arma, auto_arima, forecast AR, MA, ARMA estimation, order selection, forecasting\n2 Volatility Models estimate_garch, estimate_egarch, estimate_sv, news_impact_curve ARCH/GARCH/SV estimation, diagnostics, forecasting\n3 Three-Variable VAR estimate_var, irf, fevd Frequentist VAR with Cholesky and sign restriction identification\n4 Local Projections estimate_lp, estimate_lp_iv, estimate_smooth_lp Standard, IV, smooth, and state-dependent LP\n5 Factor Model for Large Panels estimate_factors, ic_criteria, forecast Large panel factor extraction, Bai-Ng criteria, forecasting with CIs\n6 Bayesian VAR with Minnesota Prior estimate_bvar, optimize_hyperparameters Minnesota prior, MCMC estimation, credible intervals\n7 Non-Gaussian Structural Identification identify_fastica, normality_test_suite, test_shock_gaussianity ICA, ML, heteroskedastic identification\n8 Unit Root Testing adf_test, kpss_test, johansen_test ADF, KPSS, Zivot-Andrews, Ng-Perron, Johansen\n9 GMM Estimation estimate_gmm, j_test IV regression via GMM, overidentification test\n10 Complete Workflow Multiple Unit roots → lag selection → VAR → BVAR → LP comparison\n11 Table Output (LaTeX & HTML) set_display_backend, print_table, table Export tables for papers, slides, and web\n12 Bibliographic References refs Multi-format references for models and methods\n\n","category":"section"},{"location":"examples/#Example-1:-ARIMA-Models","page":"Examples","title":"Example 1: ARIMA Models","text":"This example demonstrates univariate time series modeling with ARIMA models: estimation, order selection, diagnostics, and forecasting.\n\nusing MacroEconometricModels\nusing Random\nusing Statistics\n\nRandom.seed!(42)\n\n# Generate ARMA(1,1) data\nT = 300\ny = zeros(T)\ne = randn(T)\nfor t in 2:T\n    y[t] = 0.7 * y[t-1] + e[t] + 0.3 * e[t-1]\nend\n\n# === AR(2) via OLS ===\nar = estimate_ar(y, 2)\nprintln(\"AR(2) Estimation\")\nprintln(\"  Coefficients: \", round.(coef(ar), digits=4))\nprintln(\"  AIC: \", round(aic(ar), digits=2))\nprintln(\"  BIC: \", round(bic(ar), digits=2))\n\n# === ARMA(1,1) via CSS-MLE ===\narma = estimate_arma(y, 1, 1)\nprintln(\"\\nARMA(1,1) Estimation\")\nprintln(\"  AR coef: \", round(arma.ar_coefs[1], digits=4))\nprintln(\"  MA coef: \", round(arma.ma_coefs[1], digits=4))\nprintln(\"  AIC: \", round(aic(arma), digits=2))\n\n# === Automatic order selection ===\nbest = auto_arima(y)\nprintln(\"\\nauto_arima selection:\")\nprintln(\"  Best model: ARIMA($(best.p),$(best.d),$(best.q))\")\nprintln(\"  AIC: \", round(aic(best), digits=2))\n\n# === Information criteria table ===\nict = ic_table(y, 4, 4)\nprintln(\"\\nIC table (top 5 by AIC):\")\nfor i in 1:min(5, size(ict, 1))\n    println(\"  p=$(Int(ict[i,1])), q=$(Int(ict[i,2])): AIC=$(round(ict[i,3], digits=1)), BIC=$(round(ict[i,4], digits=1))\")\nend\n\n# === Forecast ===\nfc = forecast(arma, 12; conf_level=0.95)\nprintln(\"\\nARMA(1,1) Forecasts:\")\nfor h in [1, 4, 8, 12]\n    println(\"  h=$h: $(round(fc.forecast[h], digits=3)) [$(round(fc.ci_lower[h], digits=3)), $(round(fc.ci_upper[h], digits=3))]\")\nend\n\nThe auto_arima function performs a grid search over (p,d,q) combinations, selecting the model that minimizes AIC. The CSS-MLE estimation method initializes parameters via conditional sum of squares (CSS), then refines via exact maximum likelihood using the Kalman filter. Forecast confidence intervals widen with the horizon, reflecting accumulating prediction uncertainty.\n\n","category":"section"},{"location":"examples/#Example-2:-Volatility-Models","page":"Examples","title":"Example 2: Volatility Models","text":"This example estimates ARCH, GARCH, EGARCH, and GJR-GARCH models on the same data, compares their news impact curves, runs diagnostics, and forecasts volatility. See also Volatility Models for theory and return value tables.\n\nusing MacroEconometricModels\nusing Random\nusing Statistics\n\nRandom.seed!(42)\n\n# === Generate GARCH(1,1) data with leverage effect ===\nT = 1000\ny = zeros(T)\nh = zeros(T)\nh[1] = 1.0\n\nfor t in 2:T\n    z = randn()\n    h[t] = 0.01 + 0.08 * y[t-1]^2 + 0.12 * (y[t-1] < 0 ? 1 : 0) * y[t-1]^2 + 0.85 * h[t-1]\n    y[t] = sqrt(h[t]) * z\nend\n\nprintln(\"Simulated T=$T observations from GJR-GARCH(1,1)\")\nprintln(\"Sample kurtosis: \", round(kurtosis(y), digits=2))\n\n# === Step 1: Test for ARCH effects ===\nstat, pval, q = arch_lm_test(y, 5)\nprintln(\"\\nARCH-LM test (q=5): stat=$(round(stat, digits=2)), p=$(round(pval, digits=6))\")\n\nstat2, pval2, K = ljung_box_squared(y, 10)\nprintln(\"Ljung-Box squared (K=10): stat=$(round(stat2, digits=2)), p=$(round(pval2, digits=6))\")\n\n# === Step 2: Estimate competing models ===\ngarch   = estimate_garch(y, 1, 1)\negarch  = estimate_egarch(y, 1, 1)\ngjr     = estimate_gjr_garch(y, 1, 1)\n\nprintln(\"\\n\" * \"=\"^60)\nprintln(\"Model Comparison\")\nprintln(\"=\"^60)\nprintln(\"              AIC         BIC     Persistence\")\nprintln(\"  GARCH:   \", round(aic(garch), digits=1),\n        \"    \", round(bic(garch), digits=1),\n        \"    \", round(persistence(garch), digits=4))\nprintln(\"  EGARCH:  \", round(aic(egarch), digits=1),\n        \"    \", round(bic(egarch), digits=1),\n        \"    \", round(persistence(egarch), digits=4))\nprintln(\"  GJR:     \", round(aic(gjr), digits=1),\n        \"    \", round(bic(gjr), digits=1),\n        \"    \", round(persistence(gjr), digits=4))\n\n# === Step 3: News impact curves ===\nnic_g  = news_impact_curve(garch)\nnic_e  = news_impact_curve(egarch)\nnic_j  = news_impact_curve(gjr)\n\nprintln(\"\\nNews Impact at epsilon = -2 vs epsilon = +2:\")\nidx_neg = findfirst(x -> x >= -2.0, nic_g.shocks)\nidx_pos = findfirst(x -> x >= 2.0, nic_g.shocks)\n\nprintln(\"  GARCH:  var(-2) = \", round(nic_g.variance[idx_neg], digits=4),\n        \"   var(+2) = \", round(nic_g.variance[idx_pos], digits=4))\nprintln(\"  EGARCH: var(-2) = \", round(nic_e.variance[idx_neg], digits=4),\n        \"   var(+2) = \", round(nic_e.variance[idx_pos], digits=4))\nprintln(\"  GJR:    var(-2) = \", round(nic_j.variance[idx_neg], digits=4),\n        \"   var(+2) = \", round(nic_j.variance[idx_pos], digits=4))\n\n# === Step 4: Residual diagnostics ===\nprintln(\"\\nResidual ARCH-LM test (q=5):\")\nfor (name, m) in [(\"GARCH\", garch), (\"EGARCH\", egarch), (\"GJR\", gjr)]\n    _, p, _ = arch_lm_test(m, 5)\n    status = p > 0.05 ? \"Pass\" : \"FAIL\"\n    println(\"  $name: p=$(round(p, digits=4))  $status\")\nend\n\n# === Step 5: Volatility forecasts ===\nH = 20\nfc_g = forecast(garch, H)\nfc_e = forecast(egarch, H)\nfc_j = forecast(gjr, H)\n\nprintln(\"\\nVolatility forecasts (conditional variance):\")\nprintln(\"  h    GARCH    EGARCH   GJR      Uncond\")\nfor h_idx in [1, 5, 10, 20]\n    println(\"  $h_idx    \",\n            round(fc_g.forecast[h_idx], digits=4), \"  \",\n            round(fc_e.forecast[h_idx], digits=4), \"  \",\n            round(fc_j.forecast[h_idx], digits=4), \"  \",\n            round(unconditional_variance(garch), digits=4))\nend\n\n# === Step 6: Stochastic Volatility ===\nprintln(\"\\nEstimating SV model via MCMC...\")\nsv = estimate_sv(y; n_samples=2000, n_adapts=1000)\n\nprintln(\"SV posterior summary:\")\nprintln(\"  mu:      \", round(mean(sv.mu_post), digits=3))\nprintln(\"  phi:     \", round(mean(sv.phi_post), digits=3))\nprintln(\"  sigma_eta: \", round(mean(sv.sigma_eta_post), digits=3))\n\nfc_sv = forecast(sv, H)\nprintln(\"\\nSV forecast at h=1:  \", round(fc_sv.forecast[1], digits=4))\nprintln(\"SV forecast at h=20: \", round(fc_sv.forecast[end], digits=4))\n\nThe GJR-GARCH model should provide the best fit (lowest AIC/BIC) since the data was generated from a GJR-GARCH DGP with a leverage effect. The news impact curves reveal the asymmetry: for EGARCH and GJR-GARCH, the variance response to varepsilon = -2 exceeds that for varepsilon = +2; for symmetric GARCH, they are equal. All models' standardized residuals should pass the ARCH-LM test after fitting, confirming that the variance dynamics are adequately captured.\n\n","category":"section"},{"location":"examples/#Example-8:-Unit-Root-Testing-and-Pre-Estimation-Analysis","page":"Examples","title":"Example 8: Unit Root Testing and Pre-Estimation Analysis","text":"This example demonstrates comprehensive unit root testing before fitting VAR models. Pre-estimation analysis is the first step in any empirical macro workflow. See Hypothesis Tests for theoretical background.","category":"section"},{"location":"examples/#Individual-Unit-Root-Tests","page":"Examples","title":"Individual Unit Root Tests","text":"using MacroEconometricModels\nusing Random\nusing Statistics\n\nRandom.seed!(42)\n\n# Generate data: mix of I(0) and I(1) series\nT = 200\ny_stationary = randn(T)                      # I(0): stationary\ny_random_walk = cumsum(randn(T))             # I(1): unit root\ny_trend_stat = 0.1 .* (1:T) .+ randn(T)      # Trend stationary\ny_with_break = vcat(randn(100), randn(100) .+ 2)  # Structural break\n\n# === ADF Test ===\nprintln(\"=\"^60)\nprintln(\"ADF Test (H₀: unit root)\")\nprintln(\"=\"^60)\n\nadf_stat = adf_test(y_stationary; lags=:aic, regression=:constant)\nprintln(\"\\nStationary series:\")\nprintln(\"  Statistic: \", round(adf_stat.statistic, digits=3))\nprintln(\"  P-value: \", round(adf_stat.pvalue, digits=4))\nprintln(\"  Lags: \", adf_stat.lags)\n\nadf_rw = adf_test(y_random_walk; lags=:aic, regression=:constant)\nprintln(\"\\nRandom walk:\")\nprintln(\"  Statistic: \", round(adf_rw.statistic, digits=3))\nprintln(\"  P-value: \", round(adf_rw.pvalue, digits=4))\n\nThe ADF test statistic is compared to non-standard critical values (Dickey-Fuller distribution, not Student-t). For the stationary series, the large negative test statistic yields a small p-value, rejecting the unit root null. For the random walk, the test statistic is close to zero, failing to reject. The number of augmenting lags selected by AIC controls for residual serial correlation.","category":"section"},{"location":"examples/#KPSS-Complementary-Test","page":"Examples","title":"KPSS Complementary Test","text":"# === KPSS Test ===\nprintln(\"\\n\" * \"=\"^60)\nprintln(\"KPSS Test (H₀: stationarity)\")\nprintln(\"=\"^60)\n\nkpss_stat = kpss_test(y_stationary; regression=:constant)\nprintln(\"\\nStationary series:\")\nprintln(\"  Statistic: \", round(kpss_stat.statistic, digits=4))\nprintln(\"  P-value: \", kpss_stat.pvalue > 0.10 ? \">0.10\" : round(kpss_stat.pvalue, digits=4))\nprintln(\"  Bandwidth: \", kpss_stat.bandwidth)\n\nkpss_rw = kpss_test(y_random_walk; regression=:constant)\nprintln(\"\\nRandom walk:\")\nprintln(\"  Statistic: \", round(kpss_rw.statistic, digits=4))\nprintln(\"  P-value: \", kpss_rw.pvalue < 0.01 ? \"<0.01\" : round(kpss_rw.pvalue, digits=4))","category":"section"},{"location":"examples/#Combining-ADF-and-KPSS-for-Robust-Inference","page":"Examples","title":"Combining ADF and KPSS for Robust Inference","text":"# === Combined Analysis ===\nprintln(\"\\n\" * \"=\"^60)\nprintln(\"Combined ADF + KPSS Analysis\")\nprintln(\"=\"^60)\n\nfunction unit_root_decision(y; name=\"Series\")\n    adf = adf_test(y; lags=:aic)\n    kpss = kpss_test(y)\n\n    adf_reject = adf.pvalue < 0.05  # Reject unit root\n    kpss_reject = kpss.pvalue < 0.05  # Reject stationarity\n\n    decision = if adf_reject && !kpss_reject\n        \"I(0) - Stationary\"\n    elseif !adf_reject && kpss_reject\n        \"I(1) - Unit root\"\n    elseif adf_reject && kpss_reject\n        \"Conflicting (possible structural break)\"\n    else\n        \"Inconclusive\"\n    end\n\n    println(\"\\n$name:\")\n    println(\"  ADF p-value: \", round(adf.pvalue, digits=4))\n    println(\"  KPSS p-value: \", round(kpss.pvalue, digits=4))\n    println(\"  Decision: $decision\")\n\n    return decision\nend\n\nunit_root_decision(y_stationary; name=\"Stationary series\")\nunit_root_decision(y_random_walk; name=\"Random walk\")\nunit_root_decision(y_trend_stat; name=\"Trend stationary\")","category":"section"},{"location":"examples/#Testing-for-Structural-Breaks","page":"Examples","title":"Testing for Structural Breaks","text":"# === Zivot-Andrews Test ===\nprintln(\"\\n\" * \"=\"^60)\nprintln(\"Zivot-Andrews Test (H₀: unit root without break)\")\nprintln(\"=\"^60)\n\nza_result = za_test(y_with_break; regression=:constant, trim=0.15)\nprintln(\"\\nSeries with structural break:\")\nprintln(\"  Minimum t-stat: \", round(za_result.statistic, digits=3))\nprintln(\"  P-value: \", round(za_result.pvalue, digits=4))\nprintln(\"  Break index: \", za_result.break_index)\nprintln(\"  Break at: \", round(za_result.break_fraction * 100, digits=1), \"% of sample\")\n\n# Compare with standard ADF\nadf_break = adf_test(y_with_break)\nprintln(\"\\n  ADF (ignoring break): p=\", round(adf_break.pvalue, digits=4))\nprintln(\"  ZA (allowing break): p=\", round(za_result.pvalue, digits=4))","category":"section"},{"location":"examples/#Ng-Perron-Tests-for-Small-Samples","page":"Examples","title":"Ng-Perron Tests for Small Samples","text":"# === Ng-Perron Tests ===\nprintln(\"\\n\" * \"=\"^60)\nprintln(\"Ng-Perron Tests (improved size properties)\")\nprintln(\"=\"^60)\n\n# Generate smaller sample\ny_small = cumsum(randn(80))\nnp_result = ngperron_test(y_small; regression=:constant)\n\nprintln(\"\\nSmall sample (n=80):\")\nprintln(\"  MZα: \", round(np_result.MZa, digits=3),\n        \" (5% CV: \", np_result.critical_values[:MZa][5], \")\")\nprintln(\"  MZt: \", round(np_result.MZt, digits=3),\n        \" (5% CV: \", np_result.critical_values[:MZt][5], \")\")\nprintln(\"  MSB: \", round(np_result.MSB, digits=4),\n        \" (5% CV: \", np_result.critical_values[:MSB][5], \")\")\nprintln(\"  MPT: \", round(np_result.MPT, digits=3),\n        \" (5% CV: \", np_result.critical_values[:MPT][5], \")\")","category":"section"},{"location":"examples/#Johansen-Cointegration-Test","page":"Examples","title":"Johansen Cointegration Test","text":"# === Johansen Cointegration Test ===\nprintln(\"\\n\" * \"=\"^60)\nprintln(\"Johansen Cointegration Test\")\nprintln(\"=\"^60)\n\n# Generate cointegrated system\nT_coint = 200\nu1, u2, u3 = cumsum(randn(T_coint)), cumsum(randn(T_coint)), randn(T_coint)\nY_coint = hcat(\n    u1 + 0.1*randn(T_coint),           # I(1)\n    u1 + 0.5*u2 + 0.1*randn(T_coint),  # Cointegrated with first\n    u2 + 0.1*randn(T_coint)            # I(1)\n)\n\njohansen = johansen_test(Y_coint, 2; deterministic=:constant)\n\nprintln(\"\\nCointegrated system (3 variables):\")\nprintln(\"  Estimated rank: \", johansen.rank)\nprintln(\"\\n  Trace test:\")\nfor r in 0:2\n    stat = round(johansen.trace_stats[r+1], digits=2)\n    cv = round(johansen.critical_values_trace[r+1, 2], digits=2)\n    reject = stat > cv ? \"Reject\" : \"Fail to reject\"\n    println(\"    H₀: r ≤ $r: stat=$stat, 5% CV=$cv → $reject\")\nend\n\nprintln(\"\\n  Eigenvalues: \", round.(johansen.eigenvalues, digits=4))\n\nif johansen.rank > 0\n    println(\"\\n  Cointegrating vector(s):\")\n    for i in 1:johansen.rank\n        println(\"    β$i: \", round.(johansen.eigenvectors[:, i], digits=3))\n    end\nend\n\nThe Johansen trace test sequentially tests hypotheses about the cointegration rank. When the trace statistic exceeds the critical value, we reject the null and move to the next rank. The estimated cointegrating vectors beta represent long-run equilibrium relationships: deviations from beta y_t are stationary even though the individual series are I(1). The adjustment coefficients alpha govern how quickly variables correct back toward equilibrium.","category":"section"},{"location":"examples/#Testing-All-Variables-Before-VAR","page":"Examples","title":"Testing All Variables Before VAR","text":"# === Multi-Variable Pre-VAR Analysis ===\nprintln(\"\\n\" * \"=\"^60)\nprintln(\"Pre-VAR Unit Root Analysis\")\nprintln(\"=\"^60)\n\n# Typical macro dataset\nY_macro = hcat(\n    cumsum(randn(T)),           # GDP (I(1))\n    0.8*cumsum(randn(T)[1:T]),  # Inflation (I(1))\n    cumsum(randn(T)),           # Interest rate (I(1))\n    randn(T)                    # Output gap (I(0))\n)\nvar_names = [\"GDP\", \"Inflation\", \"Rate\", \"Output Gap\"]\n\n# Test all variables\nresults = test_all_variables(Y_macro; test=:adf)\n\nprintln(\"\\nUnit root test results:\")\nprintln(\"-\"^50)\nn_i1 = 0\nfor (i, r) in enumerate(results)\n    status = r.pvalue > 0.05 ? \"I(1)\" : \"I(0)\"\n    n_i1 += r.pvalue > 0.05\n    println(\"  $(var_names[i]): p=$(round(r.pvalue, digits=3)) → $status\")\nend\n\nprintln(\"\\nSummary: $n_i1 of $(size(Y_macro, 2)) variables appear I(1)\")\n\n# Recommendation\nif n_i1 == size(Y_macro, 2)\n    println(\"\\nRecommendation: All variables I(1)\")\n    println(\"  → Test for cointegration\")\n    println(\"  → If cointegrated: use VECM\")\n    println(\"  → If not: use VAR in first differences\")\nelseif n_i1 == 0\n    println(\"\\nRecommendation: All variables I(0)\")\n    println(\"  → Use VAR in levels\")\nelse\n    println(\"\\nRecommendation: Mixed I(0)/I(1)\")\n    println(\"  → Consider ARDL bounds test\")\n    println(\"  → Or difference I(1) variables\")\nend","category":"section"},{"location":"examples/#Complete-Pre-Estimation-Workflow","page":"Examples","title":"Complete Pre-Estimation Workflow","text":"# === Complete Workflow ===\nprintln(\"\\n\" * \"=\"^60)\nprintln(\"Complete Pre-Estimation Workflow\")\nprintln(\"=\"^60)\n\nfunction pre_estimation_analysis(Y; var_names=nothing, α=0.05)\n    T, n = size(Y)\n    var_names = isnothing(var_names) ? [\"Var$i\" for i in 1:n] : var_names\n\n    println(\"\\n1. Individual Unit Root Tests\")\n    println(\"-\"^40)\n\n    integration_orders = zeros(Int, n)\n    for i in 1:n\n        adf = adf_test(Y[:, i]; lags=:aic)\n        kpss = kpss_test(Y[:, i])\n\n        if adf.pvalue < α && kpss.pvalue > α\n            integration_orders[i] = 0\n            status = \"I(0)\"\n        elseif adf.pvalue > α && kpss.pvalue < α\n            integration_orders[i] = 1\n            status = \"I(1)\"\n        else\n            integration_orders[i] = -1  # Inconclusive\n            status = \"Inconclusive\"\n        end\n        println(\"  $(var_names[i]): $status (ADF p=$(round(adf.pvalue, digits=3)), KPSS p=$(round(kpss.pvalue, digits=3)))\")\n    end\n\n    n_i1 = sum(integration_orders .== 1)\n    n_i0 = sum(integration_orders .== 0)\n\n    println(\"\\n2. Summary\")\n    println(\"-\"^40)\n    println(\"  I(0) variables: $n_i0\")\n    println(\"  I(1) variables: $n_i1\")\n    println(\"  Inconclusive: $(n - n_i0 - n_i1)\")\n\n    # Cointegration test if all I(1)\n    if n_i1 >= 2\n        println(\"\\n3. Cointegration Test\")\n        println(\"-\"^40)\n        joh = johansen_test(Y, 2)\n        println(\"  Estimated cointegration rank: \", joh.rank)\n\n        if joh.rank > 0\n            println(\"  → Cointegration detected\")\n            println(\"  → Recommendation: VECM with rank=$(joh.rank)\")\n        else\n            println(\"  → No cointegration\")\n            println(\"  → Recommendation: VAR in first differences\")\n        end\n    elseif n_i0 == n\n        println(\"\\n3. Recommendation\")\n        println(\"-\"^40)\n        println(\"  All series stationary → VAR in levels\")\n    end\n\n    return (integration_orders=integration_orders, n_i0=n_i0, n_i1=n_i1)\nend\n\n# Run complete analysis\nresult = pre_estimation_analysis(Y_macro; var_names=var_names)\n\n","category":"section"},{"location":"examples/#Example-3:-Three-Variable-VAR-Analysis","page":"Examples","title":"Example 3: Three-Variable VAR Analysis","text":"This example walks through a complete analysis of a macroeconomic VAR with GDP growth, inflation, and the federal funds rate.","category":"section"},{"location":"examples/#Setup-and-Data-Generation","page":"Examples","title":"Setup and Data Generation","text":"using MacroEconometricModels\nusing Random\nusing LinearAlgebra\nusing Statistics\n\nRandom.seed!(42)\n\n# Generate realistic macro data from a VAR(1) DGP\nT = 200\nn = 3\np = 2\n\n# True VAR(1) coefficients (persistent, cross-correlated)\nA_true = [0.85 0.10 -0.15;   # GDP responds to own lag, inflation, rate\n          0.05 0.70  0.00;   # Inflation mainly AR\n          0.10 0.20  0.80]   # Rate responds to GDP and inflation\n\n# Shock covariance (correlated shocks)\nΣ_true = [1.00 0.50 0.20;\n          0.50 0.80 0.10;\n          0.20 0.10 0.60]\n\n# Generate data\nY = zeros(T, n)\nY[1, :] = randn(n)\nchol_Σ = cholesky(Σ_true).L\n\nfor t in 2:T\n    Y[t, :] = A_true * Y[t-1, :] + chol_Σ * randn(n)\nend\n\nvar_names = [\"GDP Growth\", \"Inflation\", \"Fed Funds Rate\"]\nprintln(\"Data: T=$T observations, n=$n variables\")","category":"section"},{"location":"examples/#Frequentist-VAR-Estimation","page":"Examples","title":"Frequentist VAR Estimation","text":"# Estimate VAR(2) model via OLS\nmodel = fit(VARModel, Y, p)\n\n# Model diagnostics\nprintln(\"Log-likelihood: \", loglikelihood(model))\nprintln(\"AIC: \", aic(model))\nprintln(\"BIC: \", bic(model))\n\n# Check stability (eigenvalues inside unit circle)\nF = companion_matrix(model.B, n, p)\neigenvalues = eigvals(F)\nprintln(\"Max eigenvalue modulus: \", maximum(abs.(eigenvalues)))\nprintln(\"Stable: \", maximum(abs.(eigenvalues)) < 1)\n\nThe AIC and BIC values measure the trade-off between fit and parsimony. Lower values indicate a better model. The maximum eigenvalue modulus should be strictly less than 1 for the VAR to be stationary; values close to 1 indicate high persistence, while values near 0 suggest rapid mean-reversion.","category":"section"},{"location":"examples/#Cholesky-Identified-IRF","page":"Examples","title":"Cholesky-Identified IRF","text":"# Compute 20-period IRF with Cholesky identification\n# Ordering: GDP → Inflation → Rate (contemporaneous causality)\nH = 20\nirfs = irf(model, H; method=:cholesky)\n\n# Display impact responses (horizon 0)\nprintln(\"\\nImpact responses (B₀):\")\nprintln(\"  GDP shock → GDP: \", round(irfs.irf[1, 1, 1], digits=3))\nprintln(\"  GDP shock → Inflation: \", round(irfs.irf[1, 2, 1], digits=3))\nprintln(\"  GDP shock → Rate: \", round(irfs.irf[1, 3, 1], digits=3))\n\n# Long-run responses (horizon H)\nprintln(\"\\nLong-run responses (h=$H):\")\nprintln(\"  GDP shock → GDP: \", round(irfs.irf[H+1, 1, 1], digits=3))","category":"section"},{"location":"examples/#Sign-Restriction-Identification","page":"Examples","title":"Sign Restriction Identification","text":"# Sign restrictions: Demand shock raises GDP and inflation on impact\nfunction check_demand_shock(irf_array)\n    # irf_array is (H+1) × n × n\n    # Check: Shock 1 → Variable 1 (GDP) positive\n    #        Shock 1 → Variable 2 (Inflation) positive\n    return irf_array[1, 1, 1] > 0 && irf_array[1, 2, 1] > 0\nend\n\n# Estimate with sign restrictions\nirfs_sign = irf(model, H; method=:sign, check_func=check_demand_shock, n_draws=1000)\n\nprintln(\"\\nSign-identified demand shock:\")\nprintln(\"  GDP response: \", round(irfs_sign.irf[1, 1, 1], digits=3))\nprintln(\"  Inflation response: \", round(irfs_sign.irf[1, 2, 1], digits=3))\n\nThe Cholesky identification assumes a recursive causal ordering (GDP → Inflation → Rate), meaning GDP responds only to its own shocks contemporaneously. Sign restrictions provide a theory-based alternative: requiring both GDP and inflation to rise on impact identifies a \"demand shock\" without imposing a specific causal ordering. If sign restrictions accept many draws, the set-identified IRFs will show wider bands than point-identified Cholesky responses.","category":"section"},{"location":"examples/#Forecast-Error-Variance-Decomposition","page":"Examples","title":"Forecast Error Variance Decomposition","text":"# Compute FEVD\nfevd_result = fevd(model, H; method=:cholesky)\n\n# Variance decomposition at horizon 1, 4, and 20\nfor h in [1, 4, 20]\n    println(\"\\nFEVD at horizon $h:\")\n    for i in 1:n\n        println(\"  $(var_names[i]):\")\n        for j in 1:n\n            pct = round(fevd_result.fevd[h, i, j] * 100, digits=1)\n            println(\"    Shock $j: $pct%\")\n        end\n    end\nend\n\nThe FEVD shows the proportion of each variable's forecast error variance attributable to each structural shock. At short horizons, own shocks typically dominate. As the horizon increases, cross-variable transmission becomes more important, and the FEVD converges to the unconditional variance decomposition. If shock 1 explains a large share of GDP variance at long horizons, it is the primary driver of GDP fluctuations in the model.\n\n","category":"section"},{"location":"examples/#Example-7:-Non-Gaussian-Structural-Identification","page":"Examples","title":"Example 7: Non-Gaussian Structural Identification","text":"When structural shocks are non-Gaussian, statistical independence provides identification without imposing economic restrictions like recursive ordering or sign constraints. This example demonstrates the full non-Gaussian identification workflow: testing for non-Gaussianity, ICA-based and ML-based identification, and post-estimation specification tests.","category":"section"},{"location":"examples/#Setup:-Generate-Non-Gaussian-Data","page":"Examples","title":"Setup: Generate Non-Gaussian Data","text":"using MacroEconometricModels\nusing Random\nusing LinearAlgebra\nusing Statistics\n\nRandom.seed!(42)\n\n# True structural parameters\nT = 500\nn = 3\n\n# True B₀ (structural impact matrix)\nB0_true = [1.0  0.0  0.0;\n           0.5  1.0  0.0;\n           0.3 -0.2  1.0]\n\n# Non-Gaussian structural shocks (Student-t with 5 df)\n# Heavy tails provide the non-Gaussianity needed for identification\neps = zeros(T, n)\nfor j in 1:n\n    # Standardized t(5): mean 0, variance 1\n    raw = randn(T) ./ sqrt.(rand(Chisq(5), T) ./ 5)\n    eps[:, j] = raw ./ std(raw)\nend\n\n# True VAR(1) dynamics\nA_true = [0.7 0.1 0.0;\n          0.0 0.6 0.1;\n          0.0 0.0 0.5]\n\n# Generate reduced-form data: u_t = B₀ ε_t\nY = zeros(T, n)\nY[1, :] = B0_true * eps[1, :]\nfor t in 2:T\n    Y[t, :] = A_true * Y[t-1, :] + B0_true * eps[t, :]\nend\n\nprintln(\"Data: T=$T, n=$n (non-Gaussian DGP with t(5) shocks)\")","category":"section"},{"location":"examples/#Step-1:-Test-for-Non-Gaussianity","page":"Examples","title":"Step 1: Test for Non-Gaussianity","text":"Before using non-Gaussian identification, verify that residuals are indeed non-Gaussian:\n\n# Estimate VAR\nmodel = estimate_var(Y, 1)\n\n# Run the full normality test suite\nsuite = normality_test_suite(model)\n\nprintln(\"Multivariate Normality Tests (H₀: residuals are Gaussian)\")\nprintln(\"=\"^55)\nfor r in suite.results\n    stars = r.pvalue < 0.01 ? \"***\" : r.pvalue < 0.05 ? \"**\" : r.pvalue < 0.10 ? \"*\" : \"\"\n    println(\"  $(r.test_name): stat=$(round(r.statistic, digits=2)), p=$(round(r.pvalue, digits=4)) $stars\")\nend\n\nAll four tests (Jarque-Bera, Mardia, Doornik-Hansen, Henze-Zirkler) should reject normality when the true shocks are t-distributed. If normality is not rejected, non-Gaussian identification may lack power and Cholesky or sign restrictions should be preferred.\n\nYou can also run individual tests:\n\n# Individual tests\njb = jarque_bera_test(model)\nmardia = mardia_test(model; type=:both)\ndh = doornik_hansen_test(model)\nhz = henze_zirkler_test(model)\n\nprintln(\"\\nDetailed Mardia test:\")\nprintln(\"  Skewness stat: \", round(mardia.statistic, digits=2))\nprintln(\"  P-value: \", round(mardia.pvalue, digits=4))","category":"section"},{"location":"examples/#Step-2:-ICA-Based-Identification","page":"Examples","title":"Step 2: ICA-Based Identification","text":"ICA (Independent Component Analysis) recovers structurally independent shocks by maximizing statistical independence:\n\n# FastICA identification (default: logcosh contrast)\nica_result = identify_fastica(model; contrast=:logcosh, approach=:deflation)\n\nprintln(\"\\nFastICA Identification\")\nprintln(\"=\"^40)\nprintln(\"  Converged: \", ica_result.converged)\nprintln(\"  Iterations: \", ica_result.iterations)\nprintln(\"  Objective: \", round(ica_result.objective, digits=6))\n\n# Structural impact matrix B₀\nprintln(\"\\nEstimated B₀ (structural impact matrix):\")\nfor i in 1:n\n    println(\"  \", [round(ica_result.B0[i, j], digits=3) for j in 1:n])\nend\n\nCompare different ICA algorithms:\n\n# JADE (Joint Approximate Diagonalization of Eigenmatrices)\njade_result = identify_jade(model)\n\n# SOBI (Second-Order Blind Identification — exploits temporal structure)\nsobi_result = identify_sobi(model; lags=1:12)\n\n# Distance-covariance ICA\ndcov_result = identify_dcov(model)\n\nprintln(\"\\nComparison of ICA methods:\")\nprintln(\"  FastICA converged: \", ica_result.converged, \" (iter: \", ica_result.iterations, \")\")\nprintln(\"  JADE converged:    \", jade_result.converged, \" (iter: \", jade_result.iterations, \")\")\nprintln(\"  SOBI converged:    \", sobi_result.converged, \" (iter: \", sobi_result.iterations, \")\")\nprintln(\"  dCov converged:    \", dcov_result.converged, \" (iter: \", dcov_result.iterations, \")\")\n\nFastICA is the fastest and most commonly used, but JADE is more robust when multiple shocks have similar kurtosis. SOBI exploits temporal dependence and works even with mildly non-Gaussian shocks.","category":"section"},{"location":"examples/#Step-3:-Compute-IRFs-with-ICA-Identification","page":"Examples","title":"Step 3: Compute IRFs with ICA Identification","text":"The rotation matrix Q from ICA integrates directly with the standard irf() and fevd() functions:\n\n# IRF using FastICA-identified structure\nirfs_ica = irf(model, 20; method=:fastica)\n\nprintln(\"\\nFastICA-identified IRF (shock 1 → all variables):\")\nfor h in [0, 4, 8, 12, 20]\n    vals = [round(irfs_ica.irf[h+1, v, 1], digits=3) for v in 1:n]\n    println(\"  h=$h: \", vals)\nend\n\n# FEVD using ICA identification\nfevd_ica = fevd(model, 20; method=:fastica)\n\nprintln(\"\\nFEVD at h=20 (ICA-identified):\")\nfor v in 1:n\n    shares = [round(fevd_ica.fevd[21, v, s] * 100, digits=1) for s in 1:n]\n    println(\"  Variable $v: \", shares, \"%\")\nend\n\nUnlike Cholesky identification, the ICA-based IRFs do not depend on variable ordering. The same data produces the same structural shocks regardless of how the columns of Y are arranged.","category":"section"},{"location":"examples/#Step-4:-ML-Based-Identification","page":"Examples","title":"Step 4: ML-Based Identification","text":"Maximum likelihood methods parameterize the shock distribution and jointly estimate B_0 and the distributional parameters:\n\n# Student-t ML identification\nml_t = identify_student_t(model)\n\nprintln(\"\\nStudent-t ML Identification\")\nprintln(\"=\"^40)\nprintln(\"  Converged: \", ml_t.converged)\nprintln(\"  Log-likelihood (non-Gaussian): \", round(ml_t.loglik, digits=2))\nprintln(\"  Log-likelihood (Gaussian):     \", round(ml_t.loglik_gaussian, digits=2))\nprintln(\"  AIC: \", round(ml_t.aic, digits=2))\nprintln(\"  BIC: \", round(ml_t.bic, digits=2))\n\n# Estimated degrees of freedom for each shock\nif haskey(ml_t.dist_params, :nu)\n    println(\"  Estimated ν (df): \", round.(ml_t.dist_params[:nu], digits=2))\nend\n\n# Standard errors for B₀ elements\nprintln(\"\\nB₀ standard errors:\")\nfor i in 1:n\n    println(\"  \", [round(ml_t.se[i, j], digits=4) for j in 1:n])\nend\n\nThe Student-t ML approach provides standard errors for B_0 elements, unlike ICA which only gives point estimates. Compare with other distributional assumptions:\n\n# Mixture of normals\nml_mix = identify_mixture_normal(model; n_components=2)\n\n# Pseudo-maximum likelihood (robust, no distributional assumption)\nml_pml = identify_pml(model)\n\n# Unified interface — select distribution via keyword\nml_auto = identify_nongaussian_ml(model; distribution=:student_t)\n\nprintln(\"\\nML method comparison (AIC):\")\nprintln(\"  Student-t:      AIC = \", round(ml_t.aic, digits=2))\nprintln(\"  Mixture normal: AIC = \", round(ml_mix.aic, digits=2))\nprintln(\"  PML:            AIC = \", round(ml_pml.aic, digits=2))\n\nLower AIC indicates a better distributional fit. The PML estimator is semiparametrically efficient and does not require specifying the shock distribution.","category":"section"},{"location":"examples/#Step-5:-Heteroskedasticity-Based-Identification","page":"Examples","title":"Step 5: Heteroskedasticity-Based Identification","text":"When shocks exhibit time-varying volatility, changes in the covariance structure can identify the structural model:\n\n# External volatility regimes (e.g., pre/post Great Moderation)\nregime = vcat(ones(Int, 250), 2 * ones(Int, 250))  # Two regimes\nvol_result = identify_external_volatility(model, regime; regimes=2)\n\nprintln(\"\\nExternal Volatility Identification\")\nprintln(\"=\"^40)\nprintln(\"  Regime 1 shock variances: \",\n        [round(vol_result.Lambda_vecs[1][j], digits=3) for j in 1:n])\nprintln(\"  Regime 2 shock variances: \",\n        [round(vol_result.Lambda_vecs[2][j], digits=3) for j in 1:n])","category":"section"},{"location":"examples/#Step-6:-Post-Estimation-Specification-Tests","page":"Examples","title":"Step 6: Post-Estimation Specification Tests","text":"Verify that the identification assumptions hold:\n\n# Test 1: Are recovered shocks non-Gaussian?\ngauss_test = test_shock_gaussianity(ica_result)\nprintln(\"\\nShock Gaussianity Test (H₀: shocks are Gaussian)\")\nprintln(\"  Statistic: \", round(gauss_test.statistic, digits=2))\nprintln(\"  P-value: \", round(gauss_test.pvalue, digits=4))\nprintln(\"  Non-Gaussian: \", gauss_test.identified)\n\n# Test 2: Are recovered shocks independent?\nindep_test = test_shock_independence(ica_result; max_lag=10)\nprintln(\"\\nShock Independence Test (H₀: shocks are independent)\")\nprintln(\"  Statistic: \", round(indep_test.statistic, digits=2))\nprintln(\"  P-value: \", round(indep_test.pvalue, digits=4))\nprintln(\"  Independent: \", indep_test.identified)\n\n# Test 3: Identification strength (bootstrap)\nstrength_test = test_identification_strength(model; method=:fastica, n_bootstrap=499)\nprintln(\"\\nIdentification Strength Test\")\nprintln(\"  Statistic: \", round(strength_test.statistic, digits=4))\nprintln(\"  P-value: \", round(strength_test.pvalue, digits=4))\nprintln(\"  Strongly identified: \", strength_test.identified)\n\n# Test 4: Gaussian vs non-Gaussian likelihood ratio\nlr_test = test_gaussian_vs_nongaussian(model; method=:fastica, n_bootstrap=499)\nprintln(\"\\nLR Test: Gaussian vs Non-Gaussian\")\nprintln(\"  LR statistic: \", round(lr_test.statistic, digits=2))\nprintln(\"  P-value: \", round(lr_test.pvalue, digits=4))\n\nA valid non-Gaussian SVAR requires: (1) rejection of shock Gaussianity (non-Gaussian shocks are needed for identification), (2) failure to reject shock independence (the identified shocks should be independent), and (3) strong identification (the structural parameters are precisely estimated). If the Gaussianity test fails to reject, the data may not contain enough non-Gaussianity to identify the model, and traditional Cholesky or sign restrictions should be used instead.","category":"section"},{"location":"examples/#Comparing-Cholesky-vs-ICA-Identification","page":"Examples","title":"Comparing Cholesky vs ICA Identification","text":"# Cholesky IRF (ordering-dependent)\nirfs_chol = irf(model, 20; method=:cholesky)\n\n# ICA IRF (ordering-independent)\nirfs_ica = irf(model, 20; method=:fastica)\n\nprintln(\"\\nCholesky vs FastICA IRF comparison (shock 1 → variable 1):\")\nprintln(\"  h   Cholesky   FastICA\")\nfor h in [0, 4, 8, 12, 20]\n    chol_val = round(irfs_chol.irf[h+1, 1, 1], digits=3)\n    ica_val = round(irfs_ica.irf[h+1, 1, 1], digits=3)\n    println(\"  $h    $chol_val      $ica_val\")\nend\n\nWhen the true DGP is recursive (lower-triangular B_0), Cholesky and ICA should yield similar IRFs. Large discrepancies suggest that the recursive assumption may be misspecified, and the data-driven ICA identification should be preferred.\n\n","category":"section"},{"location":"examples/#Example-6:-Bayesian-VAR-with-Minnesota-Prior","page":"Examples","title":"Example 6: Bayesian VAR with Minnesota Prior","text":"This example demonstrates Bayesian estimation with automatic hyperparameter optimization.","category":"section"},{"location":"examples/#Hyperparameter-Optimization","page":"Examples","title":"Hyperparameter Optimization","text":"using MacroEconometricModels\n\n# Find optimal shrinkage using marginal likelihood (Giannone et al. 2015)\nprintln(\"Optimizing hyperparameters...\")\nbest_hyper = optimize_hyperparameters(Y, p; grid_size=20)\n\nprintln(\"Optimal hyperparameters:\")\nprintln(\"  τ (overall tightness): \", round(best_hyper.tau, digits=4))\nprintln(\"  d (lag decay): \", best_hyper.d)\n\nThe optimal tau value reflects the degree of shrinkage that maximizes the marginal likelihood. A small tau (e.g., 0.05) means strong shrinkage toward the random walk prior, appropriate for large systems or short samples. A larger tau (e.g., 0.5-1.0) allows the data more influence, appropriate when the sample is informative relative to the model complexity.","category":"section"},{"location":"examples/#BVAR-Estimation-with-MCMC","page":"Examples","title":"BVAR Estimation with MCMC","text":"# Estimate BVAR with optimized Minnesota prior\nprintln(\"\\nEstimating BVAR with MCMC...\")\nchain = estimate_bvar(Y, p;\n    n_samples = 2000,\n    n_adapts = 500,\n    prior = :minnesota,\n    hyper = best_hyper\n)\n\n# Posterior summary (coefficients from first equation)\nprintln(\"\\nPosterior summary for GDP equation:\")\n# Access posterior draws and compute statistics","category":"section"},{"location":"examples/#Bayesian-IRF-with-Credible-Intervals","page":"Examples","title":"Bayesian IRF with Credible Intervals","text":"# Bayesian IRF with Cholesky identification\nbirf_chol = irf(chain, p, n, H; method=:cholesky)\n\n# Extract median and 68% credible intervals\n# birf_chol.quantiles is (H+1) × n × n × 3 array\n# [:, :, :, 1] = 16th percentile\n# [:, :, :, 2] = median\n# [:, :, :, 3] = 84th percentile\n\nprintln(\"\\nBayesian IRF of GDP to own shock:\")\nfor h in [0, 4, 8, 12, 20]\n    med = round(birf_chol.quantiles[h+1, 1, 1, 2], digits=3)\n    lo = round(birf_chol.quantiles[h+1, 1, 1, 1], digits=3)\n    hi = round(birf_chol.quantiles[h+1, 1, 1, 3], digits=3)\n    println(\"  h=$h: $med [$lo, $hi]\")\nend","category":"section"},{"location":"examples/#Bayesian-Sign-Restrictions","page":"Examples","title":"Bayesian Sign Restrictions","text":"# Bayesian IRF with sign restrictions\nbirf_sign = irf(chain, p, n, H;\n    method = :sign,\n    check_func = check_demand_shock\n)\n\nprintln(\"\\nBayesian sign-restricted demand shock → GDP:\")\nfor h in [0, 4, 8, 12]\n    med = round(birf_sign.quantiles[h+1, 1, 1, 2], digits=3)\n    lo = round(birf_sign.quantiles[h+1, 1, 1, 1], digits=3)\n    hi = round(birf_sign.quantiles[h+1, 1, 1, 3], digits=3)\n    println(\"  h=$h: $med [$lo, $hi]\")\nend\n\n","category":"section"},{"location":"examples/#Example-4:-Local-Projections","page":"Examples","title":"Example 4: Local Projections","text":"This example demonstrates various LP methods for estimating impulse responses.","category":"section"},{"location":"examples/#Standard-Local-Projection","page":"Examples","title":"Standard Local Projection","text":"using MacroEconometricModels\n\n# Estimate LP-IRF with Newey-West standard errors\nH = 20\nshock_var = 1  # GDP as the shock variable\n\nlp_model = estimate_lp(Y, shock_var, H;\n    lags = 4,\n    cov_type = :newey_west,\n    bandwidth = 0  # Automatic bandwidth selection\n)\n\n# Extract IRF with confidence intervals\nlp_result = lp_irf(lp_model; conf_level = 0.95)\n\nprintln(\"LP-IRF of shock to variable 1 → variable 1:\")\nfor h in 0:4:H\n    val = round(lp_result.values[h+1, 1], digits=3)\n    se = round(lp_result.se[h+1, 1], digits=3)\n    println(\"  h=$h: $val (SE: $se)\")\nend","category":"section"},{"location":"examples/#LP-with-Instrumental-Variables","page":"Examples","title":"LP with Instrumental Variables","text":"# Generate external instrument (e.g., monetary policy shock proxy)\nRandom.seed!(123)\nZ = 0.5 * Y[:, 3] + randn(T, 1)  # Correlated with rate but exogenous\n\n# Estimate LP-IV\nshock_var = 3  # Instrument for rate shock\nlpiv_model = estimate_lp_iv(Y, shock_var, Z, H;\n    lags = 4,\n    cov_type = :newey_west\n)\n\n# Check instrument strength\nweak_test = weak_instrument_test(lpiv_model; threshold = 10.0)\nprintln(\"\\nFirst-stage F-statistics by horizon:\")\nfor h in 0:4:H\n    F = round(weak_test.F_stats[h+1], digits=2)\n    status = F > 10 ? \"✓\" : \"⚠ weak\"\n    println(\"  h=$h: F=$F $status\")\nend\nprintln(\"All horizons pass F>10: \", weak_test.passes_threshold)\n\n# Extract IRF\nlpiv_result = lp_iv_irf(lpiv_model)","category":"section"},{"location":"examples/#Smooth-Local-Projection","page":"Examples","title":"Smooth Local Projection","text":"# Estimate smooth LP with B-splines\nsmooth_model = estimate_smooth_lp(Y, 1, H;\n    degree = 3,      # Cubic splines\n    n_knots = 4,     # Interior knots\n    lambda = 1.0,    # Smoothing parameter\n    lags = 4\n)\n\n# Cross-validate lambda\noptimal_lambda = cross_validate_lambda(Y, 1, H;\n    lambda_grid = 10.0 .^ (-4:0.5:2),\n    k_folds = 5\n)\nprintln(\"\\nOptimal smoothing parameter: \", round(optimal_lambda, digits=4))\n\n# Compare standard vs smooth LP\ncomparison = compare_smooth_lp(Y, 1, H; lambda = optimal_lambda)\nprintln(\"Variance reduction ratio: \", round(comparison.variance_reduction, digits=3))","category":"section"},{"location":"examples/#State-Dependent-Local-Projection","page":"Examples","title":"State-Dependent Local Projection","text":"# Construct state variable (moving average of GDP growth)\ngdp_level = cumsum(Y[:, 1])  # Integrate growth to get level\ngdp_growth = [NaN; diff(gdp_level)]\n\n# 4-period moving average, standardized\nstate_var = zeros(T)\nfor t in 4:T\n    state_var[t] = mean(Y[t-3:t, 1])\nend\nstate_var = (state_var .- mean(state_var[4:end])) ./ std(state_var[4:end])\n\n# Estimate state-dependent LP\nstate_model = estimate_state_lp(Y, 1, state_var, H;\n    gamma = 1.5,           # Transition speed\n    threshold = :median,    # Threshold at median\n    lags = 4\n)\n\n# Extract regime-specific IRFs\nirf_both = state_irf(state_model; regime = :both)\n\nprintln(\"\\nState-dependent IRFs (shock 1 → variable 1):\")\nprintln(\"Expansion vs Recession comparison:\")\nfor h in [0, 4, 8, 12]\n    exp_val = round(irf_both.expansion.values[h+1, 1], digits=3)\n    rec_val = round(irf_both.recession.values[h+1, 1], digits=3)\n    diff = round(exp_val - rec_val, digits=3)\n    println(\"  h=$h: Expansion=$exp_val, Recession=$rec_val, Diff=$diff\")\nend\n\n# Test for regime differences\ndiff_test = test_regime_difference(state_model)\nprintln(\"\\nJoint test for regime differences:\")\nprintln(\"  Average |t|: \", round(diff_test.joint_test.avg_t_stat, digits=2))\nprintln(\"  p-value: \", round(diff_test.joint_test.p_value, digits=4))\n\n","category":"section"},{"location":"examples/#Example-5:-Factor-Model-for-Large-Panels","page":"Examples","title":"Example 5: Factor Model for Large Panels","text":"This example demonstrates factor extraction and selection from a large macroeconomic panel.","category":"section"},{"location":"examples/#Simulate-Large-Panel-Data","page":"Examples","title":"Simulate Large Panel Data","text":"using MacroEconometricModels\nusing Random\nusing Statistics\n\nRandom.seed!(42)\n\n# Panel dimensions\nT = 150   # Time periods\nN = 50    # Variables\nr_true = 3  # True number of factors\n\n# Generate true factors (with persistence)\nF_true = zeros(T, r_true)\nfor j in 1:r_true\n    F_true[1, j] = randn()\n    for t in 2:T\n        F_true[t, j] = 0.8 * F_true[t-1, j] + 0.3 * randn()\n    end\nend\n\n# Factor loadings (sparse structure)\nΛ_true = randn(N, r_true)\n# Make first 15 vars load strongly on factor 1, etc.\nΛ_true[1:15, 1] .*= 2\nΛ_true[16:30, 2] .*= 2\nΛ_true[31:45, 3] .*= 2\n\n# Generate panel\nX = F_true * Λ_true' + 0.5 * randn(T, N)\n\nprintln(\"Panel: T=$T, N=$N, true r=$r_true\")","category":"section"},{"location":"examples/#Determine-Number-of-Factors","page":"Examples","title":"Determine Number of Factors","text":"# Bai-Ng information criteria\nr_max = 10\nic = ic_criteria(X, r_max)\n\nprintln(\"\\nBai-Ng information criteria:\")\nprintln(\"  IC1 selects: \", ic.r_IC1, \" factors\")\nprintln(\"  IC2 selects: \", ic.r_IC2, \" factors\")\nprintln(\"  IC3 selects: \", ic.r_IC3, \" factors\")\nprintln(\"  (True: $r_true factors)\")\n\n# IC values for each r\nprintln(\"\\nIC values by number of factors:\")\nfor r in 1:r_max\n    println(\"  r=$r: IC1=$(round(ic.IC1[r], digits=4)), IC2=$(round(ic.IC2[r], digits=4))\")\nend","category":"section"},{"location":"examples/#Estimate-Factor-Model","page":"Examples","title":"Estimate Factor Model","text":"# Use IC2's recommendation\nr_opt = ic.r_IC2\n\n# Estimate factor model\nfm = estimate_factors(X, r_opt; standardize = true)\n\nprintln(\"\\nEstimated factor model:\")\nprintln(\"  Number of factors: \", fm.r)\nprintln(\"  Factors dimension: \", size(fm.factors))\nprintln(\"  Loadings dimension: \", size(fm.loadings))\n\n# Variance explained\nprintln(\"\\nVariance explained:\")\nfor j in 1:r_opt\n    pct = round(fm.explained_variance[j] * 100, digits=1)\n    cum = round(fm.cumulative_variance[j] * 100, digits=1)\n    println(\"  Factor $j: $pct% (cumulative: $cum%)\")\nend","category":"section"},{"location":"examples/#Model-Diagnostics","page":"Examples","title":"Model Diagnostics","text":"# R² for each variable\nr2_vals = r2(fm)\n\nprintln(\"\\nR² statistics:\")\nprintln(\"  Mean: \", round(mean(r2_vals), digits=3))\nprintln(\"  Median: \", round(median(r2_vals), digits=3))\nprintln(\"  Min: \", round(minimum(r2_vals), digits=3))\nprintln(\"  Max: \", round(maximum(r2_vals), digits=3))\n\n# Variables well-explained (R² > 0.5)\nwell_explained = sum(r2_vals .> 0.5)\nprintln(\"  Variables with R² > 0.5: $well_explained / $N\")\n\n# Factor-true factor correlation (up to rotation)\nprintln(\"\\nFactor recovery (correlation with true factors):\")\nfor j in 1:r_opt\n    cors = [abs(cor(fm.factors[:, j], F_true[:, k])) for k in 1:r_true]\n    best_match = argmax(cors)\n    println(\"  Estimated factor $j matches true factor $best_match: r=$(round(cors[best_match], digits=3))\")\nend\n\nThe Bai-Ng information criteria select the number of factors by balancing fit against complexity. IC2 tends to perform best in simulations. High correlations between estimated and true factors (above 0.9) confirm reliable factor recovery. The R² values show how well the common factors explain each variable; variables with low R² are primarily driven by idiosyncratic shocks and contribute less to the common component.","category":"section"},{"location":"examples/#Factor-Model-Forecasting","page":"Examples","title":"Factor Model Forecasting","text":"# Forecast 12 steps ahead with theoretical (analytical) CIs\nfc = forecast(fm, 12; ci_method=:theoretical, conf_level=0.95)\n\nprintln(\"\\nFactor forecast with 95% CIs:\")\nprintln(\"  Factors: \", size(fc.factors))        # 12×r\nprintln(\"  Observables: \", size(fc.observables)) # 12×N\nprintln(\"  CI method: \", fc.ci_method)\n\n# SEs should increase with horizon (growing uncertainty)\nprintln(\"\\nFactor 1 SE by horizon:\")\nfor h in [1, 4, 8, 12]\n    println(\"  h=$h: SE=$(round(fc.factors_se[h, 1], digits=4))\")\nend\n\n# Bootstrap CIs (non-parametric, no Gaussian assumption)\nfc_boot = forecast(fm, 12; ci_method=:bootstrap, n_boot=500, conf_level=0.90)\n\nprintln(\"\\nBootstrap vs theoretical CI widths (Factor 1, h=12):\")\nwidth_theory = fc.factors_upper[12, 1] - fc.factors_lower[12, 1]\nwidth_boot = fc_boot.factors_upper[12, 1] - fc_boot.factors_lower[12, 1]\nprintln(\"  Theoretical: \", round(width_theory, digits=3))\nprintln(\"  Bootstrap: \", round(width_boot, digits=3))\n\nThe theoretical SEs grow monotonically with the forecast horizon for stationary factor dynamics, reflecting accumulating forecast uncertainty. Bootstrap CIs are useful when factor innovations may be non-Gaussian or exhibit conditional heteroskedasticity.","category":"section"},{"location":"examples/#Dynamic-Factor-Model-Forecasting","page":"Examples","title":"Dynamic Factor Model Forecasting","text":"# Estimate DFM with VAR(2) factor dynamics\ndfm = estimate_dynamic_factors(X, r_opt, 2)\n\n# Forecast with all CI methods\nfc_none = forecast(dfm, 12)                                    # Point only\nfc_theo = forecast(dfm, 12; ci_method=:theoretical)            # Analytical CIs\nfc_boot = forecast(dfm, 12; ci_method=:bootstrap, n_boot=500)  # Bootstrap CIs\nfc_sim  = forecast(dfm, 12; ci_method=:simulation, n_boot=500) # Simulation CIs\n\nprintln(\"\\nDFM forecast comparison (Observable 1, h=12):\")\nprintln(\"  Point forecast: \", round(fc_none.observables[12, 1], digits=3))\nprintln(\"  Theoretical CI: [\", round(fc_theo.observables_lower[12, 1], digits=3),\n        \", \", round(fc_theo.observables_upper[12, 1], digits=3), \"]\")\nprintln(\"  Bootstrap CI:   [\", round(fc_boot.observables_lower[12, 1], digits=3),\n        \", \", round(fc_boot.observables_upper[12, 1], digits=3), \"]\")\n\nThe DFM supports four CI methods: :theoretical (fastest, assumes Gaussian innovations), :bootstrap (residual resampling), :simulation (full Monte Carlo draws), and the legacy ci=true interface which maps to :simulation.\n\n","category":"section"},{"location":"examples/#Example-9:-GMM-Estimation","page":"Examples","title":"Example 9: GMM Estimation","text":"This example demonstrates GMM estimation of a simple model with moment conditions.","category":"section"},{"location":"examples/#Define-Moment-Conditions","page":"Examples","title":"Define Moment Conditions","text":"using MacroEconometricModels\n\n# Example: IV regression via GMM\n# Model: y = x'β + ε\n# Moment conditions: E[z(y - x'β)] = 0\n\n# Generate data with endogeneity\nRandom.seed!(42)\nn_obs = 500\nn_params = 2\n\n# Instruments\nZ = randn(n_obs, 3)\n\n# Endogenous regressor (correlated with error)\nu = randn(n_obs)\nX = hcat(ones(n_obs), Z[:, 1] + 0.5 * u + 0.2 * randn(n_obs))\n\n# Outcome\nβ_true = [1.0, 2.0]\nY = X * β_true + u\n\n# Data bundle\ndata = (Y = Y, X = X, Z = hcat(ones(n_obs), Z))\n\n# Moment function: E[Z'(Y - Xβ)] = 0\nfunction moment_conditions(theta, data)\n    residuals = data.Y - data.X * theta\n    data.Z .* residuals  # n_obs × n_moments matrix\nend","category":"section"},{"location":"examples/#GMM-Estimation","page":"Examples","title":"GMM Estimation","text":"# Initial values\ntheta0 = zeros(n_params)\n\n# Two-step efficient GMM\ngmm_result = estimate_gmm(moment_conditions, theta0, data;\n    weighting = :two_step,\n    hac = true\n)\n\nprintln(\"GMM Estimation Results:\")\nprintln(\"  True β: \", β_true)\nprintln(\"  Estimated β: \", round.(gmm_result.theta, digits=4))\nprintln(\"  Converged: \", gmm_result.converged)\nprintln(\"  Iterations: \", gmm_result.iterations)\n\n# Standard errors\nse = sqrt.(diag(gmm_result.vcov))\nprintln(\"\\n  Standard errors: \", round.(se, digits=4))\n\n# Confidence intervals\nz = 1.96\nfor i in 1:n_params\n    lo = round(gmm_result.theta[i] - z * se[i], digits=4)\n    hi = round(gmm_result.theta[i] + z * se[i], digits=4)\n    println(\"  β[$i]: 95% CI = [$lo, $hi]\")\nend","category":"section"},{"location":"examples/#J-Test-for-Overidentification","page":"Examples","title":"J-Test for Overidentification","text":"# Test overidentifying restrictions\nj_result = j_test(gmm_result)\n\nprintln(\"\\nHansen J-test:\")\nprintln(\"  J-statistic: \", round(j_result.J_stat, digits=4))\nprintln(\"  Degrees of freedom: \", j_result.df)\nprintln(\"  p-value: \", round(j_result.p_value, digits=4))\nprintln(\"  Reject at 5%: \", j_result.reject_05)\n\nThe GMM estimates should be close to the true values beta = 10 20 when instruments are valid and strong. The standard errors from two-step efficient GMM are asymptotically optimal. The Hansen J-test evaluates whether the moment conditions are jointly satisfied: a large p-value (failing to reject) indicates that the instruments are valid and the model is correctly specified. Rejection suggests either invalid instruments or model misspecification.\n\n","category":"section"},{"location":"examples/#Example-10:-Complete-Workflow","page":"Examples","title":"Example 10: Complete Workflow","text":"This example shows a complete empirical workflow combining multiple techniques.\n\nusing MacroEconometricModels\nusing Random\nusing Statistics\n\nRandom.seed!(2024)\n\n# === Step 1: Data Preparation ===\nT, n = 200, 4\nY = randn(T, n)\nfor t in 2:T\n    Y[t, :] = 0.6 * Y[t-1, :] + 0.3 * randn(n)\nend\nvar_names = [\"Output\", \"Inflation\", \"Rate\", \"Exchange Rate\"]\n\n# === Step 2: Lag Selection ===\nprintln(\"=\"^50)\nprintln(\"Step 1: Lag Selection\")\nprintln(\"=\"^50)\n\naics = Float64[]\nbics = Float64[]\nfor p in 1:8\n    m = fit(VARModel, Y, p)\n    push!(aics, aic(m))\n    push!(bics, bic(m))\nend\np_aic = argmin(aics)\np_bic = argmin(bics)\nprintln(\"AIC selects p=$p_aic, BIC selects p=$p_bic\")\np = p_bic  # Use BIC's conservative choice\n\n# === Step 3: VAR Estimation ===\nprintln(\"\\n\" * \"=\"^50)\nprintln(\"Step 2: VAR Estimation\")\nprintln(\"=\"^50)\n\nmodel = fit(VARModel, Y, p)\nprintln(\"Estimated VAR($p)\")\nprintln(\"Log-likelihood: \", round(loglikelihood(model), digits=2))\n\n# === Step 4: Frequentist IRF ===\nprintln(\"\\n\" * \"=\"^50)\nprintln(\"Step 3: Impulse Response Analysis\")\nprintln(\"=\"^50)\n\nH = 20\nirfs = irf(model, H; method=:cholesky)\nfevd_res = fevd(model, H; method=:cholesky)\n\n# === Step 5: Bayesian Estimation ===\nprintln(\"\\n\" * \"=\"^50)\nprintln(\"Step 4: Bayesian Analysis\")\nprintln(\"=\"^50)\n\n# Optimize priors\nbest_hyper = optimize_hyperparameters(Y, p; grid_size=15)\nprintln(\"Optimal τ: \", round(best_hyper.tau, digits=4))\n\n# BVAR with MCMC\nchain = estimate_bvar(Y, p; n_samples=1000, n_adapts=300,\n                      prior=:minnesota, hyper=best_hyper)\n\n# Bayesian IRF\nbirf = irf(chain, p, n, H; method=:cholesky)\n\n# === Step 6: Local Projections Comparison ===\nprintln(\"\\n\" * \"=\"^50)\nprintln(\"Step 5: LP vs VAR Comparison\")\nprintln(\"=\"^50)\n\nlp_model = estimate_lp(Y, 1, H; lags=p, cov_type=:newey_west)\nlp_result = lp_irf(lp_model)\n\nprintln(\"IRF(1→1) at h=0:\")\nprintln(\"  VAR: \", round(irfs.irf[1, 1, 1], digits=3))\nprintln(\"  LP: \", round(lp_result.values[1, 1], digits=3))\n\nprintln(\"\\nIRF(1→1) at h=8:\")\nprintln(\"  VAR: \", round(irfs.irf[9, 1, 1], digits=3))\nprintln(\"  LP: \", round(lp_result.values[9, 1], digits=3))\n\n# === Step 7: Robustness Check with Smooth LP ===\nsmooth_lp = estimate_smooth_lp(Y, 1, H; lambda=1.0, lags=p)\nsmooth_result = smooth_lp_irf(smooth_lp)\n\nprintln(\"\\nSmooth LP variance reduction: \",\n        round(mean(smooth_result.se.^2) / mean(lp_result.se.^2), digits=3))\n\nprintln(\"\\n\" * \"=\"^50)\nprintln(\"Analysis Complete!\")\nprintln(\"=\"^50)\n\nComparing VAR and LP impulse responses at the same horizon provides a robustness check. Under correct specification, both estimators are consistent for the same causal parameter (Plagborg-Møller & Wolf, 2021), but LP is less efficient. Large discrepancies suggest potential dynamic misspecification in the VAR. The smooth LP variance reduction ratio measures efficiency gains from B-spline regularization; values well below 1.0 indicate substantial noise reduction from imposing smoothness.\n\n","category":"section"},{"location":"examples/#Example-11:-Table-Output-—-Text,-LaTeX,-and-HTML","page":"Examples","title":"Example 11: Table Output — Text, LaTeX, and HTML","text":"All show, print_table, and Base.show methods in MacroEconometricModels route through a unified PrettyTables backend. Switching from terminal text to LaTeX or HTML output requires a single call to set_display_backend. This is useful for embedding results directly into papers (LaTeX), slides (HTML), or reports.","category":"section"},{"location":"examples/#Setup","page":"Examples","title":"Setup","text":"using MacroEconometricModels\nusing Random\n\nRandom.seed!(42)\n\n# Estimate a VAR and compute IRFs + FEVD\nY = randn(200, 3)\nfor t in 2:200\n    Y[t, :] = [0.8 0.1 0.0; 0.05 0.7 0.0; 0.1 0.2 0.75] * Y[t-1, :] + 0.3 * randn(3)\nend\n\nmodel = estimate_var(Y, 2)\nirfs = irf(model, 12; method=:cholesky, ci_type=:bootstrap, n_boot=500)\nfevd_result = fevd(model, 12; method=:cholesky)","category":"section"},{"location":"examples/#Text-Output-(Default)","page":"Examples","title":"Text Output (Default)","text":"The default backend is :text, producing terminal-friendly borderless tables:\n\n# Confirm default backend\nget_display_backend()   # :text\n\n# Print IRF table for variable 1, shock 1\nprint_table(irfs, 1, 1)\n\nOutput:\n\n           IRF: Var 1 ← Shock 1\n  ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n      h      IRF    CI_lo    CI_hi\n  ────────────────────────────────\n      1   1.0000   1.0000   1.0000\n      4   0.5765   0.3821   0.7542\n      8   0.2134   0.0512   0.3891\n     12   0.0712  -0.0203   0.1744\n\n# Print FEVD table for variable 2\nprint_table(fevd_result, 2)","category":"section"},{"location":"examples/#LaTeX-Output-for-Papers","page":"Examples","title":"LaTeX Output for Papers","text":"Switch to LaTeX to get tables ready for \\input{} in your .tex file:\n\n# Switch to LaTeX backend\nset_display_backend(:latex)\n\n# Print IRF table — output is now LaTeX\nprint_table(irfs, 1, 1)\n\nOutput:\n\n\\begin{table}\n  \\caption{IRF: Var 1 ← Shock 1}\n  \\begin{tabular}{rrrr}\n    \\hline\n    h & IRF & CI\\_lo & CI\\_hi \\\\\n    \\hline\n    1 & 1.0 & 1.0 & 1.0 \\\\\n    4 & 0.5765 & 0.3821 & 0.7542 \\\\\n    8 & 0.2134 & 0.0512 & 0.3891 \\\\\n    12 & 0.0712 & -0.0203 & 0.1744 \\\\\n    \\hline\n  \\end{tabular}\n\\end{table}\n\nTo save LaTeX output directly to a file:\n\nset_display_backend(:latex)\n\n# Write IRF table to file\nopen(\"tables/irf_table.tex\", \"w\") do io\n    print_table(io, irfs, 1, 1)\nend\n\n# Write FEVD table to file\nopen(\"tables/fevd_table.tex\", \"w\") do io\n    print_table(io, fevd_result, 2)\nend\n\nThen in your LaTeX document:\n\n\\begin{document}\nTable~\\ref{tab:irf} reports the impulse responses...\n\\input{tables/irf_table.tex}\n\\end{document}","category":"section"},{"location":"examples/#HTML-Output-for-Slides-and-Web","page":"Examples","title":"HTML Output for Slides and Web","text":"Switch to HTML for Jupyter notebooks, web dashboards, or HTML-based presentations:\n\n# Switch to HTML backend\nset_display_backend(:html)\n\n# Print IRF table — output is now an HTML <table>\nprint_table(irfs, 1, 1)\n\nOutput:\n\n<table>\n  <caption>IRF: Var 1 ← Shock 1</caption>\n  <tr><th>h</th><th>IRF</th><th>CI_lo</th><th>CI_hi</th></tr>\n  <tr><td>1</td><td>1.0</td><td>1.0</td><td>1.0</td></tr>\n  <tr><td>4</td><td>0.5765</td><td>0.3821</td><td>0.7542</td></tr>\n  ...\n</table>\n\nTo save HTML output to a file:\n\nset_display_backend(:html)\n\nopen(\"tables/irf_table.html\", \"w\") do io\n    print_table(io, irfs, 1, 1)\nend","category":"section"},{"location":"examples/#Switching-Backends-in-a-Workflow","page":"Examples","title":"Switching Backends in a Workflow","text":"You can switch backends freely within a session. A common pattern for a research workflow:\n\nusing MacroEconometricModels\nusing Random\n\nRandom.seed!(42)\n\nY = randn(200, 3)\nfor t in 2:200\n    Y[t, :] = [0.8 0.1 0.0; 0.05 0.7 0.0; 0.1 0.2 0.75] * Y[t-1, :] + 0.3 * randn(3)\nend\n\nmodel = estimate_var(Y, 2)\nH = 20\nirfs = irf(model, H; method=:cholesky, ci_type=:bootstrap, n_boot=500)\nfevd_result = fevd(model, H; method=:cholesky)\nhd_result = historical_decomposition(model)\n\n# === Step 1: Inspect in terminal ===\nset_display_backend(:text)\nprint_table(irfs, 1, 1)       # Quick look at IRF\nprint_table(fevd_result, 1)    # Quick look at FEVD\n\n# === Step 2: Export LaTeX for the paper ===\nset_display_backend(:latex)\n\nopen(\"tables/irf_gdp.tex\", \"w\") do io\n    print_table(io, irfs, 1, 1; horizons=[1, 4, 8, 12, 20])\nend\n\nopen(\"tables/fevd_gdp.tex\", \"w\") do io\n    print_table(io, fevd_result, 1; horizons=[1, 4, 8, 12, 20])\nend\n\n# === Step 3: Export HTML for slides ===\nset_display_backend(:html)\n\nopen(\"slides/irf_gdp.html\", \"w\") do io\n    print_table(io, irfs, 1, 1)\nend\n\n# === Step 4: Reset to text for continued interactive work ===\nset_display_backend(:text)","category":"section"},{"location":"examples/#Using-table()-to-Extract-Raw-Data","page":"Examples","title":"Using table() to Extract Raw Data","text":"The table() function returns a plain Matrix that you can manipulate, pass to DataFrames, or export with CSV:\n\nusing DataFrames, CSV\n\n# Extract IRF as a matrix: columns are [h, IRF, CI_lo, CI_hi]\nirf_data = table(irfs, 1, 1; horizons=[1, 4, 8, 12, 20])\n\n# Convert to DataFrame\ndf = DataFrame(irf_data, [:h, :IRF, :CI_lo, :CI_hi])\n\n# Save as CSV\nCSV.write(\"tables/irf_data.csv\", df)\n\n# Extract FEVD as a matrix: columns are [h, Shock1, Shock2, ..., ShockN]\nfevd_data = table(fevd_result, 1; horizons=[1, 4, 8, 12, 20])","category":"section"},{"location":"examples/#Backend-Affects-show()-Too","page":"Examples","title":"Backend Affects show() Too","text":"The display backend also controls how objects render when printed in the REPL or displayed in Jupyter:\n\nset_display_backend(:latex)\n\n# REPL display is now LaTeX\nmodel    # VARModel show → LaTeX tables\nirfs     # ImpulseResponse show → LaTeX tables\n\nset_display_backend(:text)  # Reset\n\nThis means in a Jupyter notebook, you can set the backend to :html once at the top:\n\n# Top of Jupyter notebook\nusing MacroEconometricModels\nset_display_backend(:html)\n\n# All subsequent cells render as formatted HTML tables\nmodel = estimate_var(Y, 2)\nirfs = irf(model, 12; method=:cholesky)\nirfs   # Displays as an HTML table","category":"section"},{"location":"examples/#Summary-of-Output-Functions","page":"Examples","title":"Summary of Output Functions","text":"Function Returns Use Case\ntable(result, ...) Matrix Raw numeric data for custom processing, CSV export\nprint_table([io], result, ...) Nothing (prints) Formatted output via current backend (text/LaTeX/HTML)\nshow(io, result) Nothing (prints) REPL display, also respects backend\nset_display_backend(:text) Nothing Terminal output (default)\nset_display_backend(:latex) Nothing LaTeX \\begin{tabular} output\nset_display_backend(:html) Nothing HTML <table> output\nget_display_backend() Symbol Check current backend\n\n","category":"section"},{"location":"examples/#Example-12:-Bibliographic-References","page":"Examples","title":"Example 12: Bibliographic References","text":"The refs() function returns bibliographic references for any model, result type, or identification method. References are available in four formats: AEA text (default), BibTeX, LaTeX \\bibitem, and HTML with clickable DOI links.","category":"section"},{"location":"examples/#Basic-Usage","page":"Examples","title":"Basic Usage","text":"using MacroEconometricModels\nusing Random\n\nRandom.seed!(42)\n\n# Estimate a model\nY = randn(200, 3)\nfor t in 2:200\n    Y[t, :] = 0.5 * Y[t-1, :] + 0.3 * randn(3)\nend\nmodel = estimate_var(Y, 2)\n\n# Get references for this model type (AEA text format)\nrefs(model)\n\nOutput:\n\nSims, Christopher A. 1980. \"Macroeconomics and Reality.\" Econometrica 48 (1): 1-48.\nLutkepohl, Helmut. 2005. New Introduction to Multiple Time Series Analysis. Berlin: Springer.","category":"section"},{"location":"examples/#Multiple-Output-Formats","page":"Examples","title":"Multiple Output Formats","text":"# BibTeX format — paste into your .bib file\nrefs(model; format=:bibtex)\n\n# LaTeX \\bibitem format\nrefs(model; format=:latex)\n\n# HTML with clickable DOI links\nrefs(model; format=:html)","category":"section"},{"location":"examples/#References-by-Method-Name","page":"Examples","title":"References by Method Name","text":"# References for identification methods\nrefs(:cholesky)       # Cholesky decomposition\nrefs(:fastica)        # FastICA for SVAR\nrefs(:sign)           # Sign restrictions\nrefs(:johansen)       # Johansen cointegration\nrefs(:garch)          # GARCH models\n\n# References for specific result types\ngarch = estimate_garch(randn(500), 1, 1)\nrefs(garch)           # Bollerslev (1986)\n\nsv = estimate_sv(randn(500); n_samples=500, n_adapts=200)\nrefs(sv)              # Taylor (1986)","category":"section"},{"location":"examples/#Export-to-.bib-File","page":"Examples","title":"Export to .bib File","text":"# Write BibTeX entries for all models used in your analysis\nopen(\"references.bib\", \"w\") do io\n    refs(io, model; format=:bibtex)\n    println(io)\n    refs(io, :fastica; format=:bibtex)\n    println(io)\n    refs(io, :johansen; format=:bibtex)\nend\n\nThe refs() function covers all 45+ references in the package's database, including every estimation method, identification scheme, and test. This ensures correct citation of the methods used in your empirical analysis.\n\n","category":"section"},{"location":"examples/#Best-Practices","page":"Examples","title":"Best Practices","text":"","category":"section"},{"location":"examples/#Data-Preparation","page":"Examples","title":"Data Preparation","text":"Stationarity: Test for unit roots using ADF and KPSS together\nBoth fail to reject → inconclusive, consider structural breaks\nADF rejects, KPSS doesn't → stationary (I(0))\nADF doesn't reject, KPSS rejects → unit root (I(1))\nStructural Breaks: Use Zivot-Andrews test if visual inspection suggests breaks\nCointegration: For I(1) variables, test for cointegration before differencing\nOutliers: Check for and handle outliers\nMissing data: Factor models can handle some missing data; VARs require complete data\nScaling: For factor models, standardize variables","category":"section"},{"location":"examples/#Model-Selection","page":"Examples","title":"Model Selection","text":"Lag length: Use information criteria (BIC is more conservative)\nNumber of factors: Use Bai-Ng criteria; prefer IC2 or IC3\nPrior tightness: Optimize via marginal likelihood for large models","category":"section"},{"location":"examples/#Identification","page":"Examples","title":"Identification","text":"Economic theory: Base restrictions on economic reasoning\nRobustness: Try multiple identification schemes\nNarrative: Use historical knowledge when available\nNon-Gaussian: Test residuals with normality_test_suite first; if non-Gaussian, ICA/ML methods provide ordering-free identification\nSpecification tests: Validate non-Gaussian identification with test_shock_gaussianity and test_shock_independence","category":"section"},{"location":"examples/#Inference","page":"Examples","title":"Inference","text":"HAC standard errors: Always use for LP at horizons > 0\nCredible intervals: Report 68% and 90% bands for Bayesian\nBootstrap: Use for frequentist VAR confidence intervals","category":"section"},{"location":"examples/#Reporting","page":"Examples","title":"Reporting","text":"Present both: VAR and LP estimates as robustness check\nHorizon selection: Focus on economically meaningful horizons\nFEVD: Report at multiple horizons (short, medium, long-run)\nLaTeX export: Use set_display_backend(:latex) then print_table(io, ...) for paper-ready tables\nHTML export: Use set_display_backend(:html) for Jupyter notebooks and web reports\nRaw data: Use table(result, ...) to extract matrices for custom formatting or CSV export","category":"section"},{"location":"arima/#Univariate-Time-Series-(ARIMA)","page":"ARIMA","title":"Univariate Time Series (ARIMA)","text":"This chapter covers univariate ARIMA-class models: AR, MA, ARMA, and ARIMA. These models are fundamental building blocks for time series analysis, forecasting, and as diagnostic tools for checking residual autocorrelation in multivariate models.","category":"section"},{"location":"arima/#Introduction","page":"ARIMA","title":"Introduction","text":"Univariate time series models capture temporal dependence through autoregressive (AR) and moving average (MA) components. The general ARIMA(p,d,q) model nests:\n\nAR(p): Autoregressive model — current value depends on past values\nMA(q): Moving average model — current value depends on past shocks\nARMA(p,q): Combined autoregressive–moving average\nARIMA(p,d,q): Integrated ARMA — models non-stationary series via differencing\n\nReferences: Box & Jenkins (1976), Hamilton (1994, Chapters 3–5), Brockwell & Davis (1991)","category":"section"},{"location":"arima/#Quick-Start","page":"ARIMA","title":"Quick Start","text":"ar = estimate_ar(y, 2)                                    # AR(2) via OLS\nma = estimate_ma(y, 1; method=:css_mle)                   # MA(1) via CSS-MLE\narma = estimate_arma(y, 1, 1)                              # ARMA(1,1)\narima = estimate_arima(y, 1, 1, 0)                         # ARIMA(1,1,0)\nfc = forecast(arma, 12; conf_level=0.95)                   # 12-step forecast with CI\nsel = select_arima_order(y, 4, 4)                          # Grid search for best (p,q)\n\n","category":"section"},{"location":"arima/#The-AR(p)-Model","page":"ARIMA","title":"The AR(p) Model","text":"","category":"section"},{"location":"arima/#Model-Specification","page":"ARIMA","title":"Model Specification","text":"An autoregressive model of order p is:\n\ny_t = c + phi_1 y_t-1 + phi_2 y_t-2 + cdots + phi_p y_t-p + varepsilon_t\n\nwhere varepsilon_t sim textWN(0 sigma^2) is white noise. Using the lag operator L:\n\nphi(L) y_t = c + varepsilon_t quad phi(L) = 1 - phi_1 L - phi_2 L^2 - cdots - phi_p L^p","category":"section"},{"location":"arima/#Stationarity","page":"ARIMA","title":"Stationarity","text":"The process is stationary if all roots of the characteristic polynomial phi(z) = 0 lie outside the unit circle, equivalently if all eigenvalues of the companion matrix\n\nF = beginbmatrix\nphi_1  phi_2  cdots  phi_p-1  phi_p \n1  0  cdots  0  0 \n0  1  cdots  0  0 \nvdots  vdots  ddots  vdots  vdots \n0  0  cdots  1  0\nendbmatrix_p times p\n\nhave modulus less than 1: lambda_i(F)  1 for all i.","category":"section"},{"location":"arima/#Estimation","page":"ARIMA","title":"Estimation","text":"AR models support two estimation methods:\n\nOLS (:ols, default): For AR(p), construct the regression:\n\ny_t = beta_0 + beta_1 y_t-1 + cdots + beta_p y_t-p + varepsilon_t\n\nand apply ordinary least squares. This is consistent and asymptotically efficient for stationary AR processes.\n\nMaximum Likelihood (:mle): Maximizes the exact Gaussian log-likelihood via the Kalman filter (see Exact MLE via Kalman Filter below).\n\nusing MacroEconometricModels\n\ny = randn(200)\n\n# OLS estimation (default)\nar_ols = estimate_ar(y, 2)\n\n# MLE estimation\nar_mle = estimate_ar(y, 2; method=:mle)\n\n# Access results\nar_ols.phi      # AR coefficients [φ₁, φ₂]\nar_ols.sigma2   # Innovation variance\nar_ols.aic      # Akaike Information Criterion\nar_ols.bic      # Bayesian Information Criterion","category":"section"},{"location":"arima/#ARModel-Return-Values","page":"ARIMA","title":"ARModel Return Values","text":"Field Type Description\ny Vector{T} Original time series\np Int AR order\nc T Intercept (constant term)\nphi Vector{T} AR coefficients phi_1 ldots phi_p\nsigma2 T Innovation variance hatsigma^2\nresiduals Vector{T} Estimated residuals\nfitted Vector{T} Fitted values\nloglik T Log-likelihood\naic T Akaike Information Criterion\nbic T Bayesian Information Criterion\nmethod Symbol Estimation method (:ols or :mle)\nconverged Bool Convergence indicator\niterations Int Number of optimization iterations\n\nReference: Hamilton (1994, Section 5.2)\n\n","category":"section"},{"location":"arima/#The-MA(q)-Model","page":"ARIMA","title":"The MA(q) Model","text":"","category":"section"},{"location":"arima/#Model-Specification-2","page":"ARIMA","title":"Model Specification","text":"A moving average model of order q is:\n\ny_t = c + varepsilon_t + theta_1 varepsilon_t-1 + theta_2 varepsilon_t-2 + cdots + theta_q varepsilon_t-q\n\nor equivalently:\n\ny_t = c + theta(L) varepsilon_t quad theta(L) = 1 + theta_1 L + theta_2 L^2 + cdots + theta_q L^q","category":"section"},{"location":"arima/#Invertibility","page":"ARIMA","title":"Invertibility","text":"The MA process is invertible if all roots of theta(z) = 0 lie outside the unit circle. Invertibility ensures a unique MA representation and is checked via the companion matrix eigenvalue condition, identical in form to the AR stationarity check.","category":"section"},{"location":"arima/#Estimation-2","page":"ARIMA","title":"Estimation","text":"MA parameters cannot be estimated by OLS. Three methods are available:\n\nCSS (:css): Conditional Sum of Squares — fast, approximate\nMLE (:mle): Exact MLE via Kalman filter\nCSS-MLE (:css_mle, default): CSS initialization followed by MLE refinement\n\nma_model = estimate_ma(y, 1; method=:css_mle)\nma_model.theta   # MA coefficient [θ₁]","category":"section"},{"location":"arima/#MAModel-Return-Values","page":"ARIMA","title":"MAModel Return Values","text":"Field Type Description\ny Vector{T} Original time series\nq Int MA order\nc T Intercept\ntheta Vector{T} MA coefficients theta_1 ldots theta_q\nsigma2 T Innovation variance\nresiduals Vector{T} Estimated residuals\nfitted Vector{T} Fitted values\nloglik T Log-likelihood\naic T AIC\nbic T BIC\nmethod Symbol Estimation method (:css, :mle, :css_mle)\nconverged Bool Convergence indicator\niterations Int Number of optimization iterations\n\n","category":"section"},{"location":"arima/#The-ARMA(p,q)-Model","page":"ARIMA","title":"The ARMA(p,q) Model","text":"","category":"section"},{"location":"arima/#Model-Specification-3","page":"ARIMA","title":"Model Specification","text":"The ARMA(p,q) model combines autoregressive and moving average components:\n\ny_t = c + sum_i=1^p phi_i y_t-i + varepsilon_t + sum_j=1^q theta_j varepsilon_t-j\n\nor in lag-operator form:\n\nphi(L) y_t = c + theta(L) varepsilon_t\n\nThe process is stationary and invertible when all roots of phi(z) and theta(z) lie outside the unit circle, respectively.","category":"section"},{"location":"arima/#Estimation-3","page":"ARIMA","title":"Estimation","text":"The same three methods (:css, :mle, :css_mle) are available. The unified internal pipeline _estimate_arma_internal dispatches to the appropriate method:\n\nCSS: Minimizes the conditional sum of squared residuals using Nelder-Mead. Residuals are computed recursively: hatvarepsilon_t = y_t - c - sum_i phi_i y_t-i - sum_j theta_j hatvarepsilon_t-j\nMLE: Maximizes the exact Gaussian log-likelihood via the Kalman filter using L-BFGS optimization. The variance parameter is optimized on the log scale for unconstrained optimization.\nCSS-MLE (default): Uses CSS estimates to initialize MLE, combining the robustness of CSS with the efficiency of exact MLE.\n\narma_model = estimate_arma(y, 1, 1; method=:css_mle)\narma_model.phi     # AR coefficients\narma_model.theta   # MA coefficients\narma_model.loglik  # Log-likelihood","category":"section"},{"location":"arima/#ARMAModel-Return-Values","page":"ARIMA","title":"ARMAModel Return Values","text":"Field Type Description\ny Vector{T} Original time series\np Int AR order\nq Int MA order\nc T Intercept\nphi Vector{T} AR coefficients phi_1 ldots phi_p\ntheta Vector{T} MA coefficients theta_1 ldots theta_q\nsigma2 T Innovation variance\nresiduals Vector{T} Estimated residuals\nfitted Vector{T} Fitted values\nloglik T Log-likelihood\naic T AIC\nbic T BIC\nmethod Symbol Estimation method\nconverged Bool Convergence indicator\niterations Int Number of iterations\n\nnote: Technical Note\nCSS (Conditional Sum of Squares) is fast but approximate: it conditions on initial residuals being zero, which introduces bias in small samples. MLE via the Kalman filter provides exact inference by properly handling initialization but is computationally more expensive and can be sensitive to starting values. The default :css_mle combines both: CSS provides robust starting values, then MLE refines to the exact optimum. For pure AR models, OLS is equivalent to CSS and is preferred for speed.\n\nReference: Hamilton (1994, Chapter 5), Harvey (1993, Chapter 3)\n\n","category":"section"},{"location":"arima/#The-ARIMA(p,d,q)-Model","page":"ARIMA","title":"The ARIMA(p,d,q) Model","text":"","category":"section"},{"location":"arima/#Model-Specification-4","page":"ARIMA","title":"Model Specification","text":"The ARIMA(p,d,q) model applies d-fold differencing to produce a stationary series, then fits an ARMA(p,q):\n\nphi(L) (1-L)^d y_t = c + theta(L) varepsilon_t\n\nwhere (1-L)^d y_t denotes the d-th difference of y_t. Common cases:\n\nd = 1: Delta y_t = y_t - y_t-1 (first difference, for I(1) series)\nd = 2: Delta^2 y_t = Delta y_t - Delta y_t-1 (second difference, for I(2) series)","category":"section"},{"location":"arima/#Estimation-4","page":"ARIMA","title":"Estimation","text":"The implementation differences the series d times, then estimates ARMA(p,q) on the differenced series using the unified estimation pipeline.\n\n# Random walk with drift\ny = cumsum(randn(200))\n\n# Fit ARIMA(1,1,0) — differenced once, then AR(1)\nmodel = estimate_arima(y, 1, 1, 0)\nmodel.phi    # AR coefficients on differenced series\nmodel.d      # Integration order","category":"section"},{"location":"arima/#ARIMAModel-Return-Values","page":"ARIMA","title":"ARIMAModel Return Values","text":"Field Type Description\ny Vector{T} Original (undifferenced) time series\ny_diff Vector{T} d-fold differenced series\np Int AR order\nd Int Integration order\nq Int MA order\nc T Intercept (on differenced series)\nphi Vector{T} AR coefficients\ntheta Vector{T} MA coefficients\nsigma2 T Innovation variance\nresiduals Vector{T} Estimated residuals\nfitted Vector{T} Fitted values (on differenced scale)\nloglik T Log-likelihood\naic T AIC\nbic T BIC\nmethod Symbol Estimation method\nconverged Bool Convergence indicator\niterations Int Number of iterations\n\n","category":"section"},{"location":"arima/#kalman_mle","page":"ARIMA","title":"Exact MLE via Kalman Filter","text":"","category":"section"},{"location":"arima/#State-Space-Representation","page":"ARIMA","title":"State-Space Representation","text":"For exact maximum likelihood estimation, the ARMA(p,q) model is cast into Harvey's (1993) state-space form:\n\nObservation equation:\n\ny_t = c + Z alpha_t\n\nState equation:\n\nalpha_t+1 = T alpha_t + R eta_t quad eta_t sim N(0 Q)\n\nwhere the state vector alpha_t = a_t a_t-1 ldots a_t-r+1 has dimension r = max(p q+1), and:\n\nZ = 1 theta_1 ldots theta_r-1 is the observation vector\nT is the r times r companion matrix with AR coefficients in the first row\nR = 1 0 ldots 0 is the selection vector\nQ = sigma^2 is the innovation variance","category":"section"},{"location":"arima/#Kalman-Filter-Log-Likelihood","page":"ARIMA","title":"Kalman Filter Log-Likelihood","text":"The log-likelihood is computed via the prediction error decomposition:\n\nell(Theta) = -fracn2 log(2pi) - frac12 sum_t=1^n left( log f_t + fracv_t^2f_t right)\n\nwhere\n\nv_t = y_t - haty_tt-1 is the one-step prediction error\nf_t = Z P_tt-1 Z + H is its variance\nn is the number of observations\nTheta denotes the full parameter vector (phi_1 ldots phi_p theta_1 ldots theta_q sigma^2)\n\nInitialization: Uses the unconditional (stationary) distribution when the system is stable, falling back to diffuse initialization (P_0 = 10^6 I) for non-stationary parameters.\n\nReference: Harvey (1993, Chapters 3–4), Durbin & Koopman (2012, Chapter 2)\n\n","category":"section"},{"location":"arima/#Forecasting","page":"ARIMA","title":"Forecasting","text":"","category":"section"},{"location":"arima/#Point-Forecasts","page":"ARIMA","title":"Point Forecasts","text":"The optimal h-step ahead forecast minimizes mean squared error. For an ARMA(p,q) process, forecasts are computed recursively:\n\nhaty_T+hT = c + sum_i=1^p phi_i haty_T+h-iT + sum_j=1^q theta_j hatvarepsilon_T+h-j\n\nwhere haty_T+kT = y_T+k for k leq 0 and hatvarepsilon_T+k = 0 for k geq 1 (future residuals are set to zero as the best linear predictor).","category":"section"},{"location":"arima/#Forecast-Uncertainty","page":"ARIMA","title":"Forecast Uncertainty","text":"Forecast standard errors are derived from the MA(infty) representation. The psi-weights satisfy:\n\npsi_j = sum_i=1^min(pj) phi_i psi_j-i + theta_j mathbb1(j leq q) quad psi_0 = 1\n\nwhere\n\npsi_j is the j-th coefficient in the MA(infty) representation y_t = sum_j=0^infty psi_j varepsilon_t-j\nphi_i are the AR coefficients (zero for i  p)\ntheta_j are the MA coefficients (zero for j  q)\n\nThe h-step ahead forecast variance is:\n\ntextVar(e_T+hT) = sigma^2 left(1 + psi_1^2 + psi_2^2 + cdots + psi_h-1^2 right)\n\nConfidence intervals are symmetric: haty_T+hT pm z_alpha2 cdot textse_h.","category":"section"},{"location":"arima/#ARIMA-Forecasting","page":"ARIMA","title":"ARIMA Forecasting","text":"For ARIMA(p,d,q) models, forecasts are computed on the differenced series and then integrated back to the original scale. For d = 1:\n\nhaty_T+h = y_T + sum_j=1^h widehatDelta y_T+jT\n\nStandard errors are adjusted for the integration via cumulative variance accumulation.\n\n# Forecast 12 steps ahead\nfc = forecast(model, 12)\nfc.forecast    # Point forecasts\nfc.ci_lower    # Lower 95% confidence bound\nfc.ci_upper    # Upper 95% confidence bound\nfc.se          # Standard errors\n\n# Forecast with different confidence level\nfc99 = forecast(model, 12; conf_level=0.99)","category":"section"},{"location":"arima/#ARIMAForecast-Return-Values","page":"ARIMA","title":"ARIMAForecast Return Values","text":"Field Type Description\nforecast Vector{T} Point forecasts haty_T+1 ldots haty_T+h\nci_lower Vector{T} Lower confidence bound\nci_upper Vector{T} Upper confidence bound\nse Vector{T} Forecast standard errors (from psi-weights)\nhorizon Int Forecast horizon h\nconf_level T Confidence level (e.g., 0.95)\n\n","category":"section"},{"location":"arima/#Order-Selection","page":"ARIMA","title":"Order Selection","text":"","category":"section"},{"location":"arima/#Grid-Search","page":"ARIMA","title":"Grid Search","text":"select_arima_order evaluates all ARMA(p,q) combinations up to specified maxima and selects the best model by AIC or BIC:\n\n# Search over p ∈ {0,...,4}, q ∈ {0,...,4}\nselection = select_arima_order(y, 4, 4)\nselection.best_p_bic    # Optimal AR order (BIC)\nselection.best_q_bic    # Optimal MA order (BIC)\nselection.best_p_aic    # Optimal AR order (AIC)\nselection.best_q_aic    # Optimal MA order (AIC)","category":"section"},{"location":"arima/#ARIMAOrderSelection-Return-Values","page":"ARIMA","title":"ARIMAOrderSelection Return Values","text":"Field Type Description\nbest_p_aic Int Optimal AR order by AIC\nbest_q_aic Int Optimal MA order by AIC\nbest_p_bic Int Optimal AR order by BIC\nbest_q_bic Int Optimal MA order by BIC\naic_matrix Matrix{T} (p_max+1) times (q_max+1) matrix of AIC values\nbic_matrix Matrix{T} (p_max+1) times (q_max+1) matrix of BIC values\nbest_model_aic AbstractARIMAModel Best model by AIC\nbest_model_bic AbstractARIMAModel Best model by BIC","category":"section"},{"location":"arima/#Automatic-Selection","page":"ARIMA","title":"Automatic Selection","text":"auto_arima implements an automatic model selection procedure:\n\nbest_model = auto_arima(y)\nbest_model.p     # Selected AR order\nbest_model.q     # Selected MA order (for ARMA) or d (for ARIMA)","category":"section"},{"location":"arima/#Information-Criteria-Table","page":"ARIMA","title":"Information Criteria Table","text":"ic_table provides a formatted comparison of models:\n\n# Get IC values for a grid of models\ntable = ic_table(y, 3, 3)\n\n","category":"section"},{"location":"arima/#StatsAPI-Interface","page":"ARIMA","title":"StatsAPI Interface","text":"All ARIMA models implement the Julia StatsAPI.RegressionModel interface:\n\nusing StatsAPI\n\nmodel = estimate_arma(y, 1, 1)\n\n# StatsAPI accessors\ncoef(model)         # Coefficient vector\nnobs(model)         # Number of observations\ndof(model)          # Degrees of freedom (number of parameters)\ndof_residual(model) # Residual degrees of freedom\nloglikelihood(model) # Log-likelihood\naic(model)          # AIC\nbic(model)          # BIC\nresiduals(model)    # Residual vector\nfitted(model)       # Fitted values\n\n# StatsAPI fit interface\nmodel = fit(ARModel, y, 2)           # AR(2)\nmodel = fit(MAModel, y, 1)           # MA(1)\nmodel = fit(ARMAModel, y, 1, 1)      # ARMA(1,1)\nmodel = fit(ARIMAModel, y, 1, 1, 1)  # ARIMA(1,1,1)\n\n# Prediction\nyhat = predict(model, 12)  # 12-step point forecasts\n\n","category":"section"},{"location":"arima/#Complete-Example","page":"ARIMA","title":"Complete Example","text":"using MacroEconometricModels\nusing Random\n\nRandom.seed!(123)\n\n# Generate an ARMA(1,1) process\nn = 300\nε = randn(n)\ny = zeros(n)\nϕ, θ = 0.7, -0.4\nfor t in 2:n\n    y[t] = ϕ * y[t-1] + ε[t] + θ * ε[t-1]\nend\n\n# Step 1: Check for unit root\nadf_result = adf_test(y; lags=:aic, regression=:constant)\n# (Should reject → stationary, no differencing needed)\n\n# Step 2: Select ARMA order\nsel = select_arima_order(y, 4, 4)\nprintln(\"Best order: ARMA($(sel.best_p_bic), $(sel.best_q_bic))\")\n\n# Step 3: Estimate the model\nmodel = estimate_arma(y, sel.best_p_bic, sel.best_q_bic)\nprintln(\"φ = $(model.phi), θ = $(model.theta)\")\nprintln(\"σ² = $(model.sigma2)\")\nprintln(\"AIC = $(model.aic), BIC = $(model.bic)\")\n\n# Step 4: Forecast\nfc = forecast(model, 20; conf_level=0.95)\nprintln(\"1-step forecast: $(fc.forecast[1]) ± $(1.96 * fc.se[1])\")\n\n# Step 5: Diagnostics\nprintln(\"Converged: $(model.converged)\")\nprintln(\"Log-likelihood: $(model.loglik)\")\n\n","category":"section"},{"location":"arima/#References","page":"ARIMA","title":"References","text":"Box, George E. P., and Gwilym M. Jenkins. 1976. Time Series Analysis: Forecasting and Control. San Francisco: Holden-Day. ISBN 978-0-816-21104-3.\nBrockwell, Peter J., and Richard A. Davis. 1991. Time Series: Theory and Methods. 2nd ed. New York: Springer. ISBN 978-1-4419-0319-8.\nDurbin, James, and Siem Jan Koopman. 2012. Time Series Analysis by State Space Methods. 2nd ed. Oxford: Oxford University Press. https://doi.org/10.1093/acprof:oso/9780199641178.001.0001\nHamilton, James D. 1994. Time Series Analysis. Princeton, NJ: Princeton University Press. ISBN 978-0-691-04289-3.\nHarvey, Andrew C. 1993. Time Series Models. 2nd ed. Cambridge, MA: MIT Press. ISBN 978-0-262-08224-2.","category":"section"},{"location":"lp/#Local-Projections","page":"Local Projections","title":"Local Projections","text":"This chapter provides a comprehensive treatment of Local Projection (LP) methods for estimating impulse response functions, an alternative to the VAR-based approach that offers greater robustness and flexibility.","category":"section"},{"location":"lp/#Introduction","page":"Local Projections","title":"Introduction","text":"Local Projections, introduced by Jordà (2005), estimate impulse responses by running a series of predictive regressions at each forecast horizon. Unlike VARs, which derive IRFs from a single estimated dynamic system, LPs directly estimate the response at each horizon without imposing the dynamic restrictions inherent in VAR specifications.","category":"section"},{"location":"lp/#Key-Advantages-of-Local-Projections","page":"Local Projections","title":"Key Advantages of Local Projections","text":"Robustness to Misspecification: LPs do not impose the lag structure of VARs, making them robust to dynamic misspecification\nFlexibility: Easy to incorporate nonlinearities, state-dependence, and instrumental variables\nTransparency: Each horizon's estimate is independent, making the source of identification transparent\nInference: Standard regression-based inference applies (with HAC corrections)\n\nReference: Jordà (2005), Plagborg-Møller & Wolf (2021)","category":"section"},{"location":"lp/#Quick-Start","page":"Local Projections","title":"Quick Start","text":"lp = estimate_lp(Y, 1, 20; lags=4, cov_type=:newey_west)               # Standard LP\nlpiv = estimate_lp_iv(Y, 3, Z, 20; lags=4)                             # LP-IV\nslp = estimate_smooth_lp(Y, 1, 20; lambda=1.0, n_knots=4)              # Smooth LP\nsdlp = estimate_state_lp(Y, 1, state, 20; gamma=1.5)                   # State-dependent LP\nplp = estimate_propensity_lp(Y, treatment, covariates, 20)              # Propensity LP\nirf_result = lp_irf(lp; conf_level=0.95)                               # Extract IRF\nstruc = structural_lp(Y, 20; method=:cholesky)                          # Structural LP\nfc = forecast(lp, ones(20); ci_method=:analytical)                      # LP forecast\nlfevd = lp_fevd(struc, 20; method=:r2, bias_correct=true)              # LP-FEVD\n\n","category":"section"},{"location":"lp/#Standard-Local-Projections","page":"Local Projections","title":"Standard Local Projections","text":"","category":"section"},{"location":"lp/#The-LP-Regression","page":"Local Projections","title":"The LP Regression","text":"For each horizon h = 0 1 ldots H, we estimate:\n\ny_it+h = alpha_ih + beta_ih x_t + gamma_ih w_t + varepsilon_it+h\n\nwhere:\n\ny_it+h is the response variable i at time t+h\nx_t is the shock/treatment variable at time t\nw_t is a vector of controls (typically lagged y and x)\nbeta_ih is the impulse response of variable i to shock x at horizon h","category":"section"},{"location":"lp/#Control-Variables","page":"Local Projections","title":"Control Variables","text":"Standard controls include lags of all endogenous variables:\n\nw_t = (y_t-1 y_t-2 ldots y_t-p x_t-1 ldots x_t-p)\n\nThe number of lags p is typically selected using information criteria or set to match the VAR lag order.","category":"section"},{"location":"lp/#Estimation","page":"Local Projections","title":"Estimation","text":"At each horizon h, OLS yields:\n\nhatbeta_h = (XX)^-1 XY_h\n\nwhere\n\nhatbeta_h is the k times 1 OLS coefficient vector at horizon h\nX is the T_eff times k regressor matrix containing the intercept, shock variable, and controls\nY_h is the T_eff times 1 vector of responses at horizon h\nk = 2 + np (intercept + shock + p lags of n variables)","category":"section"},{"location":"lp/#HAC-Standard-Errors","page":"Local Projections","title":"HAC Standard Errors","text":"Since varepsilon_t+h is serially correlated (at least MA(h-1) under the null), we use Newey-West standard errors:\n\nhatV_NW = (XX)^-1 hatS (XX)^-1\n\nwhere\n\nhatV_NW is the HAC variance-covariance matrix of hatbeta_h\nhatS = hatGamma_0 + sum_j=1^m w_j (hatGamma_j + hatGamma_j) is the long-run covariance\nw_j are Bartlett kernel weights, m is the bandwidth (typically h + 1)\n\nReference: Jordà (2005), Newey & West (1987)","category":"section"},{"location":"lp/#Julia-Implementation","page":"Local Projections","title":"Julia Implementation","text":"note: Technical Note\nLP residuals varepsilon_t+h are serially correlated at least MA(h-1) under the null of correct specification, even when the true DGP has i.i.d. errors. This is because overlapping forecast horizons create mechanical dependence. HAC standard errors (Newey-West) are therefore essential for all horizons h  0. The default bandwidth is set to h + 1 following standard practice.\n\nusing MacroEconometricModels\n\n# Data: Y is T×n matrix of variables\n# shock_var is the index of the shock variable\n# Estimate LP-IRF up to horizon H\n\nlp_model = estimate_lp(Y, shock_var, H;\n    lags = 4,                  # Control lags\n    cov_type = :newey_west,    # HAC standard errors\n    bandwidth = 0              # 0 = automatic bandwidth\n)\n\n# Extract IRF with confidence intervals\nirf_result = lp_irf(lp_model; conf_level = 0.95)\n\nThe irf_result.values matrix has dimension (H+1) times n_resp, where each row gives the response at a particular horizon. At h = 0, the coefficient hatbeta_0 captures the contemporaneous (impact) effect of a one-unit innovation in the shock variable on each response variable. The standard errors in irf_result.se widen as h increases because longer-horizon LP residuals exhibit stronger serial correlation, and the effective sample shrinks by one observation per horizon.","category":"section"},{"location":"lp/#LPModel-Return-Values","page":"Local Projections","title":"LPModel Return Values","text":"Field Type Description\nY Matrix{T} Original data matrix\nshock_var Int Index of the shock variable\nresponse_vars Vector{Int} Indices of response variables\nhorizon Int Maximum horizon H\nlags Int Number of control lags\nB Vector{Matrix{T}} Coefficient matrices (one per horizon)\nresiduals Vector{Matrix{T}} Residuals at each horizon\nvcov Vector{Matrix{T}} Variance-covariance matrices (HAC)\nT_eff Vector{Int} Effective sample size at each horizon\ncov_estimator AbstractCovarianceEstimator Covariance estimator used","category":"section"},{"location":"lp/#LPImpulseResponse-Return-Values","page":"Local Projections","title":"LPImpulseResponse Return Values","text":"Field Type Description\nvalues Matrix{T} (H+1) times n_resp IRF point estimates\nci_lower Matrix{T} Lower confidence bounds\nci_upper Matrix{T} Upper confidence bounds\nse Matrix{T} Standard errors at each horizon\nhorizon Int Maximum horizon\nresponse_vars Vector{String} Response variable names\nshock_var String Shock variable name\ncov_type Symbol Covariance estimator type\nconf_level T Confidence level (e.g., 0.95)\n\n","category":"section"},{"location":"lp/#Local-Projections-with-Instrumental-Variables-(LP-IV)","page":"Local Projections","title":"Local Projections with Instrumental Variables (LP-IV)","text":"","category":"section"},{"location":"lp/#Motivation","page":"Local Projections","title":"Motivation","text":"When the shock variable x_t is endogenous or measured with error, we need external instruments for identification. Stock & Watson (2018) develop the LP-IV methodology for using external instruments in a local projection framework.","category":"section"},{"location":"lp/#The-LP-IV-Model","page":"Local Projections","title":"The LP-IV Model","text":"We use two-stage least squares (2SLS) at each horizon:\n\nFirst Stage: Regress the endogenous shock on instruments and controls:\n\nx_t = pi_0 + pi_1 z_t + pi_2 w_t + v_t\n\nSecond Stage: Use fitted values in the LP regression:\n\ny_it+h = alpha_ih + beta_ih hatx_t + gamma_ih w_t + varepsilon_it+h\n\nwhere z_t is the vector of external instruments.","category":"section"},{"location":"lp/#Identification-Assumptions","page":"Local Projections","title":"Identification Assumptions","text":"Relevance: Ez_t x_t neq 0 (instruments predict the shock)\nExogeneity: Ez_t varepsilon_t+h = 0 (instruments are uncorrelated with structural errors)","category":"section"},{"location":"lp/#First-Stage-F-Statistic","page":"Local Projections","title":"First-Stage F-Statistic","text":"The first-stage F-statistic tests instrument relevance:\n\nF = frac(hatpi_1 hatV_pi^-1 hatpi_1)q\n\nwhere\n\nhatpi_1 is the vector of first-stage coefficients on the instruments\nhatV_pi is the estimated variance-covariance of hatpi_1\nq is the number of instruments\n\nA rule of thumb is F  10 for strong instruments (Stock & Yogo, 2005).","category":"section"},{"location":"lp/#Weak-Instrument-Robust-Inference","page":"Local Projections","title":"Weak Instrument Robust Inference","text":"When instruments are weak, standard 2SLS inference is unreliable. Options include:\n\nAnderson-Rubin confidence sets\nConditional likelihood ratio tests\nWeak-instrument robust standard errors\n\nReference: Stock & Watson (2018), Stock & Yogo (2005)","category":"section"},{"location":"lp/#Julia-Implementation-2","page":"Local Projections","title":"Julia Implementation","text":"using MacroEconometricModels\n\n# Y: T×n data matrix\n# shock_var: index of endogenous shock variable\n# Z: T×q matrix of external instruments\n\nlpiv_model = estimate_lp_iv(Y, shock_var, Z, H;\n    lags = 4,\n    cov_type = :newey_west\n)\n\n# Check first-stage strength\nweak_test = weak_instrument_test(lpiv_model; threshold = 10.0)\nprintln(\"Minimum F-statistic: \", weak_test.min_F)\nprintln(\"All horizons pass: \", weak_test.passes_threshold)\n\n# Extract IRF\nirf_iv = lp_iv_irf(lpiv_model)\n\nThe weak_test.min_F reports the minimum first-stage F-statistic across all horizons. If it exceeds the Stock & Yogo (2005) threshold of 10, instruments are considered strong at every horizon. First-stage strength typically declines at longer horizons because the instrument's predictive power for the endogenous shock weakens. If weak_test.passes_threshold is false, the IV estimates at affected horizons should be interpreted cautiously — consider Anderson-Rubin confidence sets for robust inference.","category":"section"},{"location":"lp/#LPIVModel-Return-Values","page":"Local Projections","title":"LPIVModel Return Values","text":"Field Type Description\nY Matrix{T} Original data matrix\nshock_var Int Index of the endogenous shock variable\nresponse_vars Vector{Int} Response variable indices\ninstruments Matrix{T} External instrument matrix\nhorizon Int Maximum horizon\nlags Int Number of control lags\nB Vector{Matrix{T}} Second-stage coefficient matrices\nresiduals Vector{Matrix{T}} Residuals at each horizon\nvcov Vector{Matrix{T}} Variance-covariance matrices\nfirst_stage_F Vector{T} First-stage F-statistics by horizon\nfirst_stage_coef Vector{Vector{T}} First-stage instrument coefficients\nT_eff Vector{Int} Effective sample sizes\ncov_estimator AbstractCovarianceEstimator Covariance estimator used\n\n","category":"section"},{"location":"lp/#Smooth-Local-Projections","page":"Local Projections","title":"Smooth Local Projections","text":"","category":"section"},{"location":"lp/#Motivation-2","page":"Local Projections","title":"Motivation","text":"Standard LPs can produce noisy, erratic impulse responses because each horizon is estimated independently. Barnichon & Brownlees (2019) propose Smooth Local Projections that parameterize the IRF as a smooth function of the horizon using B-spline basis functions.","category":"section"},{"location":"lp/#B-Spline-Representation","page":"Local Projections","title":"B-Spline Representation","text":"The impulse response is modeled as:\n\nbeta(h) = sum_j=1^J theta_j B_j(h)\n\nwhere B_j(h) are B-spline basis functions and theta_j are spline coefficients.","category":"section"},{"location":"lp/#Cubic-B-Splines","page":"Local Projections","title":"Cubic B-Splines","text":"For degree d = 3 (cubic splines), the basis functions are computed recursively using the Cox-de Boor formula:\n\nB_i0(x) = begincases 1  textif  t_i leq x  t_i+1  0  textotherwise endcases\n\nB_id(x) = fracx - t_it_i+d - t_i B_id-1(x) + fract_i+d+1 - xt_i+d+1 - t_i+1 B_i+1d-1(x)","category":"section"},{"location":"lp/#Smoothness-Penalty","page":"Local Projections","title":"Smoothness Penalty","text":"To enforce smoothness, we add a roughness penalty on the second derivative:\n\nmin_theta sum_h=0^H left( hatbeta_h - B(h)theta right)^2 + lambda int left( beta(h) right)^2 dh\n\nwhere\n\nhatbeta_h are the standard LP estimates at horizon h\nB(h) is the J times 1 B-spline basis vector evaluated at h\ntheta is the J times 1 vector of spline coefficients\nlambda geq 0 is the smoothing penalty (lambda = 0 gives unpenalized fit)\n\nThe penalty is computed as theta R theta where:\n\nR_ij = int B_i(x) B_j(x) dx","category":"section"},{"location":"lp/#Two-Step-Estimation","page":"Local Projections","title":"Two-Step Estimation","text":"Estimate standard LP to get hatbeta_h and textVar(hatbeta_h)\nFit weighted penalized spline:\n\nhattheta = left( BWB + lambda R right)^-1 BW hatbeta\n\nwhere\n\nB is the (H+1) times J basis matrix\nW = textdiag(1textVar(hatbeta_h)) is the precision-weight matrix\nR is the J times J roughness penalty matrix\nhatbeta is the (H+1) times 1 vector of standard LP estimates","category":"section"},{"location":"lp/#Cross-Validation-for-λ-Selection","page":"Local Projections","title":"Cross-Validation for λ Selection","text":"The smoothing parameter lambda can be selected by k-fold cross-validation to minimize out-of-sample prediction error.\n\nReference: Barnichon & Brownlees (2019)","category":"section"},{"location":"lp/#Julia-Implementation-3","page":"Local Projections","title":"Julia Implementation","text":"using MacroEconometricModels\n\n# Smooth LP with cubic splines\nsmooth_model = estimate_smooth_lp(Y, shock_var, H;\n    degree = 3,           # Cubic splines\n    n_knots = 4,          # Interior knots\n    lambda = 1.0,         # Smoothing penalty\n    lags = 4\n)\n\n# Automatic lambda selection via CV\noptimal_lambda = cross_validate_lambda(Y, shock_var, H;\n    lambda_grid = 10.0 .^ (-4:0.5:2),\n    k_folds = 5\n)\n\n# Compare smooth vs standard LP\ncomparison = compare_smooth_lp(Y, shock_var, H; lambda = optimal_lambda)\nprintln(\"Variance reduction: \", comparison.variance_reduction)\n\nLarger lambda values impose more smoothness, shrinking the IRF toward a low-frequency polynomial. When variance_reduction is positive, the smooth IRF achieves lower pointwise variance at the cost of some bias — a favorable trade-off in moderate samples where standard LP confidence bands are wide. Cross-validation selects the lambda that minimizes out-of-sample prediction error, automatically balancing the bias-variance trade-off.","category":"section"},{"location":"lp/#SmoothLPModel-Return-Values","page":"Local Projections","title":"SmoothLPModel Return Values","text":"Field Type Description\nY Matrix{T} Original data matrix\nshock_var Int Shock variable index\nresponse_vars Vector{Int} Response variable indices\nhorizon Int Maximum horizon\nlags Int Number of control lags\nspline_basis BSplineBasis{T} B-spline basis (knots, degree, basis matrix)\ntheta Matrix{T} Spline coefficients\nvcov_theta Matrix{T} Variance-covariance of spline coefficients\nlambda T Smoothing penalty parameter\nirf_values Matrix{T} Smoothed IRF point estimates\nirf_se Matrix{T} Standard errors of smoothed IRF\nresiduals Matrix{T} Regression residuals\nT_eff Int Effective sample size\ncov_estimator AbstractCovarianceEstimator Covariance estimator used\n\n","category":"section"},{"location":"lp/#State-Dependent-Local-Projections","page":"Local Projections","title":"State-Dependent Local Projections","text":"","category":"section"},{"location":"lp/#Motivation-3","page":"Local Projections","title":"Motivation","text":"Economic responses may differ across states of the economy (e.g., recessions vs. expansions). Auerbach & Gorodnichenko (2012, 2013) develop state-dependent LPs using smooth transition functions.","category":"section"},{"location":"lp/#The-State-Dependent-Model","page":"Local Projections","title":"The State-Dependent Model","text":"y_t+h = F(z_t) left alpha_E + beta_E x_t + gamma_E w_t right + (1 - F(z_t)) left alpha_R + beta_R x_t + gamma_R w_t right + varepsilon_t+h\n\nwhere:\n\nF(z_t) is the smooth transition function\nz_t is the state variable (e.g., moving average of GDP growth)\nSubscript E denotes \"expansion\" regime (F to 0)\nSubscript R denotes \"recession\" regime (F to 1)","category":"section"},{"location":"lp/#Logistic-Transition-Function","page":"Local Projections","title":"Logistic Transition Function","text":"The standard specification uses a logistic function:\n\nF(z_t) = fracexp(-gamma(z_t - c))1 + exp(-gamma(z_t - c))\n\nwhere:\n\ngamma  0 controls the transition speed (higher = sharper)\nc is the threshold (often set to 0 for standardized z_t)\n\nProperties:\n\nF(z) to 1 as z to -infty (deep recession)\nF(z) to 0 as z to +infty (strong expansion)\nF(c) = 05 (neutral state)","category":"section"},{"location":"lp/#State-Variable-Construction","page":"Local Projections","title":"State Variable Construction","text":"Following Auerbach & Gorodnichenko, the state variable is typically:\n\nz_t = frac1k sum_j=0^k-1 Delta y_t-j\n\nA k = 7 quarter moving average of GDP growth is common, then standardized to have zero mean and unit variance.","category":"section"},{"location":"lp/#Estimation-2","page":"Local Projections","title":"Estimation","text":"The model is estimated by nonlinear least squares or by treating it as a linear regression in the interaction terms. The parameters (gamma c) can be:\n\nFixed based on prior research\nEstimated via grid search or NLS\nSelected to maximize fit","category":"section"},{"location":"lp/#Testing-for-Regime-Differences","page":"Local Projections","title":"Testing for Regime Differences","text":"Test whether responses differ across regimes:\n\nH_0 beta_E - beta_R = 0\n\nusing a t-test with HAC standard errors:\n\nt = frachatbeta_E - hatbeta_RsqrttextVar(hatbeta_E) + textVar(hatbeta_R) - 2textCov(hatbeta_E hatbeta_R)\n\nReference: Auerbach & Gorodnichenko (2012, 2013), Ramey & Zubairy (2018)","category":"section"},{"location":"lp/#Julia-Implementation-4","page":"Local Projections","title":"Julia Implementation","text":"using MacroEconometricModels\n\n# Construct state variable (e.g., 7-quarter MA of GDP growth)\ngdp_growth = diff(log.(Y[:, 1]))\nstate_var = [mean(gdp_growth[max(1, t-6):t]) for t in 1:length(gdp_growth)]\nstate_var = (state_var .- mean(state_var)) ./ std(state_var)\n\n# Estimate state-dependent LP\nstate_model = estimate_state_lp(Y, shock_var, state_var, H;\n    gamma = :estimate,      # Estimate transition speed\n    threshold = :median,    # Set threshold at median\n    lags = 4\n)\n\n# Extract regime-specific IRFs\nirf_both = state_irf(state_model; regime = :both)\nirf_expansion = state_irf(state_model; regime = :expansion)\nirf_recession = state_irf(state_model; regime = :recession)\n\n# Test for regime differences\ndiff_test = test_regime_difference(state_model)\n\nThe irf_expansion and irf_recession objects contain regime-specific impulse responses. Comparing them reveals whether a shock (e.g., fiscal spending) has asymmetric effects across the business cycle — a prediction of many New Keynesian models with binding ZLB or liquidity traps. The test_regime_difference function computes a Wald-type test of H_0 beta_E = beta_R at each horizon using HAC standard errors; rejection implies statistically significant state dependence.","category":"section"},{"location":"lp/#StateLPModel-Return-Values","page":"Local Projections","title":"StateLPModel Return Values","text":"Field Type Description\nY Matrix{T} Original data matrix\nshock_var Int Shock variable index\nresponse_vars Vector{Int} Response variable indices\nhorizon Int Maximum horizon\nlags Int Number of control lags\nstate StateTransition{T} State transition function (gamma, threshold, F(z_t) values)\nB_expansion Vector{Matrix{T}} Expansion regime coefficients\nB_recession Vector{Matrix{T}} Recession regime coefficients\nresiduals Vector{Matrix{T}} Residuals at each horizon\nvcov_expansion Vector{Matrix{T}} Expansion regime variance-covariance\nvcov_recession Vector{Matrix{T}} Recession regime variance-covariance\nvcov_diff Vector{Matrix{T}} Variance-covariance of regime difference\nT_eff Vector{Int} Effective sample sizes\ncov_estimator AbstractCovarianceEstimator Covariance estimator used\n\n","category":"section"},{"location":"lp/#Propensity-Score-Local-Projections","page":"Local Projections","title":"Propensity Score Local Projections","text":"","category":"section"},{"location":"lp/#Motivation-4","page":"Local Projections","title":"Motivation","text":"When the shock is a discrete treatment (e.g., policy intervention), selection bias may confound causal inference. Angrist, Jordà & Kuersteiner (2018) develop LP with inverse propensity weighting (IPW) to address selection.","category":"section"},{"location":"lp/#The-Setup","page":"Local Projections","title":"The Setup","text":"Let D_t in 0 1 be a binary treatment indicator. We want to estimate the Average Treatment Effect (ATE):\n\ntextATE_h = Ey_t+h(1) - y_t+h(0)\n\nwhere y_t+h(d) is the potential outcome under treatment status d.","category":"section"},{"location":"lp/#Propensity-Score","page":"Local Projections","title":"Propensity Score","text":"The propensity score is the probability of treatment given covariates:\n\np(X_t) = P(D_t = 1  X_t)\n\nestimated via logit or probit:\n\np(X_t) = frac11 + exp(-X_tbeta)","category":"section"},{"location":"lp/#Inverse-Propensity-Weighting-(IPW)","page":"Local Projections","title":"Inverse Propensity Weighting (IPW)","text":"The IPW estimator weights observations by the inverse of their selection probability:\n\nTreated: weight = 1p(X_t)\nControl: weight = 1(1-p(X_t))\n\nThis reweighting creates a pseudo-population where treatment is independent of covariates.","category":"section"},{"location":"lp/#IPW-LP-Estimation","page":"Local Projections","title":"IPW-LP Estimation","text":"At each horizon h:\n\nhattextATE_h = frac1n sum_t D_t=1 fracy_t+hhatp(X_t) - frac1n sum_t D_t=0 fracy_t+h1-hatp(X_t)\n\nOr via weighted regression:\n\ny_t+h = alpha_h + beta_h D_t + gamma_h X_t + varepsilon_t+h\n\nestimated by WLS with IPW weights.","category":"section"},{"location":"lp/#Doubly-Robust-Estimation","page":"Local Projections","title":"Doubly Robust Estimation","text":"The doubly robust (DR) estimator combines IPW with outcome regression:\n\nhattextATE^DR_h = frac1n sum_t left fracD_t(y_t+h - mu_1(X_t))hatp(X_t) + mu_1(X_t) right - frac1n sum_t left frac(1-D_t)(y_t+h - mu_0(X_t))1-hatp(X_t) + mu_0(X_t) right\n\nwhere mu_d(X_t) = Ey_t+h  D_t = d X_t is the outcome regression.\n\nProperty: DR is consistent if either the propensity score or the outcome model is correctly specified.","category":"section"},{"location":"lp/#Practical-Considerations","page":"Local Projections","title":"Practical Considerations","text":"Trimming: Propensity scores near 0 or 1 lead to extreme weights. Trim at [0.01, 0.99].\nOverlap: Verify that treated and control groups have overlapping covariate distributions.\nBalance: Check that covariates are balanced after reweighting (standardized mean differences < 0.1).\n\nReference: Angrist, Jordà & Kuersteiner (2018), Hirano, Imbens & Ridder (2003)","category":"section"},{"location":"lp/#Julia-Implementation-5","page":"Local Projections","title":"Julia Implementation","text":"using MacroEconometricModels\n\n# treatment: Bool vector of treatment indicators\n# covariates: matrix of selection-relevant covariates\n\n# IPW estimation\nprop_model = estimate_propensity_lp(Y, treatment, covariates, H;\n    ps_method = :logit,\n    trimming = (0.01, 0.99),\n    lags = 4\n)\n\n# Doubly robust estimation\ndr_model = doubly_robust_lp(Y, treatment, covariates, H)\n\n# Extract ATE impulse response\nate_irf = propensity_irf(prop_model)\n\n# Diagnostics\ndiagnostics = propensity_diagnostics(prop_model)\nprintln(\"Propensity score overlap: \", diagnostics.overlap)\nprintln(\"Max covariate imbalance: \", diagnostics.balance.max_weighted)\n\nThe ate_irf object contains the estimated Average Treatment Effect at each horizon. The doubly robust estimator is preferred when there is uncertainty about the propensity score or outcome model specification, since it requires only one of the two to be correctly specified for consistency. The diagnostics check two key assumptions: overlap (sufficient common support between treated and control distributions) and balance (covariate means equalized after reweighting, with standardized differences below 0.1).","category":"section"},{"location":"lp/#PropensityLPModel-Return-Values","page":"Local Projections","title":"PropensityLPModel Return Values","text":"Field Type Description\nY Matrix{T} Original data matrix\ntreatment Vector{Bool} Binary treatment indicator\nresponse_vars Vector{Int} Response variable indices\ncovariates Matrix{T} Selection-relevant covariates\nhorizon Int Maximum horizon\npropensity_scores Vector{T} Estimated propensity scores hatp(X_t)\nipw_weights Vector{T} Inverse propensity weights\nB Vector{Matrix{T}} Weighted regression coefficients\nresiduals Vector{Matrix{T}} Weighted residuals\nvcov Vector{Matrix{T}} Variance-covariance matrices\nate Matrix{T} Average treatment effect estimates\nate_se Matrix{T} Standard errors of ATE\nconfig PropensityScoreConfig{T} Configuration (method, trimming, normalize)\nT_eff Vector{Int} Effective sample sizes\ncov_estimator AbstractCovarianceEstimator Covariance estimator used\n\n","category":"section"},{"location":"lp/#Structural-Local-Projections","page":"Local Projections","title":"Structural Local Projections","text":"","category":"section"},{"location":"lp/#Motivation-5","page":"Local Projections","title":"Motivation","text":"Standard LP estimates the response to a single shock variable. In multivariate settings, however, we often want to trace the dynamic effects of orthogonalized structural shocks — just as in SVAR analysis. Structural Local Projections combine VAR-based identification with LP estimation to achieve this.\n\nPlagborg-Møller & Wolf (2021) establish a deep connection: under correct specification, LP and VAR estimate the same impulse responses. Structural LP leverages this equivalence by using the VAR only for shock identification (computing the rotation matrix Q), then estimating the dynamic responses via LP regressions — gaining the robustness of LP while retaining the structural interpretability of SVAR.","category":"section"},{"location":"lp/#Algorithm","page":"Local Projections","title":"Algorithm","text":"The structural LP procedure proceeds in five steps:\n\nEstimate VAR(p): Fit a VAR on the data Y to obtain the residual covariance hatSigma and reduced-form residuals hatu_t\nIdentify structural shocks: Compute the rotation matrix Q via the chosen identification method (Cholesky, sign restrictions, long-run, ICA, etc.)\nRecover structural shocks: Compute hatvarepsilon_t = QL^-1hatu_t where L = textchol(hatSigma)\nRun LP regressions: For each structural shock j, estimate LP regressions using hatvarepsilon_jt as the shock variable:\n\ny_it+h = alpha_ih^(j) + beta_ih^(j) hatvarepsilon_jt + gamma_ih^(j)prime w_t + u_it+h^(j)\n\nwhere\n\ny_it+h is the response variable i at horizon t+h\nhatvarepsilon_jt is the identified structural shock j\nw_t contains lagged values of Y as controls\nbeta_ih^(j) is the structural impulse response of variable i to shock j at horizon h\n\nStack into 3D IRF array: Thetah i j = hatbeta_ih^(j) for h = 1 ldots H","category":"section"},{"location":"lp/#Identification-Methods","page":"Local Projections","title":"Identification Methods","text":"Structural LP supports all identification methods available for SVAR:\n\nMethod Keyword Description\nCholesky :cholesky Recursive ordering (lower triangular B_0)\nSign restrictions :sign Constrain signs of responses (Uhlig, 2005)\nLong-run :long_run Blanchard-Quah (1989) zero long-run effect\nNarrative :narrative Historical events + sign restrictions (Antolín-Díaz & Rubio-Ramírez, 2018)\nFastICA :fastica Non-Gaussian ICA (Hyvärinen, 1999)\nJADE :jade Joint Approximate Diagonalization of Eigenmatrices\nSOBI :sobi Second-Order Blind Identification\ndCov :dcov Distance covariance independence criterion\nHSIC :hsic Hilbert-Schmidt independence criterion\nStudent-t ML :student_t Maximum likelihood with Student-t errors\nMixture-normal ML :mixture_normal Gaussian mixture ML\nPML :pml Pseudo maximum likelihood","category":"section"},{"location":"lp/#Julia-Implementation-6","page":"Local Projections","title":"Julia Implementation","text":"using MacroEconometricModels\nusing Random\n\nRandom.seed!(42)\nT, n = 200, 3\nY = randn(T, n)\nfor t in 2:T\n    Y[t, :] = 0.5 * Y[t-1, :] + 0.3 * randn(n)\nend\n\n# Structural LP with Cholesky identification\nslp = structural_lp(Y, 20; method=:cholesky, lags=4)\n\n# Access 3D IRF array: irfs.values[h, i, j]\nprintln(\"Shock 1 → Var 1 at h=1: \", round(slp.irf.values[1, 1, 1], digits=4))\nprintln(\"Shock 2 → Var 1 at h=8: \", round(slp.irf.values[8, 1, 2], digits=4))\n\n# Standard errors\nprintln(\"SE at h=1: \", round(slp.se[1, 1, 1], digits=4))\n\n# With bootstrap CIs\nslp_ci = structural_lp(Y, 20; method=:cholesky, ci_type=:bootstrap, reps=500)\n\n# With sign restrictions\ncheck_fn(irf) = irf[1, 1, 1] > 0 && irf[1, 2, 1] > 0\nslp_sign = structural_lp(Y, 20; method=:sign, check_func=check_fn)\n\n# Dispatch to IRF, FEVD, HD\nirf_result = irf(slp)           # Returns the ImpulseResponse from StructuralLP\ndecomp = fevd(slp, 20)          # LP-FEVD (Gorodnichenko & Lee 2019)\nhd = historical_decomposition(slp)  # LP-based historical decomposition\n\nThe slp.irf.values array has shape H times n times n, where values[h, i, j] gives the response of variable i to structural shock j at horizon h. Under Cholesky identification, the ordering determines which variables respond contemporaneously to each shock — variable 1 responds only to shock 1 at impact, variable 2 responds to shocks 1 and 2, and so on. The standard errors in slp.se are computed from HAC-corrected LP regressions and tend to be wider than VAR-based IRF confidence bands, reflecting the efficiency cost of LP's robustness to dynamic misspecification.","category":"section"},{"location":"lp/#StructuralLP-Return-Values","page":"Local Projections","title":"StructuralLP Return Values","text":"Field Type Description\nirf ImpulseResponse{T} 3D IRF result (H times n times n) with optional bootstrap CIs\nstructural_shocks Matrix{T} T_eff times n recovered structural shocks\nvar_model VARModel{T} Underlying VAR model used for identification\nQ Matrix{T} n times n rotation/identification matrix\nmethod Symbol Identification method used\nlags Int Number of LP control lags\ncov_type Symbol HAC estimator type (:newey_west, :white)\nse Array{T,3} H times n times n standard errors\nlp_models Vector{LPModel{T}} Individual LP model per shock\n\nReference: Plagborg-Møller & Wolf (2021)\n\n","category":"section"},{"location":"lp/#LP-Forecasting","page":"Local Projections","title":"LP Forecasting","text":"","category":"section"},{"location":"lp/#Direct-Multi-Step-Forecasts","page":"Local Projections","title":"Direct Multi-Step Forecasts","text":"LP-based forecasts use horizon-specific regression coefficients directly — no VAR recursion required. For each horizon h = 1 ldots H, the forecast is:\n\nhaty_T+h = hatalpha_h + hatbeta_h cdot s_h + hatGamma_h w_T\n\nwhere\n\nhaty_T+h is the h-step-ahead point forecast\nhatalpha_h is the horizon-specific intercept\nhatbeta_h is the coefficient on the assumed shock path value s_h\nhatGamma_h is the coefficient vector on controls w_T (last p observations of Y)\n\nThis \"direct\" approach has a key advantage over recursive (iterated) VAR forecasts: each horizon uses its own regression, so misspecification in the short-horizon model does not compound into longer horizons.","category":"section"},{"location":"lp/#Confidence-Intervals","page":"Local Projections","title":"Confidence Intervals","text":"Three CI methods are available:\n\nMethod Description\n:analytical HAC standard errors + normal quantiles: haty_T+h pm z_alpha2 cdot hatsigma_h\n:bootstrap Residual resampling with percentile CIs\n:none Point forecasts only (no CIs)","category":"section"},{"location":"lp/#Julia-Implementation-7","page":"Local Projections","title":"Julia Implementation","text":"using MacroEconometricModels\nusing Random\n\nRandom.seed!(42)\nT, n = 200, 3\nY = randn(T, n)\nfor t in 2:T\n    Y[t, :] = 0.5 * Y[t-1, :] + 0.3 * randn(n)\nend\n\n# Estimate LP model\nlp = estimate_lp(Y, 1, 20; lags=4, cov_type=:newey_west)\n\n# Forecast with a unit shock path (1 at all horizons)\nshock_path = ones(20)\nfc = forecast(lp, shock_path; ci_method=:analytical, conf_level=0.95)\n\nprintln(\"Forecast at h=1: \", round(fc.forecasts[1, 1], digits=4))\nprintln(\"Forecast at h=8: \", round(fc.forecasts[8, 1], digits=4))\nprintln(\"95% CI at h=8: [\", round(fc.ci_lower[8, 1], digits=4),\n        \", \", round(fc.ci_upper[8, 1], digits=4), \"]\")\n\n# Structural LP forecast with a specific shock\nslp = structural_lp(Y, 20; method=:cholesky)\nfc_struct = forecast(slp, 1, shock_path;  # shock_idx=1\n                     ci_method=:bootstrap, n_boot=500)\n\nThe fc.forecasts matrix has shape H times n_resp, where each row gives the point forecast at a given horizon. The analytical CIs widen with the horizon because the LP regression residuals exhibit increasing variance at longer horizons and the effective sample shrinks. The bootstrap CIs are generally more reliable in small samples because they do not rely on the normal approximation; however, they require the LP residuals to be approximately exchangeable, which holds under correct specification.","category":"section"},{"location":"lp/#LPForecast-Return-Values","page":"Local Projections","title":"LPForecast Return Values","text":"Field Type Description\nforecasts Matrix{T} H times n_resp point forecasts\nci_lower Matrix{T} Lower CI bounds\nci_upper Matrix{T} Upper CI bounds\nse Matrix{T} Standard errors at each horizon\nhorizon Int Maximum forecast horizon H\nresponse_vars Vector{Int} Response variable indices\nshock_var Int Shock variable index\nshock_path Vector{T} Assumed shock trajectory\nconf_level T Confidence level\nci_method Symbol CI method used (:analytical, :bootstrap, :none)\n\nReference: Jordà (2005), Plagborg-Møller & Wolf (2021)\n\n","category":"section"},{"location":"lp/#LP-Based-FEVD","page":"Local Projections","title":"LP-Based FEVD","text":"","category":"section"},{"location":"lp/#Motivation-6","page":"Local Projections","title":"Motivation","text":"Standard FEVD computes the share of forecast error variance attributable to each structural shock using the VMA (Vector Moving Average) representation. However, if the VAR is misspecified, VMA-based FEVD inherits those errors. Gorodnichenko & Lee (2019) propose an LP-based FEVD that estimates variance shares directly via R² regressions, inheriting the robustness properties of LP.","category":"section"},{"location":"lp/#The-R-Estimator","page":"Local Projections","title":"The R² Estimator","text":"At each horizon h, the share of variable i's forecast error variance due to shock j is estimated by:\n\nObtain LP forecast error residuals hatf_t+ht-1 from the LP regression\nRegress these residuals on structural shock leads hatvarepsilon_jt+h hatvarepsilon_jt+h-1 ldots hatvarepsilon_jt\nThe R² from this regression is the FEVD share:\n\nwidehattextFEVD_ij(h) = R^2left(hatf_it+ht-1 sim hatvarepsilon_jt+h hatvarepsilon_jt+h-1 ldots hatvarepsilon_jtright)\n\nwhere\n\nhatf_it+ht-1 are LP forecast error residuals for variable i at horizon h\nhatvarepsilon_jt+k are leads and current values of structural shock j\nR^2 measures the fraction of forecast error variance explained by shock j","category":"section"},{"location":"lp/#Alternative-Estimators","page":"Local Projections","title":"Alternative Estimators","text":"Two additional estimators are available:\n\nLP-A Estimator (Gorodnichenko & Lee 2019, Eq. 9):\n\nhats_ij^A(h) = fracsum_k=0^h (hatbeta_0ik^LP)^2 hatsigma_varepsilon_j^2textVar(hatf_it+ht-1)\n\nwhere\n\nhatbeta_0ik^LP is the LP coefficient on shock j at horizon k\nhatsigma_varepsilon_j^2 is the variance of structural shock j\n\nLP-B Estimator (Gorodnichenko & Lee 2019, Eq. 10):\n\nhats_ij^B(h) = fractextnumerator^Atextnumerator^A + textVar(tildev_t+h)\n\nwhere tildev_t+h are the residuals from the R² regression. LP-B replaces the total forecast error variance in the denominator with the sum of explained and unexplained components, which can improve finite-sample performance.","category":"section"},{"location":"lp/#Bias-Correction","page":"Local Projections","title":"Bias Correction","text":"LP-FEVD estimates can be biased in finite samples. Following Kilian (1998), the package implements VAR-based bootstrap bias correction:\n\nFit a bivariate VAR(L) on (z y) with HQIC-selected lag order\nCompute the \"true\" FEVD from this VAR (theoretical benchmark)\nSimulate B bootstrap samples from the VAR\nFor each simulation, compute LP-FEVD and estimate bias = textmean(textboot) - texttrue\nBias-corrected estimate = raw - bias\nCIs from the centered bootstrap distribution","category":"section"},{"location":"lp/#Julia-Implementation-8","page":"Local Projections","title":"Julia Implementation","text":"using MacroEconometricModels\nusing Random\n\nRandom.seed!(42)\nT, n = 200, 3\nY = randn(T, n)\nfor t in 2:T\n    Y[t, :] = 0.5 * Y[t-1, :] + 0.3 * randn(n)\nend\n\n# First estimate structural LP\nslp = structural_lp(Y, 20; method=:cholesky, lags=4)\n\n# R²-based LP-FEVD with bias correction\nlfevd = lp_fevd(slp, 20; method=:r2, bias_correct=true, n_boot=500)\n\n# Access results\nprintln(\"FEVD of Var 1 due to Shock 1:\")\nfor h in [1, 4, 8, 12, 20]\n    raw = round(lfevd.proportions[1, 1, h] * 100, digits=1)\n    bc = round(lfevd.bias_corrected[1, 1, h] * 100, digits=1)\n    println(\"  h=$h: raw=$(raw)%, bias-corrected=$(bc)%\")\nend\n\n# Alternative estimators\nlfevd_a = lp_fevd(slp, 20; method=:lp_a)\nlfevd_b = lp_fevd(slp, 20; method=:lp_b)\n\n# Via dispatch\ndecomp = fevd(slp, 20)  # Equivalent to lp_fevd(slp, 20)\n\nThe raw FEVD proportions in lfevd.proportions[i, j, h] give the R² from regressing variable i's forecast error on shock j's leads at horizon h. Bias correction typically matters most at short horizons where finite-sample bias is largest. At long horizons, the R²-based and VMA-based FEVD should converge under correct specification. Comparing the three estimators (:r2, :lp_a, :lp_b) provides a robustness check — substantial disagreement suggests the VAR specification may be unreliable, in which case the LP-based estimates are preferred.","category":"section"},{"location":"lp/#LPFEVD-Return-Values","page":"Local Projections","title":"LPFEVD Return Values","text":"Field Type Description\nproportions Array{T,3} n times n times H raw FEVD estimates: proportions[i, j, h] = share of variable i's FEV due to shock j at horizon h\nbias_corrected Array{T,3} n times n times H bias-corrected FEVD\nse Array{T,3} Bootstrap standard errors\nci_lower Array{T,3} Lower CI bounds\nci_upper Array{T,3} Upper CI bounds\nmethod Symbol Estimator used (:r2, :lp_a, :lp_b)\nhorizon Int Maximum FEVD horizon\nn_boot Int Number of bootstrap replications\nconf_level T Confidence level for CIs\nbias_correction Bool Whether bias correction was applied\n\nReference: Gorodnichenko, Yuriy, and Byoungchan Lee. 2019. \"Forecast Error Variance Decompositions with Local Projections.\" Journal of Business & Economic Statistics 38 (4): 921–933. https://doi.org/10.1080/07350015.2019.1610661\n\n","category":"section"},{"location":"lp/#Comparing-LP-and-VAR","page":"Local Projections","title":"Comparing LP and VAR","text":"","category":"section"},{"location":"lp/#LP-vs.-VAR-Trade-offs","page":"Local Projections","title":"LP vs. VAR Trade-offs","text":"Aspect VAR Local Projections\nEfficiency More efficient if correctly specified Less efficient, but robust\nBias Biased if dynamics misspecified Consistent under weak conditions\nLong horizons Compounds specification error Each horizon estimated directly\nNonlinearities Requires extensions Easy to incorporate\nExternal instruments SVAR-IV LP-IV","category":"section"},{"location":"lp/#Asymptotic-Equivalence","page":"Local Projections","title":"Asymptotic Equivalence","text":"Plagborg-Møller & Wolf (2021) show that under correct specification, LP and VAR IRFs are asymptotically equivalent:\n\nsqrtT(hatbeta_h^LP - beta_h) xrightarrowd N(0 V^LP)\n\nsqrtT(hattheta_h^VAR - theta_h) xrightarrowd N(0 V^VAR)\n\nwith V^LP geq V^VAR (VAR is weakly more efficient).","category":"section"},{"location":"lp/#When-to-Use-LP","page":"Local Projections","title":"When to Use LP","text":"Concerned about VAR misspecification\nNeed to incorporate external instruments\nInterested in nonlinear/state-dependent responses\nWorking with discrete treatments\nLong horizons where VAR error compounds\n\nReference: Plagborg-Møller & Wolf (2021)\n\n","category":"section"},{"location":"lp/#References","page":"Local Projections","title":"References","text":"","category":"section"},{"location":"lp/#Local-Projections-Core","page":"Local Projections","title":"Local Projections - Core","text":"Jordà, Òscar. 2005. \"Estimation and Inference of Impulse Responses by Local Projections.\" American Economic Review 95 (1): 161–182. https://doi.org/10.1257/0002828053828518\nPlagborg-Møller, Mikkel, and Christian K. Wolf. 2021. \"Local Projections and VARs Estimate the Same Impulse Responses.\" Econometrica 89 (2): 955–980. https://doi.org/10.3982/ECTA17813","category":"section"},{"location":"lp/#LP-IV","page":"Local Projections","title":"LP-IV","text":"Stock, James H., and Mark W. Watson. 2018. \"Identification and Estimation of Dynamic Causal Effects in Macroeconomics Using External Instruments.\" Economic Journal 128 (610): 917–948. https://doi.org/10.1111/ecoj.12593\nStock, James H., and Motohiro Yogo. 2005. \"Testing for Weak Instruments in Linear IV Regression.\" In Identification and Inference for Econometric Models: Essays in Honor of Thomas Rothenberg, edited by Donald W. K. Andrews and James H. Stock, 80–108. Cambridge: Cambridge University Press.","category":"section"},{"location":"lp/#Smooth-LP","page":"Local Projections","title":"Smooth LP","text":"Barnichon, Regis, and Christian Brownlees. 2019. \"Impulse Response Estimation by Smooth Local Projections.\" Review of Economics and Statistics 101 (3): 522–530. https://doi.org/10.1162/resta00778","category":"section"},{"location":"lp/#State-Dependent-LP","page":"Local Projections","title":"State-Dependent LP","text":"Auerbach, Alan J., and Yuriy Gorodnichenko. 2012. \"Measuring the Output Responses to Fiscal Policy.\" American Economic Journal: Economic Policy 4 (2): 1–27. https://doi.org/10.1257/pol.4.2.1\nAuerbach, Alan J., and Yuriy Gorodnichenko. 2013. \"Fiscal Multipliers in Recession and Expansion.\" In Fiscal Policy after the Financial Crisis, edited by Alberto Alesina and Francesco Giavazzi, 63–98. Chicago: University of Chicago Press. https://doi.org/10.7208/9780226018584-004\nRamey, Valerie A., and Sarah Zubairy. 2018. \"Government Spending Multipliers in Good Times and in Bad: Evidence from US Historical Data.\" Journal of Political Economy 126 (2): 850–901. https://doi.org/10.1086/696277","category":"section"},{"location":"lp/#Structural-LP-and-LP-FEVD","page":"Local Projections","title":"Structural LP and LP-FEVD","text":"Gorodnichenko, Yuriy, and Byoungchan Lee. 2019. \"Forecast Error Variance Decompositions with Local Projections.\" Journal of Business & Economic Statistics 38 (4): 921–933. https://doi.org/10.1080/07350015.2019.1610661\nKilian, Lutz. 1998. \"Small-Sample Confidence Intervals for Impulse Response Functions.\" Review of Economics and Statistics 80 (2): 218–230. https://doi.org/10.1162/003465398557465","category":"section"},{"location":"lp/#Propensity-Score-Methods","page":"Local Projections","title":"Propensity Score Methods","text":"Angrist, Joshua D., Òscar Jordà, and Guido M. Kuersteiner. 2018. \"Semiparametric Estimates of Monetary Policy Effects: String Theory Revisited.\" Journal of Business & Economic Statistics 36 (3): 371–387. https://doi.org/10.1080/07350015.2016.1204919\nHirano, Keisuke, Guido W. Imbens, and Geert Ridder. 2003. \"Efficient Estimation of Average Treatment Effects Using the Estimated Propensity Score.\" Econometrica 71 (4): 1161–1189. https://doi.org/10.1111/1468-0262.00442","category":"section"},{"location":"lp/#Inference","page":"Local Projections","title":"Inference","text":"Newey, Whitney K., and Kenneth D. West. 1987. \"A Simple, Positive Semi-Definite, Heteroskedasticity and Autocorrelation Consistent Covariance Matrix.\" Econometrica 55 (3): 703–708. https://doi.org/10.2307/1913610","category":"section"},{"location":"#MacroEconometricModels.jl","page":"Home","title":"MacroEconometricModels.jl","text":"A comprehensive Julia package for macroeconometric research and analysis","category":"section"},{"location":"#Overview","page":"Home","title":"Overview","text":"MacroEconometricModels.jl provides a unified, high-performance framework for estimating and analyzing macroeconometric models in Julia. The package implements state-of-the-art methods for Vector Autoregression (VAR), Bayesian VAR (BVAR), Local Projections (LP), Factor Models, and Generalized Method of Moments (GMM) estimation.","category":"section"},{"location":"#Key-Features","page":"Home","title":"Key Features","text":"ARIMA Models: AR, MA, ARMA, and ARIMA estimation via OLS, CSS, MLE (Kalman filter), and CSS-MLE; automatic order selection; multi-step forecasting with confidence intervals\nVolatility Models: ARCH (Engle 1982), GARCH (Bollerslev 1986), EGARCH (Nelson 1991), GJR-GARCH (Glosten et al. 1993) via MLE with two-stage optimization; Stochastic Volatility (Taylor 1986) via Bayesian MCMC; news impact curves, ARCH-LM and Ljung-Box diagnostics, multi-step volatility forecasting with simulation-based CIs\nVector Autoregression (VAR): OLS estimation with comprehensive diagnostics, impulse response functions (IRFs), and forecast error variance decomposition (FEVD)\nStructural Identification: Multiple identification schemes including Cholesky, sign restrictions, long-run (Blanchard-Quah), and narrative restrictions\nBayesian VAR: Minnesota/Litterman prior with automatic hyperparameter optimization via marginal likelihood (Giannone, Lenza & Primiceri, 2015)\nLocal Projections: Jordà (2005) methodology with extensions for IV (Stock & Watson, 2018), smooth LP (Barnichon & Brownlees, 2019), state-dependence (Auerbach & Gorodnichenko, 2013), propensity score methods (Angrist, Jordà & Kuersteiner, 2018), structural LP (Plagborg-Møller & Wolf, 2021), LP forecasting, and LP-FEVD (Gorodnichenko & Lee, 2019)\nFactor Models: Static, dynamic, and generalized dynamic factor models with Bai & Ng (2002) information criteria; unified forecasting with theoretical (analytical) and bootstrap confidence intervals\nNon-Gaussian Structural Identification: ICA-based identification (FastICA, JADE, SOBI, dCov, HSIC), non-Gaussian ML (Student-t, mixture-normal, PML, skew-normal), heteroskedasticity-based identification (Markov-switching, GARCH, smooth-transition), multivariate normality tests, identifiability diagnostics\nHypothesis Tests: Comprehensive unit root tests (ADF, KPSS, Phillips-Perron, Zivot-Andrews, Ng-Perron) and Johansen cointegration test\nGMM Estimation: Flexible GMM framework with one-step, two-step, and iterated estimation\nRobust Inference: Newey-West, White, and Driscoll-Kraay HAC standard errors with automatic bandwidth selection\nDisplay Backends: Unified PrettyTables output with switchable backends (:text, :latex, :html) for terminal, papers, and web\nBibliographic References: refs() function for multi-format (AEA text, BibTeX, LaTeX, HTML) bibliographic references for all models and methods","category":"section"},{"location":"#Installation","page":"Home","title":"Installation","text":"using Pkg\nPkg.add(\"MacroEconometricModels\")\n\nOr from the Julia REPL package mode:\n\n] add MacroEconometricModels","category":"section"},{"location":"#Quick-Start","page":"Home","title":"Quick Start","text":"using MacroEconometricModels\nmodel = estimate_var(Y, 2)                          # VAR(2) via OLS\nirfs = irf(model, 20; method=:cholesky)             # Impulse responses\nchain = estimate_bvar(Y, 2; prior=:minnesota)       # Bayesian VAR\nlp = estimate_lp(Y, 1, 20; cov_type=:newey_west)   # Local Projections\nfm = estimate_factors(X, 3)                         # Factor model\nar = estimate_ar(y, 2)                              # AR(2)\ngarch = estimate_garch(y, 1, 1)                     # GARCH(1,1)\nsv = estimate_sv(y; n_samples=2000)                 # Stochastic Volatility\nadf = adf_test(y)                                   # Unit root test\ngmm = estimate_gmm(g, θ₀, data; weighting=:two_step)  # GMM\nrefs(model)                                         # Bibliographic references","category":"section"},{"location":"#Expanded-Examples","page":"Home","title":"Expanded Examples","text":"","category":"section"},{"location":"#Basic-VAR-Estimation","page":"Home","title":"Basic VAR Estimation","text":"using MacroEconometricModels\nusing Random\n\n# Generate synthetic macroeconomic data\nRandom.seed!(42)\nT, n = 200, 3  # 200 observations, 3 variables\nY = randn(T, n)\nfor t in 2:T\n    Y[t, :] = 0.5 * Y[t-1, :] + 0.3 * randn(3)\nend\n\n# Estimate VAR(2) model\nmodel = fit(VARModel, Y, 2)\n\n# Compute impulse responses (20 periods ahead)\nirfs = irf(model, 20; method=:cholesky)\n\n# Forecast error variance decomposition\ndecomp = fevd(model, 20; method=:cholesky)","category":"section"},{"location":"#Bayesian-VAR-with-Minnesota-Prior","page":"Home","title":"Bayesian VAR with Minnesota Prior","text":"using MacroEconometricModels\n\n# Set hyperparameters (or use optimize_hyperparameters)\nhyper = MinnesotaHyperparameters(\n    tau = 0.5,      # Overall tightness\n    decay = 2.0,    # Lag decay\n    lambda = 1.0,   # Own-lag variance\n    mu = 1.0,       # Cross-lag variance\n    omega = 1.0     # Deterministic terms\n)\n\n# Estimate BVAR with MCMC\nchain = estimate_bvar(Y, 2; n_samples=2000, n_adapts=500,\n                      prior=:minnesota, hyper=hyper)\n\n# Bayesian IRF with credible intervals\nbirf = irf(chain, 2, 3, 20; method=:cholesky)","category":"section"},{"location":"#Local-Projections","page":"Home","title":"Local Projections","text":"using MacroEconometricModels\n\n# Standard Local Projection (Jordà 2005)\nlp_model = estimate_lp(Y, 1, 20; lags=4, cov_type=:newey_west)\nlp_irfs = lp_irf(lp_model)\n\n# LP with Instrumental Variables (Stock & Watson 2018)\nZ = randn(T, 1)  # External instrument\nlpiv_model = estimate_lp_iv(Y, 1, Z, 20; lags=4)\nlpiv_irfs = lp_iv_irf(lpiv_model)\n\n# Structural LP (Plagborg-Møller & Wolf 2021)\nslp = structural_lp(Y, 20; method=:cholesky, lags=4)\nslp_irfs = irf(slp)       # 3D IRFs: values[h, i, j]\nlfevd = lp_fevd(slp, 20)  # LP-FEVD (Gorodnichenko & Lee 2019)\n\n# LP Forecasting\nfc = forecast(lp_model, ones(20); ci_method=:analytical)","category":"section"},{"location":"#Factor-Models","page":"Home","title":"Factor Models","text":"using MacroEconometricModels\n\n# Large panel: T observations, N variables\nX = randn(200, 100)\n\n# Determine optimal number of factors (Bai & Ng 2002)\nic = ic_criteria(X, 10)\nr_optimal = ic.r_IC2\n\n# Estimate static factor model\nfm = estimate_factors(X, r_optimal)\n\n# Extract factors for use in FAVAR\nfactors = fm.factors\n\n# Forecast with confidence intervals (all 3 model types supported)\nfc = forecast(fm, 12; ci_method=:theoretical)\nfc.observables       # 12×N forecasted observables\nfc.observables_lower # lower CI bounds\nfc.observables_upper # upper CI bounds","category":"section"},{"location":"#Unit-Root-Tests","page":"Home","title":"Unit Root Tests","text":"using MacroEconometricModels\n\n# Test for unit root\ny = cumsum(randn(200))  # Random walk (has unit root)\n\n# Augmented Dickey-Fuller test\nadf_result = adf_test(y; lags=:aic, regression=:constant)\n\n# KPSS stationarity test (opposite null hypothesis)\nkpss_result = kpss_test(y; regression=:constant)\n\n# Johansen cointegration test for multivariate data\nY = randn(200, 3)\njohansen_result = johansen_test(Y, 2; deterministic=:constant)","category":"section"},{"location":"#ARIMA-Models","page":"Home","title":"ARIMA Models","text":"using MacroEconometricModels\n\n# Univariate time series\ny = randn(200)\n\n# Estimate AR(2) via OLS\nar_model = estimate_ar(y, 2)\n\n# Estimate ARMA(1,1) via CSS-MLE\narma_model = estimate_arma(y, 1, 1)\n\n# Automatic ARIMA order selection\nbest = auto_arima(y)\n\n# Forecast 12 steps ahead with 95% confidence intervals\nfc = forecast(arma_model, 12)\nfc.forecast    # Point forecasts\nfc.ci_lower    # Lower bound\nfc.ci_upper    # Upper bound","category":"section"},{"location":"#Volatility-Models","page":"Home","title":"Volatility Models","text":"using MacroEconometricModels\n\n# Financial returns data\ny = randn(500)\n\n# Estimate GARCH(1,1) and EGARCH(1,1)\ngarch = estimate_garch(y, 1, 1)\negarch = estimate_egarch(y, 1, 1)\n\n# Model summary statistics\npersistence(garch)              # Volatility persistence\nhalflife(garch)                 # Variance half-life\nunconditional_variance(garch)   # Long-run variance\n\n# News impact curve (asymmetry diagnostic)\nnic = news_impact_curve(egarch)\n\n# Multi-step volatility forecast\nfc = forecast(garch, 20; conf_level=0.95)\n\n# Stochastic Volatility via MCMC\nsv = estimate_sv(y; n_samples=2000, n_adapts=1000)","category":"section"},{"location":"#Display-Backends-and-References","page":"Home","title":"Display Backends and References","text":"using MacroEconometricModels\n\n# Switch table output format\nset_display_backend(:latex)     # LaTeX tables for papers\nset_display_backend(:html)      # HTML tables for web/Jupyter\nset_display_backend(:text)      # Terminal output (default)\n\n# Bibliographic references for any model or method\nrefs(model)                     # AEA text format\nrefs(model; format=:bibtex)     # BibTeX for .bib files\nrefs(:fastica; format=:latex)   # LaTeX \\bibitem format","category":"section"},{"location":"#Package-Structure","page":"Home","title":"Package Structure","text":"The package is organized into the following modules:\n\nModule Description\ncore/ Shared infrastructure: types, utilities, display backends, covariance estimators\narima/ ARIMA suite: types, Kalman filter, estimation (CSS/MLE), forecasting, order selection\narch/ ARCH(q) estimation via MLE, volatility forecasting\ngarch/ GARCH, EGARCH, GJR-GARCH estimation via MLE, news impact curves, forecasting\nsv/ Stochastic Volatility via Bayesian MCMC (Turing.jl), posterior predictive forecasts\nvar/ VAR estimation (OLS), structural identification, IRF, FEVD, historical decomposition\nbvar/ Bayesian VAR: MCMC estimation, Minnesota prior, hyperparameter optimization\nlp/ Local Projections: core, IV, smooth, state-dependent, propensity, structural LP, forecast, LP-FEVD\nfactor/ Static (PCA), dynamic (two-step/EM), generalized (spectral) factor models with forecasting\nnongaussian/ Non-Gaussian structural identification: ICA, ML, heteroskedastic-ID\nteststat/ Statistical tests: unit root (ADF, KPSS, PP, ZA, Ng-Perron), Johansen cointegration, normality, ARCH diagnostics\ngmm/ Generalized Method of Moments\nsummary.jl Publication-quality summary tables and refs() bibliographic references","category":"section"},{"location":"#Mathematical-Notation","page":"Home","title":"Mathematical Notation","text":"Throughout this documentation, we use the following notation conventions:\n\nSymbol Description\ny_t n times 1 vector of endogenous variables at time t\nY T times n data matrix\np Number of lags in VAR\nA_i n times n coefficient matrix for lag i\nSigma n times n reduced-form error covariance\nB_0 n times n contemporaneous impact matrix\nvarepsilon_t n times 1 structural shocks\nu_t n times n reduced-form residuals\nh Forecast/impulse response horizon\nH Maximum horizon","category":"section"},{"location":"#References","page":"Home","title":"References","text":"","category":"section"},{"location":"#Univariate-Time-Series","page":"Home","title":"Univariate Time Series","text":"Box, George E. P., and Gwilym M. Jenkins. 1976. Time Series Analysis: Forecasting and Control. San Francisco: Holden-Day. ISBN 978-0-816-21104-3.\nBrockwell, Peter J., and Richard A. Davis. 1991. Time Series: Theory and Methods. 2nd ed. New York: Springer. ISBN 978-1-4419-0319-8.\nHarvey, Andrew C. 1993. Time Series Models. 2nd ed. Cambridge, MA: MIT Press. ISBN 978-0-262-08224-2.","category":"section"},{"location":"#Volatility-Models-2","page":"Home","title":"Volatility Models","text":"Bollerslev, Tim. 1986. \"Generalized Autoregressive Conditional Heteroskedasticity.\" Journal of Econometrics 31 (3): 307–327. https://doi.org/10.1016/0304-4076(86)90063-1\nEngle, Robert F. 1982. \"Autoregressive Conditional Heteroscedasticity with Estimates of the Variance of United Kingdom Inflation.\" Econometrica 50 (4): 987–1007. https://doi.org/10.2307/1912773\nGlosten, Lawrence R., Ravi Jagannathan, and David E. Runkle. 1993. \"On the Relation between the Expected Value and the Volatility of the Nominal Excess Return on Stocks.\" Journal of Finance 48 (5): 1779–1801. https://doi.org/10.1111/j.1540-6261.1993.tb05128.x\nNelson, Daniel B. 1991. \"Conditional Heteroskedasticity in Asset Returns: A New Approach.\" Econometrica 59 (2): 347–370. https://doi.org/10.2307/2938260\nTaylor, Stephen J. 1986. Modelling Financial Time Series. Chichester: Wiley. ISBN 978-0-471-90975-7.","category":"section"},{"location":"#Core-Methodology","page":"Home","title":"Core Methodology","text":"Blanchard, Olivier Jean, and Danny Quah. 1989. \"The Dynamic Effects of Aggregate Demand and Supply Disturbances.\" American Economic Review 79 (4): 655–673.\nHamilton, James D. 1994. Time Series Analysis. Princeton, NJ: Princeton University Press. ISBN 978-0-691-04289-3.\nKilian, Lutz, and Helmut Lütkepohl. 2017. Structural Vector Autoregressive Analysis. Cambridge: Cambridge University Press. https://doi.org/10.1017/9781108164818\nLütkepohl, Helmut. 2005. New Introduction to Multiple Time Series Analysis. Berlin: Springer. ISBN 978-3-540-40172-8.\nSims, Christopher A. 1980. \"Macroeconomics and Reality.\" Econometrica 48 (1): 1–48. https://doi.org/10.2307/1912017","category":"section"},{"location":"#Bayesian-Methods","page":"Home","title":"Bayesian Methods","text":"Doan, Thomas, Robert Litterman, and Christopher Sims. 1984. \"Forecasting and Conditional Projection Using Realistic Prior Distributions.\" Econometric Reviews 3 (1): 1–100. https://doi.org/10.1080/07474938408800053\nGiannone, Domenico, Michele Lenza, and Giorgio E. Primiceri. 2015. \"Prior Selection for Vector Autoregressions.\" Review of Economics and Statistics 97 (2): 436–451. https://doi.org/10.1162/RESTa00483\nLitterman, Robert B. 1986. \"Forecasting with Bayesian Vector Autoregressions—Five Years of Experience.\" Journal of Business & Economic Statistics 4 (1): 25–38. https://doi.org/10.1080/07350015.1986.10509491","category":"section"},{"location":"#Local-Projections-2","page":"Home","title":"Local Projections","text":"Angrist, Joshua D., Òscar Jordà, and Guido M. Kuersteiner. 2018. \"Semiparametric Estimates of Monetary Policy Effects: String Theory Revisited.\" Journal of Business & Economic Statistics 36 (3): 371–387. https://doi.org/10.1080/07350015.2016.1204919\nAuerbach, Alan J., and Yuriy Gorodnichenko. 2013. \"Fiscal Multipliers in Recession and Expansion.\" In Fiscal Policy after the Financial Crisis, edited by Alberto Alesina and Francesco Giavazzi, 63–98. Chicago: University of Chicago Press. https://doi.org/10.7208/9780226018584-004\nBarnichon, Regis, and Christian Brownlees. 2019. \"Impulse Response Estimation by Smooth Local Projections.\" Review of Economics and Statistics 101 (3): 522–530. https://doi.org/10.1162/resta00778\nJordà, Òscar. 2005. \"Estimation and Inference of Impulse Responses by Local Projections.\" American Economic Review 95 (1): 161–182. https://doi.org/10.1257/0002828053828518\nStock, James H., and Mark W. Watson. 2018. \"Identification and Estimation of Dynamic Causal Effects in Macroeconomics Using External Instruments.\" Economic Journal 128 (610): 917–948. https://doi.org/10.1111/ecoj.12593\nPlagborg-Møller, Mikkel, and Christian K. Wolf. 2021. \"Local Projections and VARs Estimate the Same Impulse Responses.\" Econometrica 89 (2): 955–980. https://doi.org/10.3982/ECTA17813\nGorodnichenko, Yuriy, and Byoungchan Lee. 2019. \"Forecast Error Variance Decompositions with Local Projections.\" Journal of Business & Economic Statistics 38 (4): 921–933. https://doi.org/10.1080/07350015.2019.1610661","category":"section"},{"location":"#Factor-Models-2","page":"Home","title":"Factor Models","text":"Bai, Jushan, and Serena Ng. 2002. \"Determining the Number of Factors in Approximate Factor Models.\" Econometrica 70 (1): 191–221. https://doi.org/10.1111/1468-0262.00273\nStock, James H., and Mark W. Watson. 2002. \"Forecasting Using Principal Components from a Large Number of Predictors.\" Journal of the American Statistical Association 97 (460): 1167–1179. https://doi.org/10.1198/016214502388618960","category":"section"},{"location":"#Robust-Inference","page":"Home","title":"Robust Inference","text":"Andrews, Donald W. K. 1991. \"Heteroskedasticity and Autocorrelation Consistent Covariance Matrix Estimation.\" Econometrica 59 (3): 817–858. https://doi.org/10.2307/2938229\nHansen, Lars Peter. 1982. \"Large Sample Properties of Generalized Method of Moments Estimators.\" Econometrica 50 (4): 1029–1054. https://doi.org/10.2307/1912775\nNewey, Whitney K., and Kenneth D. West. 1987. \"A Simple, Positive Semi-Definite, Heteroskedasticity and Autocorrelation Consistent Covariance Matrix.\" Econometrica 55 (3): 703–708. https://doi.org/10.2307/1913610\nNewey, Whitney K., and Kenneth D. West. 1994. \"Automatic Lag Selection in Covariance Matrix Estimation.\" Review of Economic Studies 61 (4): 631–653. https://doi.org/10.2307/2297912","category":"section"},{"location":"#License","page":"Home","title":"License","text":"This package is released under the MIT License.","category":"section"},{"location":"#Contributing","page":"Home","title":"Contributing","text":"Contributions are welcome! Please see the GitHub repository for contribution guidelines.","category":"section"},{"location":"#Contents","page":"Home","title":"Contents","text":"Pages = [\"arima.md\", \"volatility.md\", \"manual.md\", \"lp.md\", \"factormodels.md\", \"bayesian.md\", \"innovation_accounting.md\", \"nongaussian.md\", \"hypothesis_tests.md\", \"examples.md\", \"api.md\"]\nDepth = 2","category":"section"}]
}
