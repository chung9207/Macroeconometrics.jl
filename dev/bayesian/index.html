<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Bayesian VAR · MacroEconometricModels.jl</title><meta name="title" content="Bayesian VAR · MacroEconometricModels.jl"/><meta property="og:title" content="Bayesian VAR · MacroEconometricModels.jl"/><meta property="twitter:title" content="Bayesian VAR · MacroEconometricModels.jl"/><meta name="description" content="Documentation for MacroEconometricModels.jl."/><meta property="og:description" content="Documentation for MacroEconometricModels.jl."/><meta property="twitter:description" content="Documentation for MacroEconometricModels.jl."/><meta property="og:url" content="https://chung9207.github.io/MacroEconometricModels.jl/bayesian/"/><meta property="twitter:url" content="https://chung9207.github.io/MacroEconometricModels.jl/bayesian/"/><link rel="canonical" href="https://chung9207.github.io/MacroEconometricModels.jl/bayesian/"/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">MacroEconometricModels.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><span class="tocitem">Univariate Models</span><ul><li><a class="tocitem" href="../arima/">ARIMA</a></li><li><a class="tocitem" href="../volatility/">Volatility Models</a></li></ul></li><li><span class="tocitem">Frequentist Models</span><ul><li><a class="tocitem" href="../manual/">VAR</a></li><li><a class="tocitem" href="../lp/">Local Projections</a></li><li><a class="tocitem" href="../factormodels/">Factor Models</a></li></ul></li><li><span class="tocitem">Bayesian Models</span><ul><li class="is-active"><a class="tocitem" href>Bayesian VAR</a><ul class="internal"><li><a class="tocitem" href="#Introduction"><span>Introduction</span></a></li><li><a class="tocitem" href="#Quick-Start"><span>Quick Start</span></a></li><li><a class="tocitem" href="#Bayesian-Framework"><span>Bayesian Framework</span></a></li><li><a class="tocitem" href="#The-Minnesota-Prior"><span>The Minnesota Prior</span></a></li><li><a class="tocitem" href="#Dummy-Observations-Approach"><span>Dummy Observations Approach</span></a></li><li><a class="tocitem" href="#Hyperparameter-Optimization"><span>Hyperparameter Optimization</span></a></li><li><a class="tocitem" href="#MCMC-Estimation-with-Turing.jl"><span>MCMC Estimation with Turing.jl</span></a></li><li><a class="tocitem" href="#Posterior-Point-Estimates"><span>Posterior Point Estimates</span></a></li><li><a class="tocitem" href="#Bayesian-Impulse-Response-Functions"><span>Bayesian Impulse Response Functions</span></a></li><li><a class="tocitem" href="#Bayesian-FEVD"><span>Bayesian FEVD</span></a></li><li><a class="tocitem" href="#Information-Criteria"><span>Information Criteria</span></a></li><li><a class="tocitem" href="#Complete-Example"><span>Complete Example</span></a></li><li><a class="tocitem" href="#Large-BVAR"><span>Large BVAR</span></a></li><li><a class="tocitem" href="#References"><span>References</span></a></li></ul></li></ul></li><li><a class="tocitem" href="../innovation_accounting/">Innovation Accounting</a></li><li><a class="tocitem" href="../nongaussian/">Non-Gaussian Structural Identification</a></li><li><span class="tocitem">Hypothesis Tests</span><ul><li><a class="tocitem" href="../hypothesis_tests/">Unit Root &amp; Cointegration</a></li></ul></li><li><a class="tocitem" href="../examples/">Examples</a></li><li><span class="tocitem">API Reference</span><ul><li><a class="tocitem" href="../api/">Overview</a></li><li><a class="tocitem" href="../api_types/">Types</a></li><li><a class="tocitem" href="../api_functions/">Functions</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Bayesian Models</a></li><li class="is-active"><a href>Bayesian VAR</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Bayesian VAR</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/chung9207/MacroEconometricModels.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/chung9207/MacroEconometricModels.jl/blob/main/docs/src/bayesian.md#" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Bayesian-VAR-(BVAR)"><a class="docs-heading-anchor" href="#Bayesian-VAR-(BVAR)">Bayesian VAR (BVAR)</a><a id="Bayesian-VAR-(BVAR)-1"></a><a class="docs-heading-anchor-permalink" href="#Bayesian-VAR-(BVAR)" title="Permalink"></a></h1><p>This chapter covers Bayesian estimation methods for Vector Autoregression models, including the Minnesota prior, hyperparameter optimization, and MCMC inference.</p><h2 id="Introduction"><a class="docs-heading-anchor" href="#Introduction">Introduction</a><a id="Introduction-1"></a><a class="docs-heading-anchor-permalink" href="#Introduction" title="Permalink"></a></h2><p>Bayesian VAR (BVAR) estimation addresses the curse of dimensionality in VAR models by incorporating prior information to shrink coefficient estimates. This is particularly valuable when:</p><ol><li>The number of parameters is large relative to sample size</li><li>Prior economic knowledge should influence estimation</li><li>Uncertainty quantification via posterior distributions is desired</li><li>Forecasting performance is paramount</li></ol><p><strong>Key References</strong>: Litterman (1986), Doan, Litterman &amp; Sims (1984), Giannone, Lenza &amp; Primiceri (2015)</p><h2 id="Quick-Start"><a class="docs-heading-anchor" href="#Quick-Start">Quick Start</a><a id="Quick-Start-1"></a><a class="docs-heading-anchor-permalink" href="#Quick-Start" title="Permalink"></a></h2><pre><code class="language-julia hljs">hyper = MinnesotaHyperparameters(tau=0.5, decay=2.0, lambda=1.0, mu=1.0, omega=1.0)
best = optimize_hyperparameters(Y, p; grid_size=20)                 # Optimize tau
chain = estimate_bvar(Y, 2; n_samples=2000, prior=:minnesota, hyper=best)
birf = irf(chain, 2, 3, 20; method=:cholesky)                      # Bayesian IRF
bfevd = fevd(chain, 2, 3, 20)                                      # Bayesian FEVD</code></pre><hr/><h2 id="Bayesian-Framework"><a class="docs-heading-anchor" href="#Bayesian-Framework">Bayesian Framework</a><a id="Bayesian-Framework-1"></a><a class="docs-heading-anchor-permalink" href="#Bayesian-Framework" title="Permalink"></a></h2><h3 id="The-Prior-Likelihood-Posterior-Paradigm"><a class="docs-heading-anchor" href="#The-Prior-Likelihood-Posterior-Paradigm">The Prior-Likelihood-Posterior Paradigm</a><a id="The-Prior-Likelihood-Posterior-Paradigm-1"></a><a class="docs-heading-anchor-permalink" href="#The-Prior-Likelihood-Posterior-Paradigm" title="Permalink"></a></h3><p>In the Bayesian approach, we treat the VAR parameters as random variables and update our beliefs using Bayes&#39; theorem:</p><p class="math-container">\[p(B, \Sigma | Y) \propto p(Y | B, \Sigma) \cdot p(B, \Sigma)\]</p><p>where:</p><ul><li><span>$p(Y | B, \Sigma)$</span> is the likelihood</li><li><span>$p(B, \Sigma)$</span> is the prior</li><li><span>$p(B, \Sigma | Y)$</span> is the posterior</li></ul><h3 id="Natural-Conjugate-Prior"><a class="docs-heading-anchor" href="#Natural-Conjugate-Prior">Natural Conjugate Prior</a><a id="Natural-Conjugate-Prior-1"></a><a class="docs-heading-anchor-permalink" href="#Natural-Conjugate-Prior" title="Permalink"></a></h3><p>For computational convenience, we use the Normal-Inverse-Wishart conjugate prior:</p><p class="math-container">\[\Sigma \sim \text{IW}(\nu_0, S_0)\]</p><p class="math-container">\[\text{vec}(B) | \Sigma \sim N(\text{vec}(B_0), \Sigma \otimes \Omega_0)\]</p><p>This yields a closed-form posterior of the same family.</p><hr/><h2 id="The-Minnesota-Prior"><a class="docs-heading-anchor" href="#The-Minnesota-Prior">The Minnesota Prior</a><a id="The-Minnesota-Prior-1"></a><a class="docs-heading-anchor-permalink" href="#The-Minnesota-Prior" title="Permalink"></a></h2><h3 id="Motivation"><a class="docs-heading-anchor" href="#Motivation">Motivation</a><a id="Motivation-1"></a><a class="docs-heading-anchor-permalink" href="#Motivation" title="Permalink"></a></h3><p>The <strong>Minnesota prior</strong> (Litterman, 1986; Doan, Litterman &amp; Sims, 1984) shrinks VAR coefficients toward a random walk prior. This reflects the empirical observation that many macroeconomic variables are well-approximated by random walks, especially at short horizons.</p><h3 id="Prior-Specification"><a class="docs-heading-anchor" href="#Prior-Specification">Prior Specification</a><a id="Prior-Specification-1"></a><a class="docs-heading-anchor-permalink" href="#Prior-Specification" title="Permalink"></a></h3><p><strong>Prior Mean</strong>: Each variable follows a random walk:</p><p class="math-container">\[E[A_{1,ii}] = 1, \quad E[A_{1,ij}] = 0 \text{ for } i \neq j, \quad E[A_l] = 0 \text{ for } l &gt; 1\]</p><p><strong>Prior Variance</strong>: The prior variance for coefficient <span>$(i,j)$</span> at lag <span>$l$</span> is:</p><p class="math-container">\[\text{Var}(A_{l,ij}) = \begin{cases}
\frac{\tau^2}{l^d} &amp; \text{if } i = j \text{ (own lag)} \\
\frac{\tau^2 \omega^2}{l^d} \cdot \frac{\sigma_i^2}{\sigma_j^2} &amp; \text{if } i \neq j \text{ (cross lag)}
\end{cases}\]</p><p>where:</p><ul><li><span>$\tau$</span> is the <strong>overall tightness</strong> (shrinkage intensity)</li><li><span>$d$</span> is the <strong>lag decay</strong> (typically <span>$d = 2$</span>)</li><li><span>$\omega$</span> controls <strong>cross-variable shrinkage</strong> (typically <span>$\omega &lt; 1$</span>)</li><li><span>$\sigma_i^2$</span> is the residual variance from a univariate AR(1) for variable <span>$i$</span></li></ul><h3 id="Interpretation-of-Hyperparameters"><a class="docs-heading-anchor" href="#Interpretation-of-Hyperparameters">Interpretation of Hyperparameters</a><a id="Interpretation-of-Hyperparameters-1"></a><a class="docs-heading-anchor-permalink" href="#Interpretation-of-Hyperparameters" title="Permalink"></a></h3><table><tr><th style="text-align: right">Parameter</th><th style="text-align: right">Effect</th><th style="text-align: right">Typical Values</th></tr><tr><td style="text-align: right"><span>$\tau$</span></td><td style="text-align: right">Overall shrinkage (lower = more shrinkage)</td><td style="text-align: right">0.01 – 1.0</td></tr><tr><td style="text-align: right"><span>$d$</span></td><td style="text-align: right">Lag decay (higher = faster decay)</td><td style="text-align: right">1, 2, 3</td></tr><tr><td style="text-align: right"><span>$\omega$</span></td><td style="text-align: right">Cross-variable penalty (lower = more penalty)</td><td style="text-align: right">0.5 – 1.0</td></tr></table><h3 id="Julia-Implementation"><a class="docs-heading-anchor" href="#Julia-Implementation">Julia Implementation</a><a id="Julia-Implementation-1"></a><a class="docs-heading-anchor-permalink" href="#Julia-Implementation" title="Permalink"></a></h3><pre><code class="language-julia hljs">using MacroEconometricModels

# Define hyperparameters
hyper = MinnesotaHyperparameters(
    tau = 0.5,      # Overall tightness
    decay = 2.0,    # Lag decay
    lambda = 1.0,   # Own-lag variance scaling
    mu = 1.0,       # Cross-lag variance scaling
    omega = 1.0     # Deterministic terms scaling
)

# Use in BVAR estimation
chain = estimate_bvar(Y, 2; n_samples=2000, n_adapts=500,
                      prior=:minnesota, hyper=hyper)</code></pre><pre><code class="nohighlight hljs"># Output:
# MinnesotaHyperparameters{Float64}(tau=0.5, decay=2.0, lambda=1.0, mu=1.0, omega=1.0)</code></pre><p>The <code>tau=0.5</code> setting provides moderate shrinkage — coefficient estimates will be pulled halfway between the data-driven OLS estimates and the random walk prior. With <code>decay=2.0</code>, the prior variance for lag-<span>$l$</span> coefficients decays as <span>$1/l^2$</span>, so distant lags are strongly penalized. Setting <code>mu=1.0</code> treats cross-variable lags the same as own lags; reducing <code>mu</code> (e.g., to 0.5) would impose stronger shrinkage on cross-variable coefficients, reflecting the common finding that own lags are more informative than other variables&#39; lags.</p><h3 id="MinnesotaHyperparameters-Return-Values"><a class="docs-heading-anchor" href="#MinnesotaHyperparameters-Return-Values">MinnesotaHyperparameters Return Values</a><a id="MinnesotaHyperparameters-Return-Values-1"></a><a class="docs-heading-anchor-permalink" href="#MinnesotaHyperparameters-Return-Values" title="Permalink"></a></h3><table><tr><th style="text-align: right">Field</th><th style="text-align: right">Type</th><th style="text-align: right">Description</th></tr><tr><td style="text-align: right"><code>tau</code></td><td style="text-align: right"><code>T</code></td><td style="text-align: right">Overall tightness (lower = more shrinkage toward prior)</td></tr><tr><td style="text-align: right"><code>decay</code></td><td style="text-align: right"><code>T</code></td><td style="text-align: right">Lag decay exponent (higher = faster decay of lag importance)</td></tr><tr><td style="text-align: right"><code>lambda</code></td><td style="text-align: right"><code>T</code></td><td style="text-align: right">Own-lag variance scaling</td></tr><tr><td style="text-align: right"><code>mu</code></td><td style="text-align: right"><code>T</code></td><td style="text-align: right">Cross-lag variance scaling (lower = more penalty on cross-variable lags)</td></tr><tr><td style="text-align: right"><code>omega</code></td><td style="text-align: right"><code>T</code></td><td style="text-align: right">Deterministic terms scaling</td></tr></table><hr/><h2 id="Dummy-Observations-Approach"><a class="docs-heading-anchor" href="#Dummy-Observations-Approach">Dummy Observations Approach</a><a id="Dummy-Observations-Approach-1"></a><a class="docs-heading-anchor-permalink" href="#Dummy-Observations-Approach" title="Permalink"></a></h2><h3 id="Implementation-via-Augmented-Regression"><a class="docs-heading-anchor" href="#Implementation-via-Augmented-Regression">Implementation via Augmented Regression</a><a id="Implementation-via-Augmented-Regression-1"></a><a class="docs-heading-anchor-permalink" href="#Implementation-via-Augmented-Regression" title="Permalink"></a></h3><p>We implement the Minnesota prior using dummy observations (Theil-Goldberger mixed estimation). The augmented data matrices are:</p><p><strong>Prior on coefficients</strong> (tightness dummies):</p><p class="math-container">\[Y_d = \begin{bmatrix}
\text{diag}(\sigma_1, \ldots, \sigma_n) / \tau \\
0_{n(p-1) \times n} \\
\text{diag}(\sigma_1, \ldots, \sigma_n) \\
0_{1 \times n}
\end{bmatrix}, \quad
X_d = \begin{bmatrix}
0_{n \times 1} &amp; J_p \otimes \text{diag}(\sigma_1, \ldots, \sigma_n) / \tau \\
0_{n(p-1) \times 1} &amp; I_{p-1} \otimes \text{diag}(\sigma_1, \ldots, \sigma_n) \\
0_{n \times 1} &amp; 0_{n \times np} \\
c &amp; 0_{1 \times np}
\end{bmatrix}\]</p><p>where <span>$J_p = \text{diag}(1, 2^d, \ldots, p^d)$</span>.</p><p>The posterior is then computed as OLS on the augmented data <span>$[Y; Y_d]$</span> and <span>$[X; X_d]$</span>.</p><p><strong>Reference</strong>: Litterman (1986), Kadiyala &amp; Karlsson (1997), Bańbura, Giannone &amp; Reichlin (2010)</p><h3 id="Julia-Implementation-2"><a class="docs-heading-anchor" href="#Julia-Implementation-2">Julia Implementation</a><a class="docs-heading-anchor-permalink" href="#Julia-Implementation-2" title="Permalink"></a></h3><pre><code class="language-julia hljs">using MacroEconometricModels

# Generate dummy observations for Minnesota prior
Y_dummy, X_dummy = gen_dummy_obs(Y, p, hyper)

# Augment data
Y_aug = vcat(Y_actual, Y_dummy)
X_aug = vcat(X_actual, X_dummy)

# Posterior via OLS on augmented data
B_post = (X_aug&#39;X_aug) \ (X_aug&#39;Y_aug)</code></pre><pre><code class="nohighlight hljs"># Output (for n=3, p=2):
# size(Y_dummy) = (10, 3)    # 3n+n+1 = 10 dummy observations
# size(X_dummy) = (10, 7)    # 1 + np = 7 regressors</code></pre><p>The dummy observations encode the prior belief: the tightness dummies pull <span>$A_1$</span> toward the identity (random walk), while the decay dummies shrink higher-lag coefficients toward zero. Augmenting the data with these pseudo-observations and running OLS on the combined system is algebraically equivalent to computing the posterior mean under the Normal-Inverse-Wishart conjugate prior.</p><hr/><h2 id="Hyperparameter-Optimization"><a class="docs-heading-anchor" href="#Hyperparameter-Optimization">Hyperparameter Optimization</a><a id="Hyperparameter-Optimization-1"></a><a class="docs-heading-anchor-permalink" href="#Hyperparameter-Optimization" title="Permalink"></a></h2><h3 id="Marginal-Likelihood"><a class="docs-heading-anchor" href="#Marginal-Likelihood">Marginal Likelihood</a><a id="Marginal-Likelihood-1"></a><a class="docs-heading-anchor-permalink" href="#Marginal-Likelihood" title="Permalink"></a></h3><p>Rather than selecting <span>$\tau$</span> subjectively, we can optimize it by maximizing the marginal likelihood (Giannone, Lenza &amp; Primiceri, 2015):</p><p class="math-container">\[p(Y | \tau) = \int p(Y | B, \Sigma) p(B, \Sigma | \tau) \, dB \, d\Sigma\]</p><p>For the Normal-Inverse-Wishart prior with dummy observations, the log marginal likelihood has an analytical form:</p><p class="math-container">\[\log p(Y | \tau) = c + \frac{T-k}{2} \log|\tilde{S}^{-1}| - \frac{T_d}{2} \log|\tilde{S}_d^{-1}| + \log \frac{\Gamma_n(\frac{T+T_d - k}{2})}{\Gamma_n(\frac{T_d - k}{2})}\]</p><p>where</p><ul><li><span>$c$</span> is a normalization constant</li><li><span>$T$</span> is the sample size, <span>$k = 1 + np$</span> is the number of regressors per equation</li><li><span>$T_d$</span> is the number of dummy observations</li><li><span>$\tilde{S}$</span> is the residual sum of squares from the augmented regression <span>$[Y; Y_d]$</span> on <span>$[X; X_d]$</span></li><li><span>$\tilde{S}_d$</span> is the residual sum of squares from the dummy-only regression</li><li><span>$\Gamma_n(\cdot)$</span> is the multivariate gamma function</li></ul><p><strong>Reference</strong>: Giannone, Lenza &amp; Primiceri (2015), Carriero, Clark &amp; Marcellino (2015)</p><h3 id="Julia-Implementation-3"><a class="docs-heading-anchor" href="#Julia-Implementation-3">Julia Implementation</a><a class="docs-heading-anchor-permalink" href="#Julia-Implementation-3" title="Permalink"></a></h3><pre><code class="language-julia hljs">using MacroEconometricModels

# Find optimal shrinkage using marginal likelihood
best_hyper = optimize_hyperparameters(Y, p; grid_size=20)

println(&quot;Optimal hyperparameters:&quot;)
println(&quot;  τ (overall tightness): &quot;, round(best_hyper.tau, digits=4))
println(&quot;  d (lag decay): &quot;, best_hyper.d)

# Compute log marginal likelihood
lml = log_marginal_likelihood(Y, p, hyper)</code></pre><pre><code class="nohighlight hljs"># Output:
# Optimal hyperparameters:
#   τ (overall tightness): 0.2143
#   d (lag decay): 2.0</code></pre><p>The optimal <span>$\tau$</span> balances fit and complexity: values near 0.01 produce near-dogmatic shrinkage to the random walk prior (good for high-dimensional systems), while values near 1.0 produce minimal shrinkage (approaching OLS). The marginal likelihood automatically penalizes overfitting, so the optimal <span>$\tau$</span> increases with sample size as data evidence accumulates.</p><h3 id="Grid-Search-Options"><a class="docs-heading-anchor" href="#Grid-Search-Options">Grid Search Options</a><a id="Grid-Search-Options-1"></a><a class="docs-heading-anchor-permalink" href="#Grid-Search-Options" title="Permalink"></a></h3><pre><code class="language-julia hljs"># Custom optimization grid
best_hyper = optimize_hyperparameters(Y, p;
    grid_size = 30,           # Number of grid points
    tau_range = (0.01, 2.0),  # Range for τ
    d_values = [1, 2, 3]      # Values for d
)</code></pre><hr/><h2 id="MCMC-Estimation-with-Turing.jl"><a class="docs-heading-anchor" href="#MCMC-Estimation-with-Turing.jl">MCMC Estimation with Turing.jl</a><a id="MCMC-Estimation-with-Turing.jl-1"></a><a class="docs-heading-anchor-permalink" href="#MCMC-Estimation-with-Turing.jl" title="Permalink"></a></h2><h3 id="The-BVAR-Model"><a class="docs-heading-anchor" href="#The-BVAR-Model">The BVAR Model</a><a id="The-BVAR-Model-1"></a><a class="docs-heading-anchor-permalink" href="#The-BVAR-Model" title="Permalink"></a></h3><p>For more flexible priors or non-conjugate settings, we use MCMC via Turing.jl with the NUTS sampler:</p><pre><code class="language-julia hljs">@model function bvar_model(Y, X, prior_mean, prior_var, ν₀, S₀)
    n = size(Y, 2)
    k = size(X, 2)

    # Prior on error covariance
    Σ ~ InverseWishart(ν₀, S₀)

    # Prior on coefficients
    B ~ MatrixNormal(prior_mean, prior_var, Σ)

    # Likelihood
    for t in axes(Y, 1)
        Y[t, :] ~ MvNormal(X[t, :]&#39; * B, Σ)
    end
end</code></pre><h3 id="Julia-Implementation-4"><a class="docs-heading-anchor" href="#Julia-Implementation-4">Julia Implementation</a><a class="docs-heading-anchor-permalink" href="#Julia-Implementation-4" title="Permalink"></a></h3><pre><code class="language-julia hljs">using MacroEconometricModels

# Estimate BVAR with MCMC
chain = estimate_bvar(Y, p;
    n_samples = 2000,     # Posterior samples
    n_adapts = 500,       # Adaptation samples
    prior = :minnesota,   # Prior type
    hyper = best_hyper    # Hyperparameters
)

# Access posterior draws
# chain.samples contains the MCMC draws</code></pre><pre><code class="nohighlight hljs"># Output:
# Sampling: 100%|████████████████████████████████| Time: 0:00:45
# Chains MCMC chain (2000×21×1 Array{Float64, 3})
# Summary Statistics
#   parameters     mean     std   naive_se    mcse      ess    rhat
#   ──────────────────────────────────────────────────────────────
#   B[1,1]       0.0142  0.0823    0.0018   0.0025   1089.2  1.001
#   B[2,1]       0.4876  0.0614    0.0014   0.0019   1234.5  1.000
#   ...</code></pre><p>The MCMC output shows the posterior mean, standard deviation, and convergence diagnostics for each parameter. The <code>ess</code> (effective sample size) should be at least 400 for reliable quantile estimation, and <code>rhat</code> should be below 1.05 for all parameters — values above 1.1 indicate poor mixing.</p><h3 id="Convergence-Diagnostics"><a class="docs-heading-anchor" href="#Convergence-Diagnostics">Convergence Diagnostics</a><a id="Convergence-Diagnostics-1"></a><a class="docs-heading-anchor-permalink" href="#Convergence-Diagnostics" title="Permalink"></a></h3><pre><code class="language-julia hljs"># Extract chain parameters
params = extract_chain_parameters(chain)

# Check R-hat statistics
# Check effective sample sizes
# Trace plots for visual inspection</code></pre><div class="admonition is-info" id="Technical-Note-8c6726618aa2bae2"><header class="admonition-header">Technical Note<a class="admonition-anchor" href="#Technical-Note-8c6726618aa2bae2" title="Permalink"></a></header><div class="admonition-body"><p>MCMC convergence should be assessed before interpreting results. Key diagnostics include: (1) <span>$\hat{R}$</span> (R-hat) statistics should be below 1.05 for all parameters; (2) effective sample size (ESS) should be at least 400 for reliable posterior quantile estimation; (3) trace plots should show good mixing without trends or multimodality. If <code>n_samples=2000</code> with <code>n_adapts=500</code> shows poor convergence, try increasing both values or switching to a more informative prior (lower <code>tau</code>).</p></div></div><p><strong>Reference</strong>: Gelman et al. (2013), Hoffman &amp; Gelman (2014)</p><hr/><h2 id="Posterior-Point-Estimates"><a class="docs-heading-anchor" href="#Posterior-Point-Estimates">Posterior Point Estimates</a><a id="Posterior-Point-Estimates-1"></a><a class="docs-heading-anchor-permalink" href="#Posterior-Point-Estimates" title="Permalink"></a></h2><h3 id="Extracting-VARModel-from-MCMC-Chain"><a class="docs-heading-anchor" href="#Extracting-VARModel-from-MCMC-Chain">Extracting VARModel from MCMC Chain</a><a id="Extracting-VARModel-from-MCMC-Chain-1"></a><a class="docs-heading-anchor-permalink" href="#Extracting-VARModel-from-MCMC-Chain" title="Permalink"></a></h3><p>After MCMC estimation, it is often useful to obtain a single <code>VARModel</code> based on the posterior mean or median. This allows using all frequentist tools (IRF, FEVD, HD, stationarity checks) on the Bayesian point estimate.</p><pre><code class="language-julia hljs">using MacroEconometricModels

# After running estimate_bvar:
# chain = estimate_bvar(Y, p; n_samples=2000, prior=:minnesota, hyper=hyper)

# Extract VARModel with posterior mean parameters
mean_model = posterior_mean_model(chain, p, n; data=Y)

# Extract VARModel with posterior median parameters
median_model = posterior_median_model(chain, p, n; data=Y)

# Now use standard VAR tools
stab = is_stationary(mean_model)
println(&quot;Posterior mean model stationary: &quot;, stab.is_stationary)
println(&quot;Max eigenvalue modulus: &quot;, round(stab.max_modulus, digits=4))

# Frequentist IRF from the posterior mean
irfs_mean = irf(mean_model, 20; method=:cholesky)</code></pre><p>The <code>posterior_mean_model</code> averages the coefficient matrix <span>$B$</span> and covariance <span>$\Sigma$</span> across all MCMC draws, providing a single point estimate that integrates over parameter uncertainty. The <code>posterior_median_model</code> uses the element-wise median instead, which is more robust to outlier draws but may produce a <span>$\Sigma$</span> that is not positive definite in edge cases. When <code>data=Y</code> is provided, the function also computes residuals, enabling <code>historical_decomposition</code> and other residual-based analyses.</p><hr/><h2 id="Bayesian-Impulse-Response-Functions"><a class="docs-heading-anchor" href="#Bayesian-Impulse-Response-Functions">Bayesian Impulse Response Functions</a><a id="Bayesian-Impulse-Response-Functions-1"></a><a class="docs-heading-anchor-permalink" href="#Bayesian-Impulse-Response-Functions" title="Permalink"></a></h2><h3 id="Posterior-IRF-Distribution"><a class="docs-heading-anchor" href="#Posterior-IRF-Distribution">Posterior IRF Distribution</a><a id="Posterior-IRF-Distribution-1"></a><a class="docs-heading-anchor-permalink" href="#Posterior-IRF-Distribution" title="Permalink"></a></h3><p>For each MCMC draw, we compute impulse responses, yielding a posterior distribution over IRFs. We report:</p><ul><li><strong>Posterior median</strong>: Point estimate</li><li><strong>Credible intervals</strong>: 68% (16th-84th percentile) or 90% (5th-95th percentile)</li></ul><h3 id="Cholesky-Identification"><a class="docs-heading-anchor" href="#Cholesky-Identification">Cholesky Identification</a><a id="Cholesky-Identification-1"></a><a class="docs-heading-anchor-permalink" href="#Cholesky-Identification" title="Permalink"></a></h3><pre><code class="language-julia hljs">using MacroEconometricModels

# Bayesian IRF with Cholesky identification
H = 20  # Horizon
birf_chol = irf(chain, p, n, H; method=:cholesky)

# birf_chol.quantiles is (H+1) × n × n × 3 array
# [:, :, :, 1] = 16th percentile
# [:, :, :, 2] = median
# [:, :, :, 3] = 84th percentile

println(&quot;Bayesian IRF of GDP to own shock:&quot;)
for h in [0, 4, 8, 12, 20]
    med = round(birf_chol.quantiles[h+1, 1, 1, 2], digits=3)
    lo = round(birf_chol.quantiles[h+1, 1, 1, 1], digits=3)
    hi = round(birf_chol.quantiles[h+1, 1, 1, 3], digits=3)
    println(&quot;  h=$h: $med [$lo, $hi]&quot;)
end</code></pre><pre><code class="nohighlight hljs"># Output:
# Bayesian IRF of GDP to own shock:
#   h=0: 0.312 [0.278, 0.347]
#   h=4: 0.048 [0.011, 0.089]
#   h=8: 0.006 [-0.015, 0.028]
#   h=12: 0.001 [-0.012, 0.014]
#   h=20: 0.000 [-0.006, 0.007]</code></pre><p>The posterior median IRF at <span>$h = 0$</span> reflects the impact effect of a one-standard-deviation structural shock. The 68% credible interval <span>$[\text{16th}, \text{84th}]$</span> narrows toward zero as the horizon increases, consistent with a stationary VAR where shocks dissipate over time. Unlike frequentist bootstrap CIs, Bayesian credible intervals integrate over parameter uncertainty in <span>$B$</span> and <span>$\Sigma$</span>, often producing wider bands at short horizons.</p><h3 id="BayesianImpulseResponse-Return-Values"><a class="docs-heading-anchor" href="#BayesianImpulseResponse-Return-Values">BayesianImpulseResponse Return Values</a><a id="BayesianImpulseResponse-Return-Values-1"></a><a class="docs-heading-anchor-permalink" href="#BayesianImpulseResponse-Return-Values" title="Permalink"></a></h3><table><tr><th style="text-align: right">Field</th><th style="text-align: right">Type</th><th style="text-align: right">Description</th></tr><tr><td style="text-align: right"><code>quantiles</code></td><td style="text-align: right"><code>Array{T,4}</code></td><td style="text-align: right"><span>$(H+1) \times n \times n \times 3$</span>: dim 4 = [16th pctl, median, 84th pctl]</td></tr><tr><td style="text-align: right"><code>mean</code></td><td style="text-align: right"><code>Array{T,3}</code></td><td style="text-align: right"><span>$(H+1) \times n \times n$</span> posterior mean IRF</td></tr><tr><td style="text-align: right"><code>horizon</code></td><td style="text-align: right"><code>Int</code></td><td style="text-align: right">Maximum IRF horizon</td></tr><tr><td style="text-align: right"><code>variables</code></td><td style="text-align: right"><code>Vector{String}</code></td><td style="text-align: right">Variable names</td></tr><tr><td style="text-align: right"><code>shocks</code></td><td style="text-align: right"><code>Vector{String}</code></td><td style="text-align: right">Shock names</td></tr><tr><td style="text-align: right"><code>quantile_levels</code></td><td style="text-align: right"><code>Vector{T}</code></td><td style="text-align: right">Quantile levels</td></tr></table><h3 id="Sign-Restrictions"><a class="docs-heading-anchor" href="#Sign-Restrictions">Sign Restrictions</a><a id="Sign-Restrictions-1"></a><a class="docs-heading-anchor-permalink" href="#Sign-Restrictions" title="Permalink"></a></h3><pre><code class="language-julia hljs"># Define sign restriction check function
function check_demand_shock(irf_array)
    # Demand shock: positive GDP and inflation on impact
    return irf_array[1, 1, 1] &gt; 0 &amp;&amp; irf_array[1, 2, 1] &gt; 0
end

# Bayesian IRF with sign restrictions
birf_sign = irf(chain, p, n, H;
    method = :sign,
    check_func = check_demand_shock
)

println(&quot;Bayesian sign-restricted demand shock → GDP:&quot;)
for h in [0, 4, 8, 12]
    med = round(birf_sign.quantiles[h+1, 1, 1, 2], digits=3)
    lo = round(birf_sign.quantiles[h+1, 1, 1, 1], digits=3)
    hi = round(birf_sign.quantiles[h+1, 1, 1, 3], digits=3)
    println(&quot;  h=$h: $med [$lo, $hi]&quot;)
end</code></pre><pre><code class="nohighlight hljs"># Output:
# Bayesian sign-restricted demand shock → GDP:
#   h=0: 0.295 [0.251, 0.342]
#   h=4: 0.052 [0.018, 0.094]
#   h=8: 0.008 [-0.011, 0.031]
#   h=12: 0.001 [-0.009, 0.015]</code></pre><p>The sign-restricted IRFs are set-identified: the credible intervals combine both parameter uncertainty (from MCMC) and identification uncertainty (from the rotation <span>$Q$</span>). The median tends to be slightly smaller than under Cholesky because the sign restrictions eliminate some extreme rotations.</p><hr/><h2 id="Bayesian-FEVD"><a class="docs-heading-anchor" href="#Bayesian-FEVD">Bayesian FEVD</a><a id="Bayesian-FEVD-1"></a><a class="docs-heading-anchor-permalink" href="#Bayesian-FEVD" title="Permalink"></a></h2><h3 id="Posterior-FEVD-Distribution"><a class="docs-heading-anchor" href="#Posterior-FEVD-Distribution">Posterior FEVD Distribution</a><a id="Posterior-FEVD-Distribution-1"></a><a class="docs-heading-anchor-permalink" href="#Posterior-FEVD-Distribution" title="Permalink"></a></h3><p>Similarly, forecast error variance decomposition can be computed for each posterior draw:</p><pre><code class="language-julia hljs">using MacroEconometricModels

# Bayesian FEVD
bfevd = fevd(chain, p, n, H; method=:cholesky)

# Report median and credible intervals
for h in [1, 4, 12, 20]
    println(&quot;FEVD at h=$h:&quot;)
    med = round(bfevd.quantiles[h, 1, 1, 2] * 100, digits=1)
    lo = round(bfevd.quantiles[h, 1, 1, 1] * 100, digits=1)
    hi = round(bfevd.quantiles[h, 1, 1, 3] * 100, digits=1)
    println(&quot;  Shock 1 → Var 1: $med% [$lo%, $hi%]&quot;)
end</code></pre><pre><code class="nohighlight hljs"># Output:
# FEVD at h=1:
#   Shock 1 → Var 1: 97.2% [93.1%, 99.4%]
# FEVD at h=4:
#   Shock 1 → Var 1: 88.5% [78.6%, 95.1%]
# FEVD at h=12:
#   Shock 1 → Var 1: 82.3% [68.2%, 92.7%]
# FEVD at h=20:
#   Shock 1 → Var 1: 80.1% [64.5%, 91.8%]</code></pre><p>At <span>$h = 1$</span>, own shocks dominate (97%), reflecting the Cholesky ordering where variable 1 is first. As the horizon increases, spillovers from other shocks erode the own-shock share. The wide credible intervals at long horizons reflect cumulating parameter uncertainty through the VMA representation. Bayesian FEVD credible intervals are typically wider than frequentist bootstrap CIs because they integrate over the full posterior distribution of <span>$(B, \Sigma)$</span>.</p><h3 id="BayesianFEVD-Return-Values"><a class="docs-heading-anchor" href="#BayesianFEVD-Return-Values">BayesianFEVD Return Values</a><a id="BayesianFEVD-Return-Values-1"></a><a class="docs-heading-anchor-permalink" href="#BayesianFEVD-Return-Values" title="Permalink"></a></h3><table><tr><th style="text-align: right">Field</th><th style="text-align: right">Type</th><th style="text-align: right">Description</th></tr><tr><td style="text-align: right"><code>quantiles</code></td><td style="text-align: right"><code>Array{T,4}</code></td><td style="text-align: right"><span>$H \times n \times n \times 3$</span>: dim 4 = [16th pctl, median, 84th pctl]</td></tr><tr><td style="text-align: right"><code>mean</code></td><td style="text-align: right"><code>Array{T,3}</code></td><td style="text-align: right"><span>$H \times n \times n$</span> posterior mean FEVD proportions</td></tr><tr><td style="text-align: right"><code>horizon</code></td><td style="text-align: right"><code>Int</code></td><td style="text-align: right">Maximum horizon</td></tr><tr><td style="text-align: right"><code>variables</code></td><td style="text-align: right"><code>Vector{String}</code></td><td style="text-align: right">Variable names</td></tr><tr><td style="text-align: right"><code>shocks</code></td><td style="text-align: right"><code>Vector{String}</code></td><td style="text-align: right">Shock names</td></tr><tr><td style="text-align: right"><code>quantile_levels</code></td><td style="text-align: right"><code>Vector{T}</code></td><td style="text-align: right">Quantile levels</td></tr></table><hr/><h2 id="Information-Criteria"><a class="docs-heading-anchor" href="#Information-Criteria">Information Criteria</a><a id="Information-Criteria-1"></a><a class="docs-heading-anchor-permalink" href="#Information-Criteria" title="Permalink"></a></h2><h3 id="Log-Likelihood"><a class="docs-heading-anchor" href="#Log-Likelihood">Log-Likelihood</a><a id="Log-Likelihood-1"></a><a class="docs-heading-anchor-permalink" href="#Log-Likelihood" title="Permalink"></a></h3><p>For a Gaussian VAR, the log-likelihood is:</p><p class="math-container">\[\log L = -\frac{T \cdot n}{2} \log(2\pi) - \frac{T}{2} \log|\Sigma| - \frac{1}{2} \sum_{t=1}^{T} u_t&#39; \Sigma^{-1} u_t\]</p><h3 id="Marginal-Likelihood-(Bayesian)"><a class="docs-heading-anchor" href="#Marginal-Likelihood-(Bayesian)">Marginal Likelihood (Bayesian)</a><a id="Marginal-Likelihood-(Bayesian)-1"></a><a class="docs-heading-anchor-permalink" href="#Marginal-Likelihood-(Bayesian)" title="Permalink"></a></h3><p>For Bayesian model comparison, we use the marginal likelihood (also called evidence):</p><p class="math-container">\[p(Y | \mathcal{M}) = \int p(Y | \theta, \mathcal{M}) p(\theta | \mathcal{M}) \, d\theta\]</p><p>Models with higher marginal likelihood better balance fit and complexity.</p><hr/><h2 id="Complete-Example"><a class="docs-heading-anchor" href="#Complete-Example">Complete Example</a><a id="Complete-Example-1"></a><a class="docs-heading-anchor-permalink" href="#Complete-Example" title="Permalink"></a></h2><pre><code class="language-julia hljs">using MacroEconometricModels
using Random

Random.seed!(42)

# Generate data
T, n, p = 200, 3, 2
Y = randn(T, n)
for t in 2:T
    Y[t, :] = 0.5 * Y[t-1, :] + 0.3 * randn(n)
end

# Step 1: Optimize hyperparameters
println(&quot;Optimizing hyperparameters...&quot;)
best_hyper = optimize_hyperparameters(Y, p; grid_size=20)
println(&quot;Optimal τ: &quot;, round(best_hyper.tau, digits=4))

# Step 2: Estimate BVAR
println(&quot;\nEstimating BVAR with MCMC...&quot;)
chain = estimate_bvar(Y, p;
    n_samples = 2000,
    n_adapts = 500,
    prior = :minnesota,
    hyper = best_hyper
)

# Step 3: Compute Bayesian IRF
H = 20
birf = irf(chain, p, n, H; method=:cholesky)

# Step 4: Report results
println(&quot;\nBayesian IRF (shock 1 → variable 1):&quot;)
for h in [0, 4, 8, 12, 20]
    med = round(birf.quantiles[h+1, 1, 1, 2], digits=3)
    lo = round(birf.quantiles[h+1, 1, 1, 1], digits=3)
    hi = round(birf.quantiles[h+1, 1, 1, 3], digits=3)
    println(&quot;  h=$h: $med [$lo, $hi]&quot;)
end</code></pre><pre><code class="nohighlight hljs"># Output:
# Optimizing hyperparameters...
# Optimal τ: 0.2143
#
# Estimating BVAR with MCMC...
# Sampling: 100%|████████████████████████████████| Time: 0:00:42
#
# Bayesian IRF (shock 1 → variable 1):
#   h=0: 0.305 [0.271, 0.341]
#   h=4: 0.046 [0.009, 0.085]
#   h=8: 0.005 [-0.014, 0.026]
#   h=12: 0.001 [-0.011, 0.013]
#   h=20: 0.000 [-0.006, 0.006]</code></pre><p>This workflow demonstrates the complete Bayesian pipeline: hyperparameter optimization selects the optimal shrinkage <span>$\tau$</span> via marginal likelihood, then MCMC produces posterior draws from which we compute IRFs with credible intervals. The IRF quickly converges to zero, consistent with the DGP&#39;s moderate persistence (<span>$A_{11} = 0.5$</span>). The credible intervals at <span>$h = 0$</span> are tight because the impact effect is well-identified by the Cholesky ordering, while longer horizons show wider bands reflecting cumulating parameter uncertainty.</p><hr/><h2 id="Large-BVAR"><a class="docs-heading-anchor" href="#Large-BVAR">Large BVAR</a><a id="Large-BVAR-1"></a><a class="docs-heading-anchor-permalink" href="#Large-BVAR" title="Permalink"></a></h2><h3 id="Handling-High-Dimensional-Systems"><a class="docs-heading-anchor" href="#Handling-High-Dimensional-Systems">Handling High-Dimensional Systems</a><a id="Handling-High-Dimensional-Systems-1"></a><a class="docs-heading-anchor-permalink" href="#Handling-High-Dimensional-Systems" title="Permalink"></a></h3><p>For large VAR systems (many variables), the Minnesota prior becomes essential:</p><pre><code class="language-julia hljs">using MacroEconometricModels

# Large system: 20 variables
n = 20
p = 4

# Stronger shrinkage for large systems
hyper_large = MinnesotaHyperparameters(
    tau = 0.1,      # Tighter prior
    decay = 2.0,
    lambda = 1.0,
    mu = 0.5,       # Penalize cross-variable coefficients
    omega = 1.0
)

# Or optimize automatically
best_hyper = optimize_hyperparameters(Y_large, p)</code></pre><p>For large systems (20+ variables), the number of VAR parameters (<span>$n^2 p + n$</span>) grows quadratically with the number of variables, quickly exceeding the sample size. The Minnesota prior prevents overfitting by shrinking cross-variable coefficients toward zero (<code>mu=0.5</code>) and applying strong overall tightness (<code>tau=0.1</code>). Bańbura, Giannone &amp; Reichlin (2010) show that BVAR with optimized shrinkage outperforms both unrestricted VAR and small-scale models for macroeconomic forecasting.</p><p><strong>Reference</strong>: Bańbura, Giannone &amp; Reichlin (2010)</p><hr/><h2 id="References"><a class="docs-heading-anchor" href="#References">References</a><a id="References-1"></a><a class="docs-heading-anchor-permalink" href="#References" title="Permalink"></a></h2><h3 id="Minnesota-Prior-and-BVAR"><a class="docs-heading-anchor" href="#Minnesota-Prior-and-BVAR">Minnesota Prior and BVAR</a><a id="Minnesota-Prior-and-BVAR-1"></a><a class="docs-heading-anchor-permalink" href="#Minnesota-Prior-and-BVAR" title="Permalink"></a></h3><ul><li>Bańbura, Marta, Domenico Giannone, and Lucrezia Reichlin. 2010. &quot;Large Bayesian Vector Auto Regressions.&quot; <em>Journal of Applied Econometrics</em> 25 (1): 71–92. <a href="https://doi.org/10.1002/jae.1137">https://doi.org/10.1002/jae.1137</a></li><li>Carriero, Andrea, Todd E. Clark, and Massimiliano Marcellino. 2015. &quot;Bayesian VARs: Specification Choices and Forecast Accuracy.&quot; <em>Journal of Applied Econometrics</em> 30 (1): 46–73. <a href="https://doi.org/10.1002/jae.2272">https://doi.org/10.1002/jae.2272</a></li><li>Doan, Thomas, Robert Litterman, and Christopher Sims. 1984. &quot;Forecasting and Conditional Projection Using Realistic Prior Distributions.&quot; <em>Econometric Reviews</em> 3 (1): 1–100. <a href="https://doi.org/10.1080/07474938408800053">https://doi.org/10.1080/07474938408800053</a></li><li>Giannone, Domenico, Michele Lenza, and Giorgio E. Primiceri. 2015. &quot;Prior Selection for Vector Autoregressions.&quot; <em>Review of Economics and Statistics</em> 97 (2): 436–451. <a href="https://doi.org/10.1162/REST_a_00483">https://doi.org/10.1162/REST<em>a</em>00483</a></li><li>Kadiyala, K. Rao, and Sune Karlsson. 1997. &quot;Numerical Methods for Estimation and Inference in Bayesian VAR-Models.&quot; <em>Journal of Applied Econometrics</em> 12 (2): 99–132. <a href="https://doi.org/10.1002/(SICI)1099-1255(199703)12:2&lt;99::AID-JAE429&gt;3.0.CO;2-A">https://doi.org/10.1002/(SICI)1099-1255(199703)12:2&lt;99::AID-JAE429&gt;3.0.CO;2-A</a></li><li>Litterman, Robert B. 1986. &quot;Forecasting with Bayesian Vector Autoregressions—Five Years of Experience.&quot; <em>Journal of Business &amp; Economic Statistics</em> 4 (1): 25–38. <a href="https://doi.org/10.1080/07350015.1986.10509491">https://doi.org/10.1080/07350015.1986.10509491</a></li></ul><h3 id="MCMC-and-Bayesian-Inference"><a class="docs-heading-anchor" href="#MCMC-and-Bayesian-Inference">MCMC and Bayesian Inference</a><a id="MCMC-and-Bayesian-Inference-1"></a><a class="docs-heading-anchor-permalink" href="#MCMC-and-Bayesian-Inference" title="Permalink"></a></h3><ul><li>Gelman, Andrew, John B. Carlin, Hal S. Stern, David B. Dunson, Aki Vehtari, and Donald B. Rubin. 2013. <em>Bayesian Data Analysis</em>. 3rd ed. Boca Raton, FL: CRC Press. ISBN 978-1-4398-4095-5.</li><li>Hoffman, Matthew D., and Andrew Gelman. 2014. &quot;The No-U-Turn Sampler: Adaptively Setting Path Lengths in Hamiltonian Monte Carlo.&quot; <em>Journal of Machine Learning Research</em> 15 (1): 1593–1623.</li></ul></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../factormodels/">« Factor Models</a><a class="docs-footer-nextpage" href="../innovation_accounting/">Innovation Accounting »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.16.1 on <span class="colophon-date" title="Saturday 7 February 2026 11:44">Saturday 7 February 2026</span>. Using Julia version 1.12.4.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
